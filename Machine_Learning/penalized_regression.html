
<!DOCTYPE html>
<html lang="en-US">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge">
    <title>Penalized Regression - LOST</title>
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" href="/assets/css/just-the-docs.css">
    <script type="text/javascript" src="/assets/js/vendor/lunr.min.js"></script>
  <script type="text/javascript" src="/assets/js/just-the-docs.js"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Penalized Regression | LOST</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Penalized Regression" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Source code for the Library of Statistical Techniques" />
<meta property="og:description" content="Source code for the Library of Statistical Techniques" />
<meta property="og:site_name" content="LOST" />
<script type="application/ld+json">
{"description":"Source code for the Library of Statistical Techniques","@type":"WebPage","headline":"Penalized Regression","url":"/Machine_Learning/penalized_regression.html","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } } });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML">
  </script>
</head>
<body>
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
    <symbol id="link" viewBox="0 0 16 16">
      <title>Link</title>
      <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </symbol>
  </svg>
  <div class="page-wrap">
    <div class="side-bar">
      <div class="site-header">
        <a href="" class="site-title lh-tight">
  LOST
</a>
        <button class="menu-button fs-3 js-main-nav-trigger" data-text-toggle="Hide" type="button">Menu</button>
      </div>
      <div class="navigation main-nav js-main-nav">
        <nav role="navigation" aria-label="Main navigation">
  <ul class="navigation-list"><li class="navigation-list-item"><a href="/" class="navigation-list-link">Home</a></li><li class="navigation-list-item active"><a href="/Data/" class="navigation-list-link"></a></li><li class="navigation-list-item"><a href="/Data_Manipulation/data_manipulation.html" class="navigation-list-link">Data Manipulation</a><ul class="navigation-list-child-list "><li class="navigation-list-item "><a href="/Data_Manipulation/collapse_a_data_set.html" class="navigation-list-link">Collapse a Data Set</a></li><li class="navigation-list-item "><a href="/Data_Manipulation/Combining_Datasets/combining_datasets_overview.html" class="navigation-list-link">Combining Datasets</a><ul class="navigation-list-child-list"><li class="navigation-list-item ">
                            <a href="/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html" class="navigation-list-link">Vertical Combination</a>
                          </li><li class="navigation-list-item ">
                            <a href="/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html" class="navigation-list-link">Horizontal Combination (Deterministic)</a>
                          </li></ul></li><li class="navigation-list-item "><a href="/Data_Manipulation/determine_the_observation_level_of_a_data_set.html" class="navigation-list-link">Determine the Observation Level of a Data Set</a></li><li class="navigation-list-item "><a href="/Data_Manipulation/Reshaping/reshape.html" class="navigation-list-link">Reshaping Data</a><ul class="navigation-list-child-list"><li class="navigation-list-item ">
                            <a href="/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html" class="navigation-list-link">Reshape Panel Data from Wide to Long</a>
                          </li><li class="navigation-list-item ">
                            <a href="/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html" class="navigation-list-link">Reshape Panel Data from Long to Wide</a>
                          </li></ul></li><li class="navigation-list-item "><a href="/Data_Manipulation/rowwise_calculations.html" class="navigation-list-link">Rowwise Calculations</a></li></ul></li><li class="navigation-list-item"><a href="/Geo-Spatial/Geo-spatial.html" class="navigation-list-link">Geo-Spatial</a><ul class="navigation-list-child-list "><li class="navigation-list-item "><a href="/Geo-Spatial/geocoding.html" class="navigation-list-link">Geocoding</a></li><li class="navigation-list-item "><a href="/Geo-Spatial/merging_shape_files.html" class="navigation-list-link">Merging Shape Files</a></li></ul></li><li class="navigation-list-item active"><a href="/Machine_Learning/Machine_Learning.html" class="navigation-list-link">Machine Learning</a><ul class="navigation-list-child-list "><li class="navigation-list-item "><a href="/Machine_Learning/causal_forest.html" class="navigation-list-link">Causal Forest</a></li><li class="navigation-list-item  active"><a href="/Machine_Learning/penalized_regression.html" class="navigation-list-link active">Penalized Regression</a></li></ul></li><li class="navigation-list-item"><a href="/Model_Estimation/Estimation.html" class="navigation-list-link">Model Estimation</a><ul class="navigation-list-child-list "><li class="navigation-list-item "><a href="/Model_Estimation/fixed_effects_in_linear_regression.html" class="navigation-list-link">Fixed Effects in Linear Regression</a></li><li class="navigation-list-item "><a href="/Model_Estimation/heckman_correction_model.html" class="navigation-list-link">Heckman Correction Model</a></li><li class="navigation-list-item "><a href="/Model_Estimation/instrumental_variables.html" class="navigation-list-link">Instrumental Variables</a></li><li class="navigation-list-item "><a href="/Model_Estimation/interaction_terms_and_polynomials.html" class="navigation-list-link">Interaction Terms and Polynomials</a></li><li class="navigation-list-item "><a href="/Model_Estimation/linear_hypothesis_tests.html" class="navigation-list-link">Linear Hypothesis Tests</a></li><li class="navigation-list-item "><a href="/Model_Estimation/linear_mixed_effects_regression.html" class="navigation-list-link">Linear Mixed-Effects Regression</a></li><li class="navigation-list-item "><a href="/Model_Estimation/logit_model.html" class="navigation-list-link">Logit Model</a></li><li class="navigation-list-item "><a href="/Model_Estimation/mcfaddens_choice_model.html" class="navigation-list-link">McFadden's Choice Model (Alternative-Specific Conditional Logit)</a></li><li class="navigation-list-item "><a href="/Model_Estimation/ordinary_least_squares.html" class="navigation-list-link">Ordinary Least Squares (Linear Regression)</a></li><li class="navigation-list-item "><a href="/Model_Estimation/probit_model.html" class="navigation-list-link">Probit Model</a></li><li class="navigation-list-item "><a href="/Model_Estimation/regression_discontinuity_design.html" class="navigation-list-link">Regression Discontinuity Design</a></li><li class="navigation-list-item "><a href="/Model_Estimation/Nonstandard_Errors/nonstandard_errors.html" class="navigation-list-link">Nonstandard Errors</a><ul class="navigation-list-child-list"><li class="navigation-list-item ">
                            <a href="/Model_Estimation/Nonstandard_Errors/bootstrap_se.html" class="navigation-list-link">Bootstrap Standard Errors</a>
                          </li><li class="navigation-list-item ">
                            <a href="/Model_Estimation/Nonstandard_Errors/clustered_se.html" class="navigation-list-link">Cluster-Robust Standard Errors</a>
                          </li><li class="navigation-list-item ">
                            <a href="/Model_Estimation/Nonstandard_Errors/hc_se.html" class="navigation-list-link">Heteroskedasticity-consistent standard errors</a>
                          </li></ul></li></ul></li><li class="navigation-list-item"><a href="/Presentation/Presentation.html" class="navigation-list-link">Presentation</a><ul class="navigation-list-child-list "><li class="navigation-list-item "><a href="/Presentation/export_a_formatted_regression_table.html" class="navigation-list-link">Export a Formatted Regression Table</a></li><li class="navigation-list-item "><a href="/Presentation/heatmap_colored_correlation_matrix.html" class="navigation-list-link">Heatmap Colored Correlation Matrix</a></li><li class="navigation-list-item "><a href="/Presentation/line_graph_with_labels_at_the_beginning_or_end.html" class="navigation-list-link">Line Graph with Labels at the Beginning or End of Lines</a></li><li class="navigation-list-item "><a href="/Presentation/marginal_effects_plots_for_interactions_with_continuous_variables.html" class="navigation-list-link">Marginal Effects Plots for Interactions with Continuous Variables</a></li><li class="navigation-list-item "><a href="/Presentation/scatterplot_by_group_on_shared_axes.html" class="navigation-list-link">Scatterplot by Group on Shared Axes</a></li><li class="navigation-list-item "><a href="/Presentation/styling_line_graphs.html" class="navigation-list-link">Styling Line Graphs</a></li></ul></li><li class="navigation-list-item"><a href="/Summary_Statistics/Summary_Statistics.html" class="navigation-list-link">Summary Statistics</a><ul class="navigation-list-child-list "><li class="navigation-list-item "><a href="/Summary_Statistics/Balance_Tables.html" class="navigation-list-link">Balance Table</a></li><li class="navigation-list-item "><a href="/Summary_Statistics/Summary_Statistics_Tables.html" class="navigation-list-link">Summary Statistics Tables</a></li></ul></li><li class="navigation-list-item"><a href="/Time_Series/Time_Series.html" class="navigation-list-link">Time Series</a><ul class="navigation-list-child-list "><li class="navigation-list-item "><a href="/Time_Series/AR-models.html" class="navigation-list-link">Auto Regressive Models</a></li><li class="navigation-list-item "><a href="/Time_Series/creating_time_series_dataset.html" class="navigation-list-link">Creating Time Series Dataset</a></li></ul></li><li class="navigation-list-item"><a href="/Other/Other.html" class="navigation-list-link">Other</a><ul class="navigation-list-child-list "><li class="navigation-list-item "><a href="/Other/import_a_foreign_data_file.html" class="navigation-list-link">Import a Foreign Data File</a></li></ul></li><li class="navigation-list-item"><a href="/Desired_Nonexistent_Pages/desired_nonexistent_pages.html" class="navigation-list-link">Desired Nonexistent Pages</a></li><li class="navigation-list-item"><a href="/Contributing/Contributing.html" class="navigation-list-link">Contributing</a></li></ul>
</nav>
      </div>
      <footer class="site-footer">
        <p class="text-small text-grey-dk-000 mb-4">This site uses <a href="https://github.com/pmarsceill/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.</p>
      </footer>
    </div>
    <div class="main-content-wrap js-main-content" tabindex="0">
      <div class="main-content">
        <div class="page-header js-page-header">
          <div class="search">
            <div class="search-input-wrap">
              <input type="text" class="js-search-input search-input" tabindex="0" placeholder="Search LOST" aria-label="Search LOST" autocomplete="off">
              <svg width="14" height="14" viewBox="0 0 28 28" xmlns="http://www.w3.org/2000/svg" class="search-icon"><title>Search</title><g fill-rule="nonzero"><path d="M17.332 20.735c-5.537 0-10-4.6-10-10.247 0-5.646 4.463-10.247 10-10.247 5.536 0 10 4.601 10 10.247s-4.464 10.247-10 10.247zm0-4c3.3 0 6-2.783 6-6.247 0-3.463-2.7-6.247-6-6.247s-6 2.784-6 6.247c0 3.464 2.7 6.247 6 6.247z"/><path d="M11.672 13.791L.192 25.271 3.02 28.1 14.5 16.62z"/></g></svg>
            </div>
            <div class="js-search-results search-results-wrap"></div>
          </div>
            <ul class="list-style-none text-small aux-nav">
                <li class="d-inline-block my-0"><a href="https://github.com/lost-stats/lost-stats.github.io/blob/source/Machine_Learning/penalized_regression.md">Edit this page on GitHub</a></li>
            </ul>
        </div>
        <div class="page">
              <nav class="breadcrumb-nav">
                <ol class="breadcrumb-nav-list">
                    <li class="breadcrumb-nav-list-item"><a href="/Machine_Learning/Machine_Learning.html">Machine Learning</a></li>
                  <li class="breadcrumb-nav-list-item"><span>Penalized Regression</span></li>
                </ol>
              </nav>
          <div id="main-content" class="page-content" role="main">
              <h1 id="penalized-regression">
          <a href="#penalized-regression" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> Penalized Regression
      </h1>
<p>When running a regression, especially one with many predictors, the results have a tendency to overfit the data, reducing out-of-sample predictive properties.</p>
<p>Penalized regression eases this problem by forcing the regression estimator to shrink its coefficients towards 0 in order to avoid the “penalty” term imposed on the coefficients. This process is closely related to the idea of Bayesian shrinkage, and indeed standard penalized regression results are equivalent to regression performed using <a href="https://amstat.tandfonline.com/doi/abs/10.1198/016214508000000337?casa_token=DE6O93Bz7uUAAAAA:Ff_MiPXvPH32NA2hnGtZtqb8grXEiEqF0fdO3B0p_a6wOaqRciCZ4ASwxn69gdOb93Lbt-HSyK1o4As">certain Bayesian priors</a>.</p>
<p>Regular OLS selects coefficients <script type="math/tex">\hat{\beta}</script> to minimize the sum of squared errors:</p>
<script type="math/tex; mode=display">\min\sum_i(y_i - X_i\hat{\beta})^2</script>
<p>Non-OLS regressions similarly select coefficients to minimize a similar objective function. Penalized regression adds a penalty term <script type="math/tex">\lambda\lVert\beta\rVert_p</script> to that objective function, where <script type="math/tex">\lambda</script> is a tuning parameter that determines how harshly to penalize coefficients, and <script type="math/tex">\lVert\beta\rVert_p</script> is the <script type="math/tex">p</script>-norm of the coefficients, or <script type="math/tex">\sum_j\lvert\beta\rvert^p</script>.</p>
<script type="math/tex; mode=display">\min\left(\sum_i(y_i - X_i\hat{\beta})^2 + \lambda\left\lVert\beta\right\rVert_p \right)</script>
<p>Typically <script type="math/tex">p</script> is set to 1 for LASSO regression (least absolute shrinkage and selection operator), which has the effect of tending to set coefficients to 0, i.e. model selection, or to 2 for Ridge Regression. Elastic net regression provides a weighted mix of LASSO and Ridge penalties, commonly referring to the weight as <script type="math/tex">\alpha</script>.</p>
      <h2 id="keep-in-mind">
          <a href="#keep-in-mind" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> Keep in Mind
      </h2>
<ul>
  <li>To avoid being penalized for a constant term, or by differences in scale between variables, it is a very good idea to standardize each variable (subtract the mean and divide by the standard deviation) before running a penalized regression.</li>
  <li>Penalized regression can be run for logit and other kinds of regression, not just linear regression. Using penalties with general linear models like logit is common.</li>
  <li>Penalized regression coefficients are designed to improve out-of-sample prediction, but they are biased. If the goal is estimation of a parameter, rather than prediction, this should be kept in mind. A common procedure is to use LASSO to select variables, and then run regular regression models with the variables that LASSO has selected.</li>
  <li>The <script type="math/tex">\lambda</script> parameter is often chosen using cross-validation. Many penalized regression commands include an option to select <script type="math/tex">\lambda</script> by cross-validation automatically.</li>
  <li>LASSO models commonly include variables along with polynomial transformation of those variables and interactions, allowing LASSO to determine which transformations are worth keeping.</li>
</ul>
      <h2 id="also-consider">
          <a href="#also-consider" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> Also Consider
      </h2>
<ul>
  <li>If it is not important to estimate coefficients but the goal is simply to predict an outcome, then there are many other <a href="https://lost-stats.github.io/Machine_Learning/Machine_Learning.html">machine learning</a> methods that do so, and in some cases can handle higher dimensionality or work with smaller samples.</li>
</ul>
      <h1 id="implementations">
          <a href="#implementations" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> Implementations
      </h1>
      <h2 id="r">
          <a href="#r" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> R
      </h2>
<p>We will use the <strong>glmnet</strong> package.</p>
<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Install glmnet and tidyverse if necessary</span><span class="w">
</span><span class="c1"># install.packages('glmnet', 'tidyverse')</span><span class="w">

</span><span class="c1"># Load glmnet</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">glmnet</span><span class="p">)</span><span class="w">

</span><span class="c1"># Load iris data</span><span class="w">
</span><span class="n">data</span><span class="p">(</span><span class="n">iris</span><span class="p">)</span><span class="w">

</span><span class="c1"># Create a matrix with all variables other than our dependent vairable, Sepal.Length</span><span class="w">
</span><span class="c1"># and interactions. </span><span class="w">
</span><span class="c1"># -1 to omit the intercept</span><span class="w">
</span><span class="n">M</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">model.matrix</span><span class="p">(</span><span class="n">lm</span><span class="p">(</span><span class="n">Sepal.Length</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="p">(</span><span class="n">.</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">iris</span><span class="p">))</span><span class="w">
</span><span class="c1"># Add squared terms of numeric variables</span><span class="w">
</span><span class="n">numeric.var.names</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">names</span><span class="p">(</span><span class="n">iris</span><span class="p">)[</span><span class="m">2</span><span class="o">:</span><span class="m">4</span><span class="p">]</span><span class="w">
</span><span class="n">M</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cbind</span><span class="p">(</span><span class="n">M</span><span class="p">,</span><span class="n">as.matrix</span><span class="p">(</span><span class="n">iris</span><span class="p">[,</span><span class="n">numeric.var.names</span><span class="p">]</span><span class="o">^</span><span class="m">2</span><span class="p">))</span><span class="w">
</span><span class="n">colnames</span><span class="p">(</span><span class="n">M</span><span class="p">)[</span><span class="m">16</span><span class="o">:</span><span class="m">18</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">paste</span><span class="p">(</span><span class="n">numeric.var.names</span><span class="p">,</span><span class="s1">'squared'</span><span class="p">)</span><span class="w">

</span><span class="c1"># Create a matrix for our dependent variable too</span><span class="w">
</span><span class="n">Y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.matrix</span><span class="p">(</span><span class="n">iris</span><span class="o">$</span><span class="n">Sepal.Length</span><span class="p">)</span><span class="w">

</span><span class="c1"># Standardize all variables</span><span class="w">
</span><span class="n">M</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">scale</span><span class="p">(</span><span class="n">M</span><span class="p">)</span><span class="w">
</span><span class="n">Y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">scale</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span><span class="w">


</span><span class="c1"># Use glmnet to estimate penalized regression</span><span class="w">
</span><span class="c1"># We pick family = "gaussian" for linear regression;</span><span class="w">
</span><span class="c1"># other families work for other kinds of data, like binomial for binary data</span><span class="w">
</span><span class="c1"># In each case, we use cv.glmnet to pick our lambda value using cross-validation</span><span class="w">
</span><span class="c1"># using nfolds folds for cross-validation</span><span class="w">
</span><span class="c1"># Note that alpha = 1 picks LASSO</span><span class="w">
</span><span class="n">cv.lasso</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cv.glmnet</span><span class="p">(</span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="n">Y</span><span class="p">,</span><span class="w"> </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"gaussian"</span><span class="p">,</span><span class="w"> </span><span class="n">nfolds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="c1"># We might want to see how the choice of lambda relates to out-of-sample error with a plot</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">cv.lasso</span><span class="p">)</span><span class="w">
</span><span class="c1"># After doing CV, we commonly pick the lambda.min for lambda, </span><span class="w">
</span><span class="c1"># which is the lambda that minimizes out-of-sample error</span><span class="w">
</span><span class="c1"># or lambda.1se, which is one standard error above lambda.min,</span><span class="w">
</span><span class="c1"># which penalizes more harshly. The choice depends on context.</span><span class="w">
</span><span class="n">lasso.model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">glmnet</span><span class="p">(</span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="n">Y</span><span class="p">,</span><span class="w"> </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"gaussian"</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">lambda</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cv.lasso</span><span class="o">$</span><span class="n">lambda.min</span><span class="p">)</span><span class="w">
</span><span class="c1"># coefficients are shown in the beta element. . means LASSO dropped it</span><span class="w">
</span><span class="n">lasso.model</span><span class="o">$</span><span class="n">beta</span><span class="w">

</span><span class="c1"># Running Ridge, or mixing the two with elastic net, simply means picking</span><span class="w">
</span><span class="c1"># alpha = 0 (Ridge), or 0 &lt; alpha &lt; 1 (Elastic Net)</span><span class="w">
</span><span class="n">cv.ridge</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cv.glmnet</span><span class="p">(</span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="n">Y</span><span class="p">,</span><span class="w"> </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"gaussian"</span><span class="p">,</span><span class="w"> </span><span class="n">nfolds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w">
</span><span class="n">ridge.model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">glmnet</span><span class="p">(</span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="n">Y</span><span class="p">,</span><span class="w"> </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"gaussian"</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">lambda</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cv.ridge</span><span class="o">$</span><span class="n">lambda.min</span><span class="p">)</span><span class="w">

</span><span class="n">cv.elasticnet</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cv.glmnet</span><span class="p">(</span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="n">Y</span><span class="p">,</span><span class="w"> </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"gaussian"</span><span class="p">,</span><span class="w"> </span><span class="n">nfolds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">.5</span><span class="p">)</span><span class="w">
</span><span class="n">elasticnet.model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">glmnet</span><span class="p">(</span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="n">Y</span><span class="p">,</span><span class="w"> </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"gaussian"</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">.5</span><span class="p">,</span><span class="w"> </span><span class="n">lambda</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cv.elasticnet</span><span class="o">$</span><span class="n">lambda.min</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>
      <h2 id="stata">
          <a href="#stata" class="anchor-heading"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#link"></use></svg></a> Stata
      </h2>
<p>Penalized regression is one of the few machine learning algorithms that Stata does natively. This requires Stata 16. If you do not have Stata 16, you can alternately perform some forms of penalized regression by installing the <strong>lars</strong> package using <strong>ssc install lars</strong>.</p>
<div class="language-stata highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">* Use NLSY-W data
</span><span class="err">sysuse</span> <span class="err">nlsw88.dta,</span> <span class="err">clear
</span><span class="c1">
* Construct all squared and interaction terms by loop so we don't have to specify them all
* by hand in the regression function
</span><span class="err">local</span> <span class="err">numeric_vars</span> <span class="o">=</span> <span class="s2">"age grade hours ttl_exp tenure"</span><span class="err">
local</span> <span class="err">factor_vars</span> <span class="o">=</span> <span class="s2">"race married never_married collgrad south smsa c_city industry occupation union"</span><span class="err">
</span><span class="c1">
* Add all squares
</span><span class="err">foreach</span> <span class="err">x</span> <span class="err">in</span> <span class="p">`</span><span class="si">numeric_vars</span><span class="p">'</span> <span class="err">{
</span>	<span class="err">g</span> <span class="err">sq_</span><span class="p">`</span><span class="si">x' = `x</span><span class="p">'</span><span class="o">^</span><span class="err">2
}
</span><span class="c1">
* Turn all factors into dummies so we can standardize them
</span><span class="err">local</span> <span class="err">faccount</span> <span class="o">=</span> <span class="err">1
local</span> <span class="err">dummy_vars</span> <span class="o">=</span> <span class="s2">""</span><span class="err">
foreach</span> <span class="err">x</span> <span class="err">in</span> <span class="p">`</span><span class="si">factor_vars</span><span class="p">'</span> <span class="err">{
</span>	<span class="err">xi</span> <span class="err">i.</span><span class="p">`</span><span class="si">x', pre(f`count</span><span class="p">'</span><span class="err">_)
</span>	<span class="err">local</span> <span class="err">count</span> <span class="o">=</span> <span class="p">`</span><span class="si">count</span><span class="p">'</span> <span class="o">+</span> <span class="err">1
}
</span><span class="c1">
* Add all numeric-numeric interactions; these are easy
* factor interactions would need a more thorough loop
</span><span class="err">forvalues</span> <span class="err">i</span> <span class="o">=</span> <span class="err">1(1)5</span> <span class="err">{
</span>	<span class="err">local</span> <span class="err">next_i</span> <span class="o">=</span> <span class="p">`</span><span class="si">i</span><span class="p">'</span><span class="o">+</span><span class="err">1
</span>	<span class="err">forvalues</span> <span class="err">j</span> <span class="o">=</span> <span class="p">`</span><span class="si">next_i</span><span class="p">'</span><span class="err">(1)5</span> <span class="err">{
</span>		<span class="err">local</span> <span class="err">namei</span> <span class="o">=</span> <span class="err">word(</span><span class="s2">"`numeric_vars'"</span><span class="err">,</span><span class="p">`</span><span class="si">i</span><span class="p">'</span><span class="err">)
</span>		<span class="err">local</span> <span class="err">namej</span> <span class="o">=</span> <span class="err">word(</span><span class="s2">"`numeric_vars'"</span><span class="err">,</span><span class="p">`</span><span class="si">j</span><span class="p">'</span><span class="err">)
</span>		<span class="err">g</span> <span class="err">interact_</span><span class="p">`</span><span class="si">i'_`j' = `namei'*`namej</span><span class="p">'</span><span class="err">
</span>	<span class="err">}
}
</span><span class="c1">
* Standardize everything
</span><span class="err">foreach</span> <span class="err">var</span> <span class="err">of</span> <span class="err">varlist</span> <span class="p">`</span><span class="si">numeric_vars</span><span class="p">'</span> <span class="err">f</span><span class="o">*</span><span class="err">_</span><span class="o">*</span> <span class="err">interact_</span><span class="o">*</span> <span class="err">{
</span>	<span class="err">qui</span> <span class="err">summ</span> <span class="p">`</span><span class="si">var</span><span class="p">'</span><span class="err">
</span>	<span class="err">qui</span> <span class="err">replace</span> <span class="p">`</span><span class="si">var' = (`var</span><span class="p">'</span> <span class="o">-</span> <span class="err">r(mean))/r(sd)
}
</span><span class="c1">
* Use the lasso command to run LASSO
* using sel(cv) to select lambda using cross-validation
* we specify a linear model here, but logit/probit/poisson would work
</span><span class="err">lasso</span> <span class="err">linear</span> <span class="err">wage</span> <span class="p">`</span><span class="si">numeric_vars</span><span class="p">'</span> <span class="err">f</span><span class="o">*</span><span class="err">_</span><span class="o">*</span> <span class="err">interact_</span><span class="o">*</span><span class="err">,</span> <span class="err">sel(cv)
</span><span class="c1">* get list of included coefficients
</span><span class="err">lassocoef
</span><span class="c1">
* We can use elasticnet to run Elastic Net
* By default, alpha will be selected by cross-validation as well
</span><span class="err">elasticnet</span> <span class="err">linear</span> <span class="err">wage</span> <span class="p">`</span><span class="si">numeric_vars</span><span class="p">'</span> <span class="err">f</span><span class="o">*</span><span class="err">_</span><span class="o">*</span> <span class="err">interact_</span><span class="o">*</span><span class="err">,</span> <span class="err">sel(cv)
</span></code></pre></div></div>
        </div>
      </div>
    </div>
  </div>
</body>
</html>
