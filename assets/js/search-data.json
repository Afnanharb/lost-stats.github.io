{
  
  "0": {
    "title": "AR Models",
    "content": "Autoregressive (AR) Models . Autoregressive (AR) models are fundamental to time series analysis. They are estimated via regressing a variable on one or more of its lagged values. That is, AR models take the form: Where we say p is the order of our auto regression. Their estimation in statistical software packages is generally straightforward. . For additional information, see Wikipedia: Autoregressive model. . Keep In Mind . An AR model can be univariate (scalar) or multivariate (vector). This may be important to implementing an AR model in your statisical package of choice. | Data should be properly formatted before estimation. If not, non-time series objects (e.g., a date column) may be interpereted by software as a time series variable, leading to erroneous output. | . Implementations . Following the instructions for creating and formatting Time Series Data, we will use quaterly GDP data downloaded from FRED as an example. . R . #load data gdp = read.csv(&quot;https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv&quot;) #estimation via ols: pay attention to the selection of the &#39;GDPC1&#39; column. #if the column is not specified, the function call also interprets the date column as a time series variable! ar_gdp = ar.ols(gdp$GDPC1) ar_gdp #lag order is automatically selected by minimizing AIC #disable this feature with the optional command &#39;aic = F&#39;. Note: you will also likely wish to specify the argument &#39;order.max&#39;. #ar.ols() defaults to demeaning the data automatically. Also consider taking logs and first differencing for statistically meaningful results. . STATA . *load data import delimited &quot;https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv&quot;, clear *Generate the new date variable *To generalize to a different set of data, replace &#39;1947q1&#39; with your own series&#39; start date. generate date_index = tq(1947q1) + _n-1 *Index the new variable format as quarter format date_index %tq *Convert a variable into time-series data tsset date_index *Specifiy and Run AR regression: this STATA method will not automatically select a lag order. *The &#39;L.&#39; operator indicates the lagged value of a variable in STATA, &#39;L2.&#39; its second lag, and so on. reg gdpc1 L.gdpc1 L2.gdpc1 *variables are not demeaned automatically by STATA. Also consider taking logs and first differencing for statistically meaningful results. .",
    "url": "/Time_Series/AR-models.html",
    "relUrl": "/Time_Series/AR-models.html"
  }
  ,"1": {
    "title": "ARMA Models",
    "content": "Autoregressive Moving-Average (ARMA) Models . Auto regressive moving average (ARMA) models are a combination of two commonly used time series processes, the autoregressive (AR) process and the moving-average (MA) process. As such, ARMA models have the form . If an ARMA model has an AR component of order and an MA component of order , then the model is commonly refered to as an . For additional information, see Wikipedia: Autoregressive Moving-Average model. . Keep In Mind . Data must be properly formatted for estimation as a time-series. See creating a time series data set. If this is not done, then depending on your statistical package of choice, either your estimation will fail to execute or you will receive erroneous output. | ARMA models include some number of lagged error terms from the MA component, which are inherently unobservable. Consequently these models cannot be estimated using OLS alone, unlike AR models. | ARMA models are most commonly estimated using maximum likelihood estimation (MLE). One consequence of this is that, given some time series and some specified order , the estimates obtained from the estimated model will vary depending on the type of MLE estimation used. | As is the case in many situations where one is trying to estimate a time-series process, model selection is important. For ARMA models, model selection meaning chosing the number of AR and MA parameters, the and , for which a coefficient will be estimated. In practice, it is common to estimate several different potential models, then use some criterion to determine which model best fits the time-series. Common criteria used to evaluate ARMA models are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC), also referred to as the Schwarz Information Criterion (SIC). For more information on these and other model selection criteria, see Wikipedia: Model Selection. | Estimating a time series using an ARMA model relies on two assumptions. The first is the standard assumption that we have selected the correct functional form for the time series. In this case, that means assuming that we have selected the correct and . Second, we also have to assume that our time series is stationary. For a discussion of the stationarity assumption and what constraints this assumption imposes on our model, again see Wikipedia: Autoregressive Moving-Average model. | . Also Consider . ARMA models can only be estimated for univariate time series. If you are interested in estimating a time series process using multiple time series on the right hand side of your model, consider using a vector AR (VAR) model or a VARMA model. | Before estimating an ARMA model, it is standard practice to try to determine whether or not the time series appears to be stationary. See LOST: Stationarity and Weak Dependence for more details. | If the time series you are trying to estimate does not appear to be stationary, then using an ARMA model to estimate the series is innappropriate. For simpler forms of nonstationarity, an ARIMA model may be useful. An model is a more general model for a time-series than an . In these models, still signifies an component, and an component. For more information on ARIMA models, see Wikipedia: ARIMA. For information about estimating an ARIMA model, see LOST: ARIMA models | . Implementations . First, follow the instructions for creating and formatting time-series data using your software of choice. We will again use quarterly US GDP data downloaded from FRED as an example. This time, though, we will try to estimate the quarterly log change in GDP with an process. Note that an model is almost certainly not the best way to estimate this time series, and is used here solely as an example. . R . There are numerous packages to estimate ARMA models in R. For this tutorial, we will use the arima() function, which comes preloaded into R from the stats package. For our purposes, it is sufficient to note that estimating an model is largely equivalent to estimating an . For more information about estimating a true ARIMA process (where ), see the Also Consider section above. Additionally, the tsibble package can also be used to easily construct our quarterly log change in GDP variable. . The arima() function does require that we specify the order of the model (ie, pick the values of and ). For an alternative function that will evaluate multiple models and select the best performing, see the auto.arima function available through the forecast package. . ## Load and install time series packages if (!require(&quot;tsibble&quot;)) install.packages(&quot;tsibble&quot;) library(tsibble) #load data gdp = read.csv(&quot;https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv&quot;) #set our data up as a time-series gdp$DATE &lt;- as.Date(gdp$DATE) gdp_ts &lt;- as_tsibble(gdp, index = DATE, regular = FALSE) %&gt;% index_by(qtr = ~ yearquarter(.)) #construct our first difference of log gdp variable gdp_ts$lgdp=log(gdp_ts$GDPC1) gdp_ts$ldiffgdp=difference(gdp_ts$lgdp, lag=1, difference=1) #Estimate our ARMA(3,1) ##Note that because we are modeling for the first difference of log GDP, we cannot use our first observation of ##log GDP to estimate our model. arma_gdp = arima(gdp_ts$lgdp[2:292], order=c(3,0,1)) arma_gdp . Stata . In Stata we will again estimate an by estimating an using the Stata command arima. This command works similarly to Stata’s reg command. For information about the specific estimation procedure used by this function, optional arguments, etc, see Stata: ARIMA manual . *load data import delimited &quot;https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv&quot; *Generate the new date variable generate date_index = tq(1947q1) + _n-1 *Index the date variable as quarterly format date_index %tq *Convert a variable into time-series data tsset date_index *construct our first difference of log gdp variable gen lgdp = ln(gdpc1) gen dlgdp = D.lgdp *Specify the ARMA model using the arima command *Stata will automatically drop the first entry, since we do not have a value for the first difference of GDP *for this entry. arima dlgdp, arima(3,0,1) .",
    "url": "/Time_Series/ARMA-models.html",
    "relUrl": "/Time_Series/ARMA-models.html"
  }
  ,"2": {
    "title": "Balance Table",
    "content": "Balance Tables . Balance Tables are a method by which you can statistically compare differences in characteristics between a treatment and control group. Common in experimental work and when using matching estimators, balance tables show if the treatment and control group are ‘balanced’ and can be seen as similarly ‘identical’ for comparison of a causal effect. . Keep in Mind . When a characteristic is statistically different between control and treatment, your study is unbalanced in respect to that attribute. | When a characteristic is unbalanced in your study, you may want to consider controlling for that attribute as a variable in your model specification. | Balance tables can only report numeric differences and are not suitable for string value comparisions | . Implementations . R . # Import Dependency library(&quot;cobalt&quot;) # Load Data data(mtcars) # Create Balance Table bal.tab(am ~ mpg + hp, data = mtcars) . Stata . * Import Dependency: &#39;ssc install table1&#39; * Load Data sysuse auto, clear * Create Balance Table * You need to declare the kind of variable for each, as well as the variable by which you define treatment and control. * Adding test gives the statistical difference between the two groups. The ending saves your output as an .xls file table1, by(foreign) vars(price conts mpg conts weight contn length conts) test saving(bal_tab.xls, replace) . Also Consider . The World Bank’s very useful ietoolkit for Stata has a very flexible command for creating balance tables, iebaltab. You can learn more about how to use it on their Wiki page on the command. .",
    "url": "/Summary_Statistics/Balance_Tables.html",
    "relUrl": "/Summary_Statistics/Balance_Tables.html"
  }
  ,"3": {
    "title": "Contributing",
    "content": "HOW TO CONTRIBUTE . Get a GitHub account. You do not need to know Git to contribute to LOST, but you do need a GitHub account. | Read the Guide to GitHub Markdown which will show the syntax that is used on LOST pages. | Read the below LOST Writing Guide, which shows what a good LOST page looks like from top to bottom. Even if you are just adding another language to an existing page, be sure to read the Implementations section at the bottom. | Explore LOST using the navigation bar on the left, find a page that needs to be expanded, and add more content. Or find one that doesn’t exist but should (perhaps on the Desired Nonexistent Pages list, and write it yourself! Go to the GitHub Repository for LOST to find the appropriate file to edit or folder to create your new file in. | If you are a “Contributor” to the project, you can make your edits and changes directly to the repository. If not, you will need to issue a pull request to get your work on LOST. We will add you as a contributor after your first accepted pull request. If you don’t know Git or how to do a pull request, please post in Issues asking to be added as a contributor so you can edit LOST directly. | If you’ve made a new page, make sure it’s saved as a .md file, put it in the appropriate folder, and add Navigation Information at the top (see below). If you’ve written a Desired Nonexistent Page, be sure to remove it from the list. Or, if your page links to some new nonexistent pages, add those to the list! Also, try to see if other pages have attempted to link to the page you’re working on, and update their links so they go to the right place. | LOST WRITING GUIDE . A LOST page is intended to be a set of instructions for performing a statistical technique, where “statistical technique” is broadly defined as “the things you do in statistical software”, which includes everything from loading data to estimating models to cleaning data to visualization to reproducible notebooks. . After someone reads a LOST page, they should have a decent idea of: . How to implement at least a basic version of what they want to do in the language/software they’re using | What common pitfalls there might be in what they’re doing | What are the pros and cons of meaningfully-different ways of doing the same thing, if relevant | Given what they’re doing, what else should they consider doing (for example, if they’re running a regression discontinuity design, you might suggest they also run a test for bunching on either side of the cutoff) | . Things to remember while writing: . Be as clear as possible. You’re writing a set of instructions, in effect. People should be able to follow them. | The technical ability of the reader may vary by page. People reading a LOST page about how to calculate a mean probably have little experience with their software and will need a lot of hand-holding. You can assume that people reading a LOST page about Markov-Chain Monte Carlo methods probably already have a fairly solid background. | . Markdown . LOST pages are written in Markdown. Markdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for . Syntax highlighted code block # Header 1 ## Header 2 ### Header 3 - Bulleted - List 1. Numbered 2. List **Bold** and _Italic_ and `Code` text [Link](url) and ![Image](src) . For more details see GitHub Flavored Markdown. . Math . Math is rendered with MathJax, which provides support for math formatting. To use on a specific page, make sure that the YAML at the top on the underlying Markdown (i.e. .md) file includes a line saying mathjax: true. This should already be the default on most existing pages, but it is worth emphasising. For example, here is a screenshot of the “Contributing.md” file that you are reading right now. . . After that, equations and other math sections can be delimited with two dollar symbol pairs. For example, $$x = frac{1}{2}$$ is rendered inline as . Similarly, we can render math in display mode (i.e. as a distinct block) by wrapping the dollar symbol pairs on separate lines. For example, . $$ y = beta_0 + beta_1 x + beta_2 x^2 + epsilon $$ . is rendered as display math: . While we don’t include such examples here, note that standard math environments such as begin{equation} ... end{equation} (for numbered equations) and begin{align} ... end{align} (for aligned equation systems) are all supported. Just remember to wrap them between a pair of dollar symbols. More information about MathJax can be found here. . STRUCTURE OF A LOST PAGE . When starting a LOST page, you should copy the New Page Template. There are four main sections of a LOST page: . Navigation Information . Your page will begin with what’s known as YAML, i.e. something that looks like this: . title: Observation level parent: Data Manipulation has_children: false nav_order: 1 mathjax: true . You don’t need to worry too much about YAML syntax (here’s the Wikipedia entry for those interested). The important thing is that the YAML provides a set of very basic instructions for the website navigation and page structure. Make sure to fill in the title with a relevant and brief title. Also be sure to put the appropriate name for the parent — this will ensure that your page shows up in the appropriate spot in the navigation structure. Options for parent include: . Data Manipulation | Geo-Spatial | Machine Learning | Model Estimation | Presentation | Summary Statistics | Time Series | Other | . For the most part, you should generally ignore has_children. (An exception is if you are creating a new section that does have new child pages, but then you are probably better off filing an issue with us to make sure this is done correctly.) You can also ignore nav_order — leaving this at 1 for everything will put everything in alphabetical order. . Introduction . This is an introduction to the technique. Most of the time this will be just a few sentences about what it is and does, and perhaps why it is used. However, in cases of more niche or complex material, there may be a reason to include more detailed information or general non-language-specific instructions here. In general, however, for more detailed explanations or discussions of statistical properties, you can always just link to an outside trusted source like Wikipedia or a (non-paywalled) academic paper. . Keep in Mind . This is a list of details and reminders for people using the method, especially if they are not yet an expert at it or if the detail is not well-known. This may include: . Important assumptions that an estimation method makes. | Notes about interpreting the results. | Settings where the technique seems like it might be a good idea, but actually isn’t. | Features of the technique that might surprise users or be unexpected. | Rules of thumb for use (“you will want to set the number of bootstrap samples to at least 1,000 (citation)”) | . Also Consider . This is a list of other techniques that are commonly used in addition to this page’s technique, or as an alternative to this page’s technique. If not obvious, include a very brief explanation of why you might want to use that other technique in addition to/instead of the current one. Note that you can link to another LOST page even if that page doesn’t exist yet. Maybe it will inspite someone to write it! . For example, pages about estimation techniques might list standard robustness tests to be used in addition to the technique, or adjustments to standard errors they might want to use. A page about a data visualization technique might include a link to a page about setting color palettes to be used in addition. . Or, they might list an alternative technique that might be used if a certain assumption fails (“This technique requires continuous variables. So if your data is discrete, use this other method.”). . To link to other LOST pages (even if they don’t exist yet — don’t forget to add these to Desired Nonexistent Pages!, use the Markdown [Link name](url) structure, where the URL is of the format https://lost-stats.github.io/Category_Name/page_name.html. . Implementations . Implementations contains multiple subsections, one for each statistical software environment/programming language that the technique can be implemented in. . Implementations should be listed in alphabetical order of software/language. eViews, then Python, then R, then SAS, then Stata, etc. | For each language, include well-commented and as-brief-as-reasonably-possible example code that provides an example of performing the technique. Readers should be able to copy the code and have it run the technique from beginning to end, including steps like loading in data if necessary. See existing pages for examples. | If someone else on the internet has already written a clear, thorough, and general implementation example, then linking to it is perfectly fine! This includes StackOverflow/StackExchange answers, which you can link to using the share button. Extremely long demonstrations for super-complex methods may be better left as links only (perhaps with a tiny example pulled out and put on LOST). If the example is short enough, though, including the example directly in the LOST page is preferable, with link attribution of the source, so readers don’t have to go elsewhere. | Avoid creating a long list of examples showing every variant or optional setting of the technique. Instead, focus on one main example, with variants included only if they are especially important. If you like, you can mention in comments additional useful options/settings the reader might want to look into and what they do. | If the technique requires that a package or library be installed, include the code for installing the package in a comment (or if you are using a language where libraries cannot be installed inside the code, include a comment directing the user to install the library). | If a given language has multiple ways of performing the same technique, ideally report only one “best” method, whatever that might be. If other methods are only different in trivial ways, then you can describe them as being alternatives, but avoid providing examples for them. If other methods are different in important ways, then include an example for each, with text explanations of what is different about them. If two contributors seriously disagree about which way is best, then they’re probably different in some meaningful way so you can include both as long as you can explain what that difference is. | It is fine to add implementations for software that only has a graphical interface rather than code (such as Excel) using screenshots. Be sure to keep images well-cropped and small so they don’t crowd the page. If your graphical instructions are necessarily very long, consider posting them as a blog post somewhere and just put a link to that post in Implementations. | . Images . Images can be added to Implementation sections if relevant, for example if you’re working with GUI-only software, or demonstrating the output of a data visualization technique. . How can you add these images? You can upload the images somewhere on the internet that allows image linking, and include the image in your instructions with ![Image](src). Ideally, upload the image directly to the Images/name-of-your-page/ subfolder of whatever directory you’re working in, and link to the images there. . Please be sure to add alt text to images for sight-impaired users. Image filenames should make reference to the language used to make them, i.e. python_scatterplot.png. . Data . Ideally, the same data set will be uploaded to LOST directly in a format accessible by many languages (like CSV) in the Data/name-of-your-page/ subfolder of whatever directory you’re wokring in, and then that data can be used for implementation in all languages on the page. This is not required, but is encouraged. . FREQUENTLY ASKED QUESTIONS . What techniques are important enough to be their own page? This is a little subjective, but if you’re writing about X, which is a minor option/variant of Y, then you can just include it on the Y page. If X is a different technique or a variant of Y that is used in different circumstances or produces meaningfully different output, then give X its own page. | How should I title my page? Pick a single, concise description of the technique you’re talking about. If there are multiple ways to refer to the technique you’re doing, pick one. You will also need to select a file name, which should be in lower_case_with_underscores.md and you might want to make a bit shorter. So Ordinary Least Squares (Linear Regression) might be the title and H1 heading, and ordinary_least_squares.md might be the file name. | What languages can I include in Implementations? Any language is valid as long as it’s something people actually do statistical analysis in. Don’t include something just because you can (I mean, you can technically do OLS in assembly but is that useful for anyone?), but because you think someone will find it useful. | Should I include the output of my code? For data visualization, yes! Just keep the images relatively small so they don’t crowd the page. See the Implementations section above for how to add images. If your output is not visual, there’s probably no need to include output unless you think that it is especially important for some technique. | How can I discuss what I’m doing with other contributors? Head to the Issues page and find (or post) a thread with the title of the page you’re talking about. | How can I [add an image/link to another LOST page/add an external link/bold text] in the LOST wiki? See the Markdown section above. | I want to contribute but I do not like all the rules and structure on this page. I don’t even want my FAQ entry to be a question. Just let me write what I want. If you have valuable knowledge about statistical techniques to share with people and are able to explain things clearly, I don’t want to stop you. So go for it. Maybe post something in Issues when you’re done and perhaps someone else will help make your page more consistent with the rest of the Wiki. I mean, it would be nicer if you did that yourself, but hey, we all have different strengths, right? | .",
    "url": "/Contributing/Contributing.html",
    "relUrl": "/Contributing/Contributing.html"
  }
  ,"4": {
    "title": "Model Estimation",
    "content": "Model Estimation .",
    "url": "/Model_Estimation/Estimation.html",
    "relUrl": "/Model_Estimation/Estimation.html"
  }
  ,"5": {
    "title": "Geo-Spatial",
    "content": "Geo-Spatial .",
    "url": "/Geo-Spatial/Geo-spatial.html",
    "relUrl": "/Geo-Spatial/Geo-spatial.html"
  }
  ,"6": {
    "title": "Machine Learning",
    "content": "Machine Learning .",
    "url": "/Machine_Learning/Machine_Learning.html",
    "relUrl": "/Machine_Learning/Machine_Learning.html"
  }
  ,"7": {
    "title": "Title of page",
    "content": "Name of Page . INTRODUCTION SECTION . Remember that you can use inline math, e.g. . In general, you should render variables in math mode (, , etc.) . You can also render math in display mode: . Keep in Mind . LIST OF IMPORTANT THINGS TO REMEMBER ABOUT USING THE TECHNIQUE | . Also Consider . LIST OF OTHER TECHNIQUES THAT WILL COMMONLY BE USED ALONGSIDE THIS PAGE’S TECHNIQUE | (E.G. LINEAR REGRESSION LINKS TO ROBUST STANDARD ERRORS), | OR INSTEAD OF THIS TECHNIQUE | (E.G. PIE CHART LINKS TO A BAR PLOT AS AN ALTERNATIVE) | WITH EXPLANATION | INCLUDE LINKS TO OTHER LOST PAGES WITH THE FORMAT Description. Categories include Data_Manipulation, Geo-Spatial, Machine_Learning, Model_Estimation, Presentation, Summary_Statistics, Time_Series, and Other | . Implementations . NAME OF LANGUAGE/SOFTWARE 1 . identifier for language type, see this page: https://github.com/jmm/gfm-lang-ids/wiki/GitHub-Flavored-Markdown-%28GFM%29-language-IDs Commented code demonstrating the technique . ## NAME OF LANGUAGE/SOFTWARE 2 WHICH HAS MULTIPLE APPROACHES There are two ways to perform this technique in language/software 2. First, explanation of what is different about the first way: identifier for language type, see this page: https://github.com/jmm/gfm-lang-ids/wiki/GitHub-Flavored-Markdown-%28GFM%29-language-IDs Commented code demonstrating the technique . Second, explanation of what is different about the second way: . identifier for language type, see this page: https://github.com/jmm/gfm-lang-ids/wiki/GitHub-Flavored-Markdown-%28GFM%29-language-IDs Commented code demonstrating the technique .",
    "url": "/NewPageTemplate.html",
    "relUrl": "/NewPageTemplate.html"
  }
  ,"8": {
    "title": "Other",
    "content": "Other .",
    "url": "/Other/Other.html",
    "relUrl": "/Other/Other.html"
  }
  ,"9": {
    "title": "Presentation",
    "content": "Presentation .",
    "url": "/Presentation/Presentation.html",
    "relUrl": "/Presentation/Presentation.html"
  }
  ,"10": {
    "title": "Summary Statistics",
    "content": "Summary Statistics .",
    "url": "/Summary_Statistics/Summary_Statistics.html",
    "relUrl": "/Summary_Statistics/Summary_Statistics.html"
  }
  ,"11": {
    "title": "Summary Statistics Tables",
    "content": "Summary Statistics Tables . Before looking at relationships between variables, it is generally a good idea to show a reader what the distributions of individual variables look like. A common way to do this, which allows you to show information about many variables at once, is a “Summary statistics table” or “descriptive statistics table” in which each row is one variable in your data, and the columns include things like number of observations, mean, median, standard deviation, and range. . Keep in Mind . Make sure that you are using the appropriate summary measures for the variables that you have. For example, if you have a variable indicating the country someone is from coded as that country’s international calling code, don’t include it in a table that reports the mean - you’d get an answer but that answer wouldn’t make any sense. | If you have categorical variables, you can generally still incorporate them into a summary statistics table by turning them into binary “dummy” variables. | . Also Consider . Graphs can be more informative ways of showing the distribution of a variable, and you may want to show a graph of your variable’s distribution in addition to its inclusion on a summary statistics table. There are many ways to do this, but two common ones are density plots or histograms for continuous variables, or bar plots for categorical variables. | . Implementations . R . Probably the most straightforward and simplest way to do a summary statistics table in R is with the stargazer package, which also has many options for customization. There are also other options like summary_table() in qwraps2 or table1() in table1, both of which have more cross-tabulation and categorical-variable functionality but require more work to set up. See this page for a comparison of different packages other than stargazer. . # If necessary # install.packages(&#39;stargazer&#39;) library(stargazer) data(mtcars) # Feed stargazer a data.frame with the variables you want summarized mt_tosum &lt;- mtcars[,c(&#39;mpg&#39;,&#39;cyl&#39;,&#39;disp&#39;)] # Type = &#39;text&#39; to print the table to screen, or &#39;latex&#39; or &#39;html&#39; to get LaTeX or HTML tables stargazer(mt_tosum, type = &#39;text&#39;) # There are *many* options and customizations. For all of them, see # help(stargazer) # Some useful ones include out, which designates a file to send the table to # (note that HTML tables can be copied straight into Word from an output file) stargazer(mt_tosum, type = &#39;html&#39;, out = &#39;my_summary.html&#39;, median = TRUE) # Also note that stargazer does not accept tibbles. # Use as.data.frame() to stargazer a tibble library(tidyverse) data(&quot;storms&quot;) storms %&gt;% select(year, wind, pressure, ts_diameter) %&gt;% as.data.frame() %&gt;% stargazer(type = &#39;text&#39;) . Stata . The built-in Stata command summarize (which can be referred to in short as su or summ) easily creates summary statistics tables. However, while summarize is well-suited for viewing descriptive statistics on your own, it is not well-suited for making tables to publish in a paper, since it is difficult to limit the number of significant digits, and does not offer an easy way to export the table other than selecting the Stata output, selecting “Copy Table”, and pasting into a spreadsheet. . For more flexible tables that can be easily exported, we will be using the highly flexible estout package. For more information on the many different options and specifications for estout summary tables, see this page. We will also see how to use outreg2 in the outreg2 package, which is less flexible but is slightly less work to use for standard tables that are basically summarize but nicer-looking and output to a file. . * If necessary * ssc install estout * ssc install outreg2 sysuse auto.dta, clear * summarize will give us a table that is great for our own purposes, not so much for exporting summarize price mpg rep78 i.foreign * Instead using estpost summarize will give us an esttab-compatible table * Note that factor variables no longer work - we must make dummies by hand xi i.foreign, pre(f_) noomit estpost summarize price mpg rep78 f_* * We can then use esttab and cells() to pick columns * Now it&#39;s nicely formatted * The quotes around the statistics put all the statistics in one row esttab, cells(&quot;count mean sd min max&quot;) * If we want to limit the number of significant digits we must do this stat by stat * Using a standard format option (see help format) esttab, cells(&quot;count mean(fmt(%9.2f)) sd(fmt(%9.2f)) min max&quot;) * And write out to file with &quot;using&quot; esttab using mytable.rtf, cells(&quot;count mean(fmt(%9.2f)) sd(fmt(%9.2f)) min max&quot;) replace * Or we can work with outreg2 * First, limit the data to the variables we want to summarize preserve keep price mpg rep78 f_* * Then outreg2 with the sum(log) option to get summary statistics outreg2 using myoutreg2table.doc, word sum(log) replace * Defaults are very similar to what you&#39;d get with summarize, but you can do things like change * number of significant digits with dec(), or which stats are in there with eqkeep() outreg2 using mysmalltable.doc, word sum(log) eqkeep(N mean) dec(3) replace restore .",
    "url": "/Summary_Statistics/Summary_Statistics_Tables.html",
    "relUrl": "/Summary_Statistics/Summary_Statistics_Tables.html"
  }
  ,"12": {
    "title": "Time Series",
    "content": "Time Series .",
    "url": "/Time_Series/Time_Series.html",
    "relUrl": "/Time_Series/Time_Series.html"
  }
  ,"13": {
    "title": "Bar Graphs",
    "content": "Introduction . This is a brief tutorial on how to make bar graphs. It also provides a little information on how to stylize bar graphs to make them look better. There are a plethora of options to make a bar graph look like the visualization that you want it to. Lets dive in! . Implementations . R . For the R demonstration, we will be calling the tidyverse package. . if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load(tidyverse) . This tutorial will use a dataset that already exists in R, so no need to load any new data into your environment. The dataset we will use is called starwars, which uses data collected from the Star Wars Universe. The tidyverse package uses ggplot2 to construct bar graphs. For our first example, let’s look at species’ appearences in Star Wars movies. Follow along below! . First for our graph, we need write a line that calls ggplot. However we just use ‘ggplot’ to do so. Note the + after ggplot(). This + ties the subsequent lines together to form the graph. A common error when making any type of graph in ggplot() is to forget these + symbols at the end of a code line, so just remember to use them! | There are a couple of steps to construct a bar graph. First we need to specify the data we want to visulaize. We are making a bar graph, so we will use geom_bar. Since we want to use the &#39;starwars&#39; dataset, we set data = starwars. Remember the comma after this, otherwise an error will appear. | Next we want to tell ggplot what we want to map. We use the mapping function to do this. We set mapping to the aesthetic function. (mapping = aes(x = species)) Within the aes function we want to specify what we want our x value to be, in this case species. Copy the code below to make your first bar graph! | . ggplot() + geom_bar(data = starwars, mapping = aes(x = species)) . . As you can see, there are some issues. We can’t tell what the individual species are on the x axis. We also might want to give our graph a title, maybe give it some color, etc. How do we do this? By adding additional functions to our graph! . ggplot(data = starwars) + geom_bar( mapping = aes(x = species), color = &quot;black&quot;, fill = &quot;blue&quot;) + labs(x = &quot;Species&quot;, y = &quot;Total&quot;, title = &quot;Character Appearences in Movies by Species&quot;) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) . . This graph looks much more interpretable to me, though appearences are subjective. Let’s look at what we did. First there are two additional parts to our mapping function, color and fill. The “color = ” provides an outline color to the bars on the graph, while “fill = ” provides the color within the bars. The x and y axis have been renamed, and the graph has been given a title. This was done using the labs() function in R. This function has additional options as well which you should explore. Finally we come to the theme() function in ggplot2. theme() has many options to customize any type of graph in R. For this basic tutorial, the x values (species) have been rotated so that they are legible compared to our first graph. Congratualtions, you have made your first bar graph in R! . There is a similar ggplot() function in R called geom_col. In geom_col, you can specify what you want the y axis to be, whereas geom_bar is only a count. Want more information on how to customize your graph? The Hadley Wickam book called R for Data Science is a fantastic place to start, and best of all it’s free! . Stata . Stata, like R, also has pre-installed datasets available for use. To find them, click on ‘file’, then click on ‘Example Datasets’ which will open up a new window. Under ‘Description’ click on the link for ‘Example datasets installed with Stata’ which will bring up a list of datasets to use for examples. For the purposes of this demonstration we will use the &#39;bplong.dta&#39; option. To load it into stata, click ‘use’ and it will appear in Stata. . This is fictionalized blood pressure data. In your variables column you should have five variables (patient, sex, agegrp, when, bp). Let’s make a bar chart that looks at the patients within our dataset by gender and age. To make a bar chart type into your stata command console: . graph bar, over(sex) over(agegrp) . and the following output should appear in another window. . . Congratulations, you’ve made your first bar chart in Stata! We can now visually see the make-up of our dataset by gender and age. We might want to change the axis labels or give this a title. To do so type the following in your command window: . graph bar, over(sex) over(agegrp) title(Our Graph) ytitle(Percent) . and the following graph shoud appear . . Notice we gave our graph a title and capitalized the y axis. Lets add some color next. To do so type . graph bar, over(sex) over(agegrp) title(Our Graph) ytitle(Percent) bar(1, fcolor(red)) bar(2, fcolor(blue)) . and the following graph should appear . . Our bars are now red with a blue outline. Pretty neat! There are many sources of Stata help on the internet and many different way to customize your bar graphs. There is an official Stata support page that can answer queries regarding Stata. .",
    "url": "/Presentation/bar_graphs.html",
    "relUrl": "/Presentation/bar_graphs.html"
  }
  ,"14": {
    "title": "Bootstrap Standard Errors",
    "content": "Bootstrap Standard Errors . Boostrapping is a statistical method that uses random sampling with replacement to determine the sampling variation of an estimate. If you have a data set of size , then (in its simplest form) a “bootstrap sample” is a data set that randomly selects rows from the original data, perhaps taking the same row multiple times. For more information, see Wikipedia. . Bootstrap is commonly used to calculate standard errors. If you produce many bootstrap samples and calculate a statistic in each of them, then under certain conditions, the distribution of that statistic across the bootstrap samples is the sampling distribution of that statistic. So the standard deviation of the statistic across bootstrap samples can be used as an estimate of standard error. This approach is generally used in cases where calculating the standard error of a statistic parametrically would be too difficult or impossible. . Keep in Mind . Although it feels entirely data-driven, bootstrap standard errors rely on assumptions just like everything else. It assumes your original model is correctly specified, for example. Basic bootstrapping assumes no heteroskedasticity, and otherwise independent error terms. | Bootstrapping can also be used to calculate other features of the parameter’s sample distribution, like the percentile, not just the standard error. | . Also Consider . This page will consider the simplest approach to bootstrapping (the basic resampling of rows), but there are many others, such as cluster (strata) bootstrap, Bayesian bootstrap, and Wild bootstrap. For more information, see Wikipedia. Check the help files of the bootstrap package you’re using to see if they support these approaches. | Bootstrap is relatively straightforward to program yourself: resample, calculate, repeat, and then look at the distribution. If your reason for doing bootstrap is because you want your standard errors to reflect an unusual sampling or data manipulation procedure, for example, you may be best off programming your own routine. | This page contains a general approach to bootstrap, but for some statistical procedures, bootstrap standard errors are common enough that the command itself has an option to produce bootstrap standard errors. If this option is available, it is likely superior. | . Implementations . R . The standard approach to bootstrapping standard errors in R is to use the boot package. . # If necessary # install.packages(&#39;boot&#39;,&#39;broom&#39;,&#39;stargazer&#39;) # Load boot library library(boot) # Use mtcars data data(mtcars) # Create function that takes # A dataset and indices as input, and then # performs analysis and returns a parameter of interest regboot &lt;- function(data, indices) { m1 &lt;- lm(hp~mpg + cyl, data = data[indices,]) return(coefficients(m1)) } # Call boot() function using the function we just made with 200 bootstrap samples # Note the option for stratified resampling with &quot;strata&quot;, in addition to other options # in help(boot) boot_results &lt;- boot(mtcars, regboot, R = 200) # See results boot_results plot(boot_results) # There are lots of diagnostics you can look at at this point, # see https://statweb.stanford.edu/~tibs/sta305files/FoxOnBootingRegInR.pdf # Optional: print regression table with the bootstrap SEs # This uses stargazer, but the method is similar # with other table-making packages, # see https://lost-stats.github.io/Presentation/export_a_formatted_regression_table.html library(broom) tidy_results &lt;- tidy(boot_results) library(stargazer) m1 &lt;- lm(hp~mpg + cyl, data = mtcars) stargazer(m1, se = list(tidy_results$std.error), type = &#39;text&#39;) . Stata . Many commands in Stata come with a vce(bootstrap) option, which will implement bootstrap standard errors. . * Load auto data sysuse auto.dta, clear * Run a regression with bootstrap SEs reg mpg weight length, vce(bootstrap) * see help bootstrap to adjust options like number of samples * or strata reg mpg weight length, vce(bootstrap, reps(200)) * If a command does not support vce(bootstrap), there&#39;s a good chance it will * work with a bootstrap: prefix, which works similarly bootstrap, reps(200): reg mpg weight length .",
    "url": "/Model_Estimation/Nonstandard_Errors/bootstrap_se.html",
    "relUrl": "/Model_Estimation/Nonstandard_Errors/bootstrap_se.html"
  }
  ,"15": {
    "title": "Causal Forest",
    "content": "Causal Forest . Causal forests are a causal inference learning method that are an extension of Random Forests. In random forests, the data is repeatedly split in order to minimize prediction error of an outcome variable. Causal forests are built similarly, except that instead of minimizing prediction error, data is split in order to maximize the difference across splits in the relationship between an outcome variable and a “treatment” variable. This is intended to uncover how treatment effects vary across a sample. . For more information, see Explicitly Optimizing on Causal Effects via the Causal Forest. . Keep in Mind . Causal forests simply uncover heterogeneity in a causal effect, they do not by themselves make the effect causal. A standard causal forest must assume that the assignment to treatment is exogenous, as it might be in a randomized controlled trial. Some extensions of causal forest may allow for covariate adjustment or for instrumental variables. See your causal forest package’s documentation to see if it has an option for ways of identifying the causal effect when treatment is not exogenous such as conditional adjustment or “instrumental forest”. | If using causal forest to estimate confidence intervals for the effects, in addition to the effects itself, it is recommended that you increase the number of trees generated considerably. | . Also Consider . Your intuition for how causal forest works can be based on a thorough understanding of Random Forests, for which materials are much more widely available. | . Implementations . R . The grf package has a causal_forest function that can be used to estimate causal forests. Additional functions afterwards can estimate, for example, the average_treatment_effect(). See help(package=&#39;grf&#39;) for more options. . # If necessary # install.packages(&#39;grf&#39;) library(grf) # Get crime data from North Carolina df &lt;- read.csv(&#39;https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Crime.csv&#39;) # It&#39;s not, but let&#39;s pretend that &quot;percentage of young males&quot; pctymle is exogenous # and see how the effect of it on crmrte varies across the other measured covariates # Make sure the data has no missing values. Here I&#39;m dropping observations # with missing values in any variable, but you can limit the data first to just # variables used in analysis to only drop observations with missing values in those variables df &lt;- df[complete.cases(df),] # Let&#39;s use training and holdout data split &lt;- sample(c(FALSE, TRUE), nrow(df), replace = TRUE) df.train &lt;- df[split,] df.hold &lt;- df[!split,] # Isolate the &quot;treatment&quot; as a matrix pctymle &lt;- as.matrix(df.train$pctymle) # Isolate the outcome as a matrix crmrte &lt;- as.matrix(df.train$crmrte) # Use model.matrix to get our predictor matrix # We might also consider adding interaction terms X &lt;- model.matrix(lm(crmrte ~ -1 + factor(year) + prbarr + prbconv + prbpris + avgsen + polpc + density + taxpc + factor(region) + factor(smsa) + pctmin + wcon, data = df.train)) # Estimate causal forest cf &lt;- causal_forest(X,crmrte,pctymle) # Get predicted causal effects for each observation effects &lt;- predict(cf)$predictions # And use holdout X&#39;s for prediction X.hold &lt;- model.matrix(lm(crmrte ~ -1 + factor(year) + prbarr + prbconv + prbpris + avgsen + polpc + density + taxpc + factor(region) + factor(smsa) + pctmin + wcon, data = df.hold)) # And get effects effects.hold &lt;- predict(cf, X.hold)$predictions # Get standard errors for the holding data predictions - we probably should have set the num.trees # option in causal_forest higher before doing this, perhaps to 5000. SEs &lt;- sqrt(predict(cf, X.hold, estimate.variance = TRUE)$variance.estimates) . Stata . The MLRtime package allows the causal_forest function in the R grf package to be run from inside of Stata. This does require that R be installed. . * If necessary, install MLRtime * net install MLRtime, from(&quot;https://raw.githubusercontent.com/NickCH-K/MLRtime/master/&quot;) * Then, before use, install R from R-project.org * and run the MLRtimesetup function * MLRtimesetup, go * Start a fresh R session rcall clear * Get crime data from North Carolina import delimited using &quot;https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Crime.csv&quot;, clear * Turn character variables numeric so we can use them encode region, g(regionn) encode smsa, g(smsan) drop region smsa * It&#39;s not, but let&#39;s pretend that &quot;percentage of young males&quot; pctymle is exogenous * and see how the effect of it on crmrte varies across the other measured covariates * Let&#39;s use training and holdout data by sending our holdout data to R with rcall g split = runiform() &gt; .5 preserve * Keep the predictors from the holding data, send it over, so later we can make an X matrix to predict with keep if split == 0 keep year prbarr prbconv prbpris avgsen polpc density taxpc regionn smsan pctmin wcon * R needs that data pre-processed! So using the same variables as in the main model, process the variables fvrevar year prbarr prbconv prbpris avgsen polpc density taxpc i.regionn i.smsan pctmin wcon keep `r(varlist)&#39; * Then send the data to R rcall: df.hold &lt;- st.data() restore * Now go back to just the training data * Run causal_forest, storing the effect predictions for the training data in the &quot;effects&quot; variable * the SEs of those effects in effectSE * And the effects and SEs for the holdout data in matrices called effects_hold and effectSE_hold causal_forest crmrte pctymle year prbarr prbconv prbpris avgsen polpc density taxpc i.regionn i.smsan pctmin wcon, pred(effects) varreturn(effectSE = sqrt(predict(CF, X, estimate.variance = TRUE)@@variance.estimates)) return(effects_hold = predict(CF, as.matrix(df.hold))@@predictions; effectSE_hold = sqrt(predict(CF, as.matrix(df.hold), estimate.variance = TRUE)@@variance.estimates)) * Look at the holdout effects predicted di &quot;`r(effects_hold)&#39;&quot; .",
    "url": "/Machine_Learning/causal_forest.html",
    "relUrl": "/Machine_Learning/causal_forest.html"
  }
  ,"16": {
    "title": "Cluster-Robust Standard Errors",
    "content": "Cluster-Robust Standard Errors (a.k.a. Clustered Standard Errors) . Data is considered to be clustered when there are subsamples within the data that are related to each other. For example, if you had data on test scores in a school, those scores might be correlated within classroom because classrooms share the same teacher. When error terms are correlated within clusters but independent across clusters, then regular standard errors, which assume independence between all observations, will be incorrect. Cluster-robust standard errors are designed to allow for correlation between observations within cluster. For more information, see A Practitioner’s Guide to Cluster-Robust Inference. . Keep in Mind . Just because there are likely to be clusters in your data is not necessarily a good justification for using cluster-robust inference. Generally, clustering is advised only if either sampling or treatment assignment is performed at the level of the clusters. See Abadie, Athey, Imbens, &amp; Wooldridge (2017), or this simple summary of the paper. | There are multiple kinds of cluster-robust standard errors, for example CR0, CR1, and CR2. Check in to the kind available to you in the commands you’re using. | . Also Consider . Cluster Bootstrap Standard Errors, which are another way of performing cluster-robust inference that will work even outside of a standard regression context. | . Implementations . Note: Clustering of standard errors is especially common in panel models, such as linear fixed effects. For this reason, software routines for these particular models typically offer built-in support for (multiway) clustering. The implementation pages for these models should be hyperlinked in the relevant places below. Here, we instead concentrate on providing implementation guidelines for clustering in general. . Julia . For cluster-robust estimation of (high-dimensional) fixed effect models in Julia, see here. . R . For cluster-robust estimation of (high-dimensional) fixed effect models in R, see here. . Cluster-robust standard errors for many different kinds of regression objects in R can be obtained using the coeftest function in the lmtest package combined with the vcovCL function in the sandwich package. Alternately, while it does not handle as many types of regressions, the lm_robust function in estimatr can provide cluster-robust standard errors much more easily. . # If necessary, install lmtest, sandwich, and estimatr # install.packages(c(&#39;lmtest&#39;,&#39;sandwich&#39;,&#39;estimatr&#39;)) # Read in data from the College Scorecard df &lt;- read.csv(&#39;https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv&#39;) # Get robust errors using vcovCL and lmtest library(lmtest) library(sandwich) # Create a regression model without cluster-robust standard errors my_model &lt;- lm(repay_rate ~ earnings_med + state_abbr, data = df) # Put the model into vcovCL() to get a robust covariance matrix # and then put that in lmtest() to get the regression results with robust errors # Pick the kind of robust errors with &quot;type&quot; # It refers to the heteroskedasticity-consistent error types. # HC1 is the default but I&#39;ve specified it here anyway coeftest(my_model, vcov = vcovCL(my_model, cluster = df$inst_name, type = &quot;HC1&quot;)) # Alternately, just use lm_robust. # Standard error types are referred to as CR0, CR1 (&quot;stata&quot;), CR2 here. # Here, CR2 is the default library(estimatr) my_model2 &lt;- lm_robust(repay_rate ~ earnings_med + state_abbr, data = df, clusters = inst_name, se_type = &quot;stata&quot;) summary(my_model2) . Stata . Stata has clustered standard errors built into most regression commands, and they generally work the same way for all commands. . * Load in College Scorecard data import delimited &quot;https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv&quot;, clear * The missings are written as &quot;NA&quot;, let&#39;s turn this numeric destring earnings_med repay_rate, replace force * If we want to cluster on a variable or include it as a factor it must not be a string encode inst_name, g(inst_name_encoded) encode state_abbr, g(state_encoded) * Just add vce(cluster) to the options of the regression * This will give you CR1 regress repay_rate earnings_med i.state_encoded, vce(cluster inst_name_encoded) .",
    "url": "/Model_Estimation/Nonstandard_Errors/clustered_se.html",
    "relUrl": "/Model_Estimation/Nonstandard_Errors/clustered_se.html"
  }
  ,"17": {
    "title": "Collapse a Data Set",
    "content": "Collapse a Data Set . The observation level of a data set is the set of case-identifying variables which, in combination, uniquely identify every row of the data set. For example, in the data set . I J X . 1 | 1 | 3 | . 1 | 2 | 3.5 | . 2 | 1 | 2 | . 2 | 2 | 4.5 | . the variables and uniquely identify rows. The first row has and , and there is no other row with that combination. We could also say that uniquely identifies rows, but in this example is not a case-identifying variable, it’s actual data. . It is common to want to collapse a data set from one level to another, coarser level. For example, perhaps instead of one row per combination of and , we simply want one row per , perhaps with the average across all observations. This would result in: . I X . 1 | 3.25 | . 2 | 3.25 | . This can be one useful way to produce summary statistics, but can also be used to rid the data of unnecessary or unusable detail, or to change one data set to match the observation level of another. . Keep in Mind . Collapsing a data set almost by definition requires losing some information. Make sure that you actually want to lose this information, rather than, for example, doing a horizontal merge, which can match data sets with different observation levels without losing information. | Make sure that, for each variable you plan to retain in your new, collapsed data, you know the correct procedure that should be used to figure out the new, summarized value. Should the collapsed data for variable use the mean of all the observations you started with? The median? The mode? The first value found in the data? Think through these decisions. | . Also Consider . For more information about observation levels and how to determine what the current observation level is, see determine the observation level of a data set. | . Implementations . R . # If necessary, install dplyr # install.packages(&#39;dplyr&#39;) library(dplyr) # Get data on storms from dplyr data(&quot;storms&quot;) # We would like each storm to be identified by # name, year, month, and day # However, currently, they are also identified by hour, # And even then there are sometimes multiple observations per hour # To construct the collapsed data, we start with the original storms_collapsed &lt;- storms %&gt;% # group by the variables that make the new observation level group_by(name, year, month, day) %&gt;% # And use summarize() to pick the variables to keep, as well as # the functions we want to use to collapse each variable. # Let&#39;s get the mean wind and pressure, and the first category value summarize(wind = mean(wind), pressure = mean(pressure), category = first(category)) # Note that if we wanted to collapse every variable in the data with the # same function, we could instead use summarize_all() . Stata . ** Load surface.dta, which contains temperature recordings in different locations sysuse surface.dta, clear * Currently, there is one observation per latitude per longitude per date * I would like this to just be one observation per latitude/longitude * So I construct a collapse command. * I take my new target observation level and put it in by() * and then take each variable I&#39;d like to keep around and tell * Stata what function to use to create the new collapsed value, here (mean) collapse (mean) temperature, by(latitude longitude) .",
    "url": "/Data_Manipulation/collapse_a_data_set.html",
    "relUrl": "/Data_Manipulation/collapse_a_data_set.html"
  }
  ,"18": {
    "title": "Horizontal Combination (Deterministic)",
    "content": "Combining Datasets: Horizontal Combination (Deterministic) . A deterministic merge is when there is some variable(s) that uniquely and dependably identifies individual units (be it people, firms, teams, etc.) across datasets. For example, we might have two datasets containing information about the same set of people, one with their financial information, the other with their educational information. To analyze the relationship between the education and financial measures, we need them in the same dataset and so would want to combine them. If both datasets had a unique identification field for each person, such as a social security number or other national id, we could use this to match the records, so that all information from the same person appeared on the same line. . Because we expect such identifiers to be unique to an individual (unlike many names, such as John Smith) and appear exactly the same in each dataset, we can use just this field to do the match, and don’t anticipate in ambiguity in determining which records match to each other. Thus, it is a deterministic merge. . Keep in mind . For any number of reasons, one or both of the datasets may have more than one observation per unit or individual. That may be for a good reason – such as havinng multiple test scores for the same student because they took exams at different points in time – or it may be redundant information. Understanding the structure of your data is key before embarking on a deterministic merge. | It is a good idea to have a clear sense of how much overlap you anticipate across your datasets. It is important to examine the results of your merge and see if it matches the amount the overlap you expected. Subtle differences in a matching variable (e.g. if leading zeroes are present in an ID variable for one variable but not another) can be a source of major headaches for your analysis if not caught early. If something looks weird in your results later in the project, trouble with a merge is a common cause. So check your merge results early and often. | . Also Consider . Determine the observation level of a data set. | . Implementations . R . There are several ways to combine data sets horizontally in R, including base-R merge and several different approaches in the data.table package. We will be using the join functions in the dplyr package. . # If necessary, install dplyr # install.packages(&#39;dplyr&#39;) library(dplyr) # This data set contains information on GDP in local currency GDP2018 &lt;- data.frame(Country = c(&quot;UK&quot;, &quot;USA&quot;, &quot;France&quot;), Currency = c(&quot;Pound&quot;, &quot;Dollar&quot;, &quot;Euro&quot;), GDPTrillions = c(2.1, 20.58, 2.78)) # This data set contains dollar exchange rates DollarValue2018 &lt;- data.frame(Currency = c(&quot;Euro&quot;, &quot;Pound&quot;, &quot;Yen&quot;, &quot;Dollar&quot;), InDollars = c(1.104, 1.256, .00926, 1)) . Next we want to join together GDP2018 and DollarValue2018 so we can convert all the GDPs to dollars and compare them. There are three kinds of observations we could get - observations in GDP2018 but not DollarValue2018, observations in DollarValue2018 but not GDP2018, and observations in both. Use help(join) to pick the variant of join that keeps the observations we want. The “Yen” observation won’t have a match, and we don’t need to keep it. So let’s do a left_join and list GDP2018 first, so it keeps matched observations, plus any observations only in GDP2018. . GDPandExchange &lt;- left_join(GDP2018, DollarValue2018) . The join function will automatically detect that the Currency variable is shared in both data sets and use it to join them. Generally, you will want to be sure that the set of variables you are joining by uniquely identifies observations in at least one of the data sets you are joining. If you’re not sure whether that’s true, see Determine the observation level of a data set, or run join through the safe_join from the pmdplyr package. . Stata . A Quick Prelude About “Master” And “Using” Datasets . When merging two datasets together, there are two relevant datasets to consider. The first is the one currently in Stata’s memory, the other is whatever dataset (not currently loaded into Stata) that you would like to merge onto the dataset in memory. For ease of reference, Stata calls the dataset in memory the “master” dataset and the other file the “using” dataset. When you see the syntax of the merge command, the reason for calling it the “using” dataset will become clear. . In Stata, there are 3 types of deterministic merges: . 1-to-1 . A one-to-one merge expects there to be no more than one row in each dataset to have a matched pair in the other dataset. If there is more than one observation with the same identifying variable(s) in either the master or using datasets when attempting to do a one-to-one merge, Stata will throw an error. (Note: you can check to see if there is more than one observation per identifying variable by using the “duplicates report” command, or the Gtools variant for especially large datasets, called “gduplicates report.”) . *Import Data with Car Sizes and Merge it to Data on Car Prices using the ID variable “Make” webuse autosize.dta, clear merge 1:1 make using http://www.stata-press.com/data/r14/autoexpense.dta . Note that the syntax specifies “make” as the identifying variable after the merge type (1:1) and before the using statement (thus why we call the data not in memory the “using” data. The result of this merge shows 5 successful matches and one observation from the master dataset that did not have a match. . Result # of obs. _merge value . not matched from master | 1 | (_merge==1) | . not matched from using | 0 | (_merge==2) | .   |   |   | . matched | 5 | (_merge==3) | . Note that Stata creates a new variable (_merge) during the merge that stores the merge status of each observation, where a value of 1 means that the observation was only found in the master dataset, 2 means it was found only in the using dataset, and 3 means it was found in both and successfully merged. . Many-to-1 . A many-to-one merge occurs when the master dataset contains multiple observations of the same unit or individual (say, multiple test scores for the same student), while the using dataset has only one observation per unit or individual (say, the age of each student). Here is the syntax for a many-to-1 merge. . *Import Data with Car Sizes and Merge it to Data on Car Prices using the ID variable “Make” webuse sforce.dta, clear merge m:1 region using http://www.stata-press.com/data/r14/dollars.dta . Note that in this case that the syntax changes from merge 1:1 to merge m:1 where m stands for many. In this case, the identifying variable is “region.” . 1-to-Many . A one-to-many merge is the opposite of a many to one merge, with multiple observations for the same unit or individual in the using rather than the master data. The only different in the syntax is that it becomes merge 1:m rather than merge m:1. . Many-to-Many . A many-to-many merge is intended for use when there are multiple observations for each combination of the set of merging variables in both master and using data. However, merge m:m has strange behavior that is effectively never what you want, and it is not recommended. .",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html",
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html"
  }
  ,"19": {
    "title": "Combining Datasets",
    "content": "Combining Datasets Overview . There are two main ways to combine data: vertically and horizontally. That is, you can want to combine observations (adding new variables) or combine variables (adding new observations). This is perhaps easiest to show visually: . Individual Name Info |Name| ID | |–|–| |John Smith|A63240| |Desiree Thomas|B78242| . Individual Age Info |ID | Age | |–|–| |B78242|22| |A63240|27| . In the case above, we would like to combine two datasets, the Individual Name Info and the Individual Date Info, that have different information about the same people, who are identified by the ID variable. The result from the merge would be to have a new dataset with more columns than the original datasets because it contains all of the information for each individual from both of the original datasets. Here we have to combine the files according to the ID variable, placing the information from observations with the same ID on the same row in the combined dataset. . Alternatively, the below example has two datasets that collect the same information about different people. We would like to combine these datasets vertically, with the result containing more rows than the original dataset, because it contains all of the people that are present in each of the original datasets. Here we combine the files based on the name or position of the columns in the dataset. . Name ID Age . John Smith | A63240 | 22 | . Desiree Thomas | B78242 | 27 | . Name ID Age . Teresa Suarez | Y34208 | 19 | . Donald Akliberti | B72197 | 34 | . These ways of combining data are referred to by different names across different programming languages, but will largely be referred to by one common set of terms (used by Stata and Python’s Pandas): merge for horizontal combination and append for for vertical combination. .",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_overview.html",
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_overview.html"
  }
  ,"20": {
    "title": "Vertical Combination",
    "content": "Combining Datasets: Vertical Combination . When combining two datasets that collect the same information about different people, they get combined vertically because they have variables in common but different observations. The result of this combination will more rows than the original dataset because it contains all of the people that are present in each of the original datasets. Here we combine the files based on the name or position of the columns in the dataset. It is a “vertical” combination in the sense that one set of observations gets added to the bottom of the other set of observations. . Keep in Mind . Vertical combinations require datasets to have variables in common to be of much use. That said, it may not be necessary for the two datasets to have exactly the same variables. Be aware of how your statistical package handles observations for a variable that is in one dataset but not another (e.g. are such observations set to missing?). | It may be the case that the datasets you are combining have the same variables but those variables are stored differently (numeric vs. string storage types). Be aware of how the variables are stored across datasets and how your statistical package handles attempts to combine the same variable with different storage types (e.g. Stata throws an error and will now allow the combination, unless the “, force” option is specified.) | . Implementations . R . There are several ways to vertically combine data sets in R, including rbind. We will use the dplyr package function bind_rows, which allows the two data sets to combine even if they don’t contain the exact same set of variables. . # If necessary, install dplyr # install.packages(&#39;dplyr&#39;) library(dplyr) # Load in mtcars data data(mtcars) # Split it in two, so we can combine them back together mtcars1 &lt;- mtcars[1:10,] mtcars2 &lt;- mtcars[11:32,] # Use bind_rows to vertically combine the data sets mtcarswhole &lt;- bind_rows(mtcars1, mtcars2) . Stata . * Load California Population data webuse http://www.stata-press.com/data/r14/capop.dta // Import data from the web append using http://www.stata-press.com/data/r14/ilpop.dta // Merge on Illinois population data from the web . You can also append multiple datasets at once, by simply listing both datasets separated by a space: . * Load California Population data * Import data from the web webuse http://www.stata-press.com/data/r14/capop.dta * Merge on Illinois and Texas population data from the web append using http://www.stata-press.com/data/r14/ilpop.dta http://www.stata-press.com/data/r14/txpop.dta . Note that, if there are columns in one but not the other of the datasets, Stata will still append the two datasets, but observations from the dataset that did not contain those columns will have their values for that variable set to missing. . * Load Even Number Data webuse odd.dta, clear append using http://www.stata-press.com/data/r14/even.dta .",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html",
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html"
  }
  ,"21": {
    "title": "Creating Time Series Dataset",
    "content": "Introduction . Time-series estimators are, by definition, a function of the temporal ordering of the observations in the estimation sample. So a number of programmed time-series econometric routines can only be used if the software is instructed ahead of time that it is working with a time-series dataset. . Keep in Mind . Date data can be notoriously difficult to work with. Be sure before declaring your data set as a time series that your date variable has been imported properly. . | As an example, we will use data on U.S. quarterly real Gross Domestic Product (GDP). To get an Excel spreadsheet holding the GDP data, go to the Saint Louis Federal Reserve Bank FRED website. . | . Implementations . R . There are many different kinds of time series data set objects in R. Instead of R-based time series objects such as ts, zoo and xts, here we will use tsibble, will preserves time indices as the essential data column and makes heterogeneous data structures possible. . The tsibble package extends the tidyverse to temporal data and built on top of the tibble, and so is a data- and model-oriented object. . For more detail information for using tsibble such as key and index, check the tsibble page and the Introduction to tsibble. . STEP 1) Load necessary packages . # If necessary # install.packages(c(&#39;here&#39;,&#39;tsibble&#39;,&#39;tidyverse&#39;)) library(here) library(tsibble) library(tidyverse) . STEP 2) Import data into R. . gdp &lt;- read.csv(&quot;https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv&quot;) # read.csv() has read in our date variable as a factor. We need a date! gdp$DATE &lt;- as.Date(gdp$DATE) # If it were a little less well-behaved than this, we could use the lubridate package to fix it. . STEP 3) Convert a date variable formats to quarter . gdp_ts &lt;- as_tsibble(gdp, index = DATE, regular = FALSE) %&gt;% index_by(qtr = ~ yearquarter(.)) . By applying yearmonth() to the index variable (referred to as .), it creates new variable named qtr with a quarter interval which corresponds to the year-quarter for the original variable DATE. . Since the tsibble handles regularly-spaced temporal data whereas our data (GDPC1) has an irregular time interval (since it’s not the exact same number of days between quarters every time), we set the option regular = FALSE. . Now, we have a quarterly time-series dataset with the new variable date. . References for more information: . If you want to learn how to build various types of time-series forecasting models, Forecasting: Principles and Practice provides very useful information to deal with time-series data in R. | If you need more detail information on tssible, visit the tsibble page or tsibble on RDRR.io. | The fable packages provides a collection of commonly used univariate and multivariate time-series forecasting models. For more information, visit fable. | Stata . STEP 1) Import Data to Stata . import delimited &quot;https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv&quot;, clear . STEP 2) Generate the new date variable . generate date_index = tq(1947q1) + _n-1 . The function tq() converts a date variable for each of the above formats to an integer value (starting point of our data is 1947q1). . _n is a Stata command gives the index number of the current row. . STEP 3) Index the new variable format as quarter . format date_index %tq . This command will format date_index as a vector of quarterly dates which corresponds to our original date variable observation date. . STEP 4) Convert a variable into time-series data . tsset date_index . Now, we have a quarterly Stata time-series dataset. Any data you add to this file in the future will be interpreted as time-series data. .",
    "url": "/Time_Series/creating_time_series_dataset.html",
    "relUrl": "/Time_Series/creating_time_series_dataset.html"
  }
  ,"22": {
    "title": "Data Manipulation",
    "content": "Data Manipulation Techniques .",
    "url": "/Data_Manipulation/data_manipulation.html",
    "relUrl": "/Data_Manipulation/data_manipulation.html"
  }
  ,"23": {
    "title": "Desired Nonexistent Pages",
    "content": "Desired Nonexistent Pages . This is a manually-maintained list of pages that have been linked from elsewhere, or that it would be nice to have, but do not exist yet. Feel free to edit with your own wishes! . A page does not have to be listed here for you to add it! These are just the things that we thought of. There’s certainly plenty more in the world of statistical techniques to be added. . If you create one of these pages, please remove it from this list. . Data Manipulation . Aggregating Statistics | Creating dummy variables | . Model Estimation . Nonlinear Instrumental Variables Estimation | Differences in Differences | Propensity Score Matching | Event Study Estimation | Nonparametric regression | Generalized Method of Moments (GMM) | Cluster Bootstrap Standard Errors | Treatment Effect Model | Endogenous Switching Model | Nonparametric Sample Selection Models | Jackknife Standard Errors | Tobit | Average Marginal Effects | Marginal Effects at the Mean | Conditional Logit | Mixed Logit | Nested Logit | Hierarchical Bayes Conditional Logit | Multilevel Regression with Poststratification | Nonlinear Mixed Effects Models | Ordered Probit | Ordered Logit | Nonlinear hypothesis tests | . Geo-Spatial . Handling Raster Data | Handling Vector Data | Spatial Joins | Spatial Regression Model | . Machine Learning . A-B Testing | Artificial Neural Networks | Random Forest | Decision Trees | Nearest Neighbors Matching | Boosted Regression | . Presentation . Line Graph | Scatterplot | Styling Scatterplots | Marginal Effects Plots for Discrete Variables | Density plots | Histograms | Formatting graph legends | Graph themes | . Summary Statistics . Summary Statistics by Group | Cross-Tabulations | Correlation Matrix | . Time Series . Serial Correlation | Stationarity and Weak Dependence | Granger Causality | Moving Average Model | ARIMA Model | ARIMAX Model | ARCH Model | GARCH Model | TARCH Model | Rolling Regressions | Recursive Regressions | State Dependent Regression | Structural Break (Chow Test) | Dynamic Panel | Ex-Post Forecasting | Ex-Ante Forecasting | . Other . Import a Delimited Data File (CSV, TSV) | Import a Fixed-Width Data File | Export Data to a Foreign Format | Set a Working Directory | Power Analysis of Interaction Terms | .",
    "url": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html",
    "relUrl": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html"
  }
  ,"24": {
    "title": "Determine the Observation Level of a Data Set",
    "content": "Determine the Observation Level of a Data Set . The observation level of a data set is the set of case-identifying variables which, in combination, uniquely identify every row of the data set. For example, in the data set . I J X . 1 | 1 | 3 | . 1 | 2 | 3.5 | . 2 | 1 | 2 | . 2 | 2 | 4.5 | . the variables and uniquely identify rows. The first row has and , and there is no other row with that combination. We could also say that uniquely identifies rows, but in this example is not a case-identifying variable, it’s actual data. . When working with data that has case-identifier variables, like panel data, it’s generally a good idea to know what set of them makes up the observation level of a data set. Otherwise you might perform merges or case-level calculations incorrectly. . Keep in Mind . As in the above example, it’s easy to uniquely identify rows using continuous data. But the goal is to figure out which case-identifying variables, like an individual’s ID code, or a country code, or a time code, uniquely identify rows. Make sure you only try these variables. | Even if you think you know what the observation level is, it’s good to check. Lots of data is poorly behaved! | . Also Consider . You can collapse a data set to switch from one observation level to another, coarser one. | . Implementations . R . # If necessary, install dplyr # install.packages(&#39;dplyr&#39;) # We do not need dplyr to detect the observation level # But we will use it to get data, and for our alternate approach library(dplyr) # Get data on storms from dplyr data(&quot;storms&quot;) # Each storm should be identified by # name, year, month, day, and hour # anyDuplicated will return 0 if there are no duplicate combinations of these # so if we get 0, the variables in c() are our observation level. anyDuplicated(storms[,c(&#39;name&#39;,&#39;year&#39;,&#39;month&#39;,&#39;day&#39;,&#39;hour&#39;)]) # We get 2292, telling us that row 2292 is a duplicate (and possibly others!) # We can pick just the rows that are duplicates of other rows for inspection # (note this won&#39;t get the first time that duplicate shows up, just the subsequent times) duplicated_rows &lt;- storms[duplicated(storms[,c(&#39;name&#39;,&#39;year&#39;,&#39;month&#39;,&#39;day&#39;,&#39;hour&#39;)]),] # Alternately, we can use dplyr storms %&gt;% group_by(name, year, month, day, hour) %&gt;% # Add a variable with the number of times that particular combination shows up mutate(number_duplicates = n()) %&gt;% # Then take that variable out pull(number_duplicates) %&gt;% # And get the maximum of it max() # If the result is 1, then we have found the observation level. If not, we have duplicates. # We can pick out the rows that are duplicated for inspection # by filtering on n(). This approach will give you every time the duplicate appears. duplicated_rows &lt;- storms %&gt;% group_by(name, year, month, day, hour) %&gt;% # Add a variable with the number of times that particular combination shows up filter(n() &gt; 1) . Stata . * Load surface.dta, which contains temperature recordings in different locations sysuse surface.dta, clear * duplicates report followed by a variable list will show how many times * each combination shows up. * I think there is one observation level for each location, so I&#39;ll check that duplicates report latitude longitude * If I am correct, then the only number in the &quot;Copies&quot; column will be 1. * But it looks like I was not correct. * duplicates tag will create a binary variable with 1 for all duplicates * so I can examine the problem more closely * (duplicates examples is another option) duplicates tag latitude longitude, g(duplicated_data) * If I want to know not just whether there are duplicates but how many * of each there are for when I look more closely, I can instead do by latitude longitude, sort: g number_of_duplicates_in_this_group = _N . For especially large datasets the Gtools version of the various duplicates commands, gduplicates, is a great option . * Install gtools if necessary * ssc install gtools * Recreate the two duplicates tasks from above gduplicates report latitude longitude gduplicates tag latitude longitude, g(g_duplicated_data) .",
    "url": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html",
    "relUrl": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html"
  }
  ,"25": {
    "title": "Export a Formatted Regression Table",
    "content": "Export a Formatted Regression Table . Statistical packages often report regression results in a way that is not how you would want to display them in a paper or on a website. Additionally, they rarely provide an option to display multiple regression results in the same table. . Two (bad) options for including regression results in your paper include copying over each desied number by hand, or taking a screenshot of your regression output. Much better is using a command that outputs regression results in a nice format, in a way you can include in your presentation. . Keep in Mind . Any good regression table exporting command should include an option to limit the number of significant digits in your result. You should almost always make use of this option. It is very rare that the seventh or eighth decimal place (commonly reported in statistics packages) is actually meaningful, and it makes it difficult to read your table. | Variable names serve different purposes in statistical coding and in papers. Variable names in papers should be changed to be readable in the language of the paper. So for example, while employment may be recorded as EMP_STAT in your statistics package, you should rename it Employment for your paper. Most table exporting commands include options to perform this renaming. But if it doesn’t, you can always change it by hand after exporting. | If you use asterisks to indicate significance, be sure to check the significance levels that different numbers of asterisks indicate in the command you’re using, as standards for what significance levels the asterisks mean vary across fields (and so vary across commands as well). Most commands include an option to change the significance levels used. On that note, always include a table note saying what the different asterisk indicators mean! These commands should all include one by default - don’t take it out! | . Also Consider . If you are a Word user, and the command you are using does not export to Word or RTF, you can get the table into Word by exporting an HTML, CSV, or LaTeX, then opening up the result in your browser, Excel, or TtH, respectively. Excel and HTML tables can generally be copy/pasted directly into Word (and then formatted within Word). You may at that point want to use Word’s “Convert Text to Table” command, especially if you’ve pasted in HTML. | By necessity, regression-output commands often have about ten million options, and they can’t all be covered on this page. If you want it to do something, it probably can. To reduce errors, it is probably a good idea to do as little formatting and copy/pasting by hand as possible. So if you want to do something it doesn’t do by default, like adding additional calculations, check out the help file for your command to see how you can do it automatically. | . Implementations . R . There are many, many packages for exporting regression results in R, including RStudio’s gt, texreg, and xtable. Here we will focus on two: stargazer, which is probably the easiest to use, and huxtable, which is slightly more up-to-date and offers advanced formatting options, outlined on its website. . # Install stargazer if necessary # install.packages(&#39;stargazer&#39;) library(stargazer) # Get mtcars data data(mtcars) # Let&#39;s give it two regressions to output lm1 &lt;- lm(mpg ~ cyl, data = mtcars) lm2 &lt;- lm(mpg ~ cyl + hp, data = mtcars) # Let&#39;s output an HTML table, perhaps for pasting into Word # We could instead set type = &#39;latex&#39; for LaTeX or type = &#39;text&#39; for a text-only table. stargazer(lm1, lm2, type = &#39;html&#39;, out = &#39;my_reg_table.html&#39;) # In line with good practices, we should use readable names for our variables stargazer(lm1, lm2, type = &#39;html&#39;, out = &#39;my_reg_table.html&#39;, covariate.labels = c(&#39;Cylinders&#39;,&#39;Horsepower&#39;), dep.var.labels = &#39;Miles per Gallon&#39;) . This produces: . | (1) | (2) | . Cylinders | -2.876 *** | -2.265 *** | . | (0.322)&nbsp;&nbsp;&nbsp; | (0.576)&nbsp;&nbsp;&nbsp; | . Horsepower | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | -0.019&nbsp;&nbsp;&nbsp;&nbsp; | . | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | (0.015)&nbsp;&nbsp;&nbsp; | . N | 32&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | 32&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | . R2 | 0.726&nbsp;&nbsp;&nbsp;&nbsp; | 0.741&nbsp;&nbsp;&nbsp;&nbsp; | . logLik | -81.653&nbsp;&nbsp;&nbsp;&nbsp; | -80.781&nbsp;&nbsp;&nbsp;&nbsp; | . AIC | 169.306&nbsp;&nbsp;&nbsp;&nbsp; | 169.562&nbsp;&nbsp;&nbsp;&nbsp; | . *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. | . Now we will do the same thing with huxtable, using mostly defaults. . # Install huxtable and magrittr if necessary # install.packages(&#39;huxtable&#39;, &#39;magrittr&#39;) # huxtable works more easily with the pipe %&gt;% # which can come from magrittr or dplyr or tidyverse, etc. library(huxtable) library(magrittr) # First we build a huxreg object, using readable names huxreg(lm1, lm2, coefs=c(&#39;Cylinders&#39; = &#39;cyl&#39;, &#39;Horsepower&#39; = &#39;hp&#39;)) %&gt;% # We can send it to the screen to view it instantly print_screen() # Or we can send it to a file with the quick_ functions, which can # output to pdf, docx, html, xlsx, pptx, rtf, or latex. huxreg(lm1, lm2, coefs=c(&#39;Cylinders&#39; = &#39;cyl&#39;, &#39;Horsepower&#39; = &#39;hp&#39;)) %&gt;% # Let&#39;s make an HTML file quick_html(file = &#39;my_reg_output.html&#39;) . Which produces (note the different asterisks behavior, which can be changed with huxreg’s stars option): . | . | Dependent variable: | . | | . | Miles per Gallon | . | (1) | (2) | . | . Cylinders | -2.876*** | -2.265*** | . | (0.322) | (0.576) | . | | | . Horsepower | | -0.019 | . | | (0.015) | . | | | . Constant | 37.885*** | 36.908*** | . | (2.074) | (2.191) | . | | | . | . Observations | 32 | 32 | . R2 | 0.726 | 0.741 | . Adjusted R2 | 0.717 | 0.723 | . Residual Std. Error | 3.206 (df = 30) | 3.173 (df = 29) | . F Statistic | 79.561*** (df = 1; 30) | 41.422*** (df = 2; 29) | . | . Note: | *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 | . Stata . There are two main ways of outputting regression results in Stata, both of which must be installed from ssc install: outreg2 and estout. We will use estout here, as it is more flexible. More detail is available on the estout website. . Also note that, in a pinch, if you’re using a strange command that does not play nicely with estout, you can often select any Stata regression output, select the output, right-click, do “Copy Table”, and paste the result into Excel. This is only if all else fails. . * Install estout if necessary * ssc install estout * Load auto data sysuse auto.dta, clear * Let&#39;s provide it two regressions * Making sure to store the results each time reg mpg weight estimates store weightonly reg mpg weight foreign estimates store weightandforeign * Now let&#39;s export the table using estout * while renaming the variables for readability using the variable labels already in Stata * replacing any table we&#39;ve already made * and making an HTML table with style(html) * style(tex) also works, and the default is tab-delimited data for use in Excel. * Note also the default is to display t-statistics in parentheses. If we want * standard errors instead, we say so with se esttab weightonly weightandforeign using my_reg_output.html, label replace style(html) se . Which produces: . . | . | (1) | (2) | . | Mileage (mpg) | Mileage (mpg) | . . | . Weight (lbs.) | -0.00601*** | -0.00659*** | . | (0.000518) | (0.000637) | . &nbsp; | . Car type | | -1.650 | . | | (1.076) | . &nbsp; | . Constant | 39.44*** | 41.68*** | . | (1.614) | (2.166) | . . | . Observations | 74 | 74 | . . | . Standard errors in parentheses* p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001 | .",
    "url": "/Presentation/export_a_formatted_regression_table.html",
    "relUrl": "/Presentation/export_a_formatted_regression_table.html"
  }
  ,"26": {
    "title": "Faceted Graphs",
    "content": "Faceted Graphs . When plotting relationship among variables of interest, one of the useful ways to create visual impact is by way of using facet, which subsets data with faceted variable and creates plots for each of the subset seperately. The result is a panel of subplots, with each subplot depicting the plot for same set of variables. This approach can be especially useful for panel datasets, with the panel variable acting as facet variable and each subplot depicting time series trend of variable of interest. . Keep in Mind . It is important to use a categorical (discrete) variable as a facet variable for creating faceted graphs. | . Also Consider . It is important to know the basic plotting techniques such as Bar Graphs, Line Graphs and Scatterplot before learning about faceted graphs as the facets are an addition to the underlying plot such as bar graph, line graph, scatterplot etc. | . Implementations . R . Implementation of faceted graph in R explained below is taken from online book R for Data Science by Hadley Wickham and Garett Grolemund. The book is also an excellent source for various data visualization techniques in R and learning R in general. . We will use tidyverse package available in R for faceted graphs. Tidyverse is actually a meta-package which has various packages, and we will use ggplot2 package for our purpose. This package has a data frame (it is like a table in R), called ‘mpg’ which contains observations collected by the US Environmental Protection Agency on 38 models of car. . To create faceted graph, use facet_wrap() option in ggplot. The argument inside the bracket is ~ sign follwed by the categorical variable to be used to create subsets of data. Its use is illustrated in the code given below. . #Install package, if not already installed. install.packages(&quot;tidyverse&quot;) #Load the package library(tidyverse) # Now, we will create faceted graph, with variable &#39;displ&#39; (a car&#39;s engine size) on # x-axis and variable &#39;hwy (car&#39;s fuel efficiency on highway) on y-axis. We will use # `facet_wrap(~class)` option to created faceted graph. The variable &#39;class&#39; denotes # type of car. We use &#39;geom_point()` to create a scatterplot. ggplot(data = mpg)+ geom_point(mapping = aes(x = displ, y = hwy))+ facet_wrap(~class) . The above set of code results in the following panel of subplots: . . Additionally, one can create faceted graph using two variables with facet_grid(). Inside the bracket, use two variables seperated by ~. The example of the same using ‘mpg’ dataframe and two variables ‘drv’ (whether it’s front wheel, rear wheel or 4wd) and ‘cyl’ (number of cylinders) is given below. . ggplot(data = mpg)+ geom_point(mapping = aes(x = displ, y = hwy))+ facet_grid(drv ~ cyl) . The code reults in the follwing panel of subplots: . Stata . In stata, faceted graph can be created by using option by() and mentioning the faceted variable in the bracket. Let’s see an example of the same . Let’s access pre-installed dataset in Stata, called ‘auto.dta’ which has 1978 automobile data. The following code generates scatterplot with ‘length of car’ on x-axis, ‘mileage of car’ on y-axis and variable ‘foreign’ (whether the car is manufactured domestically or imported) used to create subsets of data. sysuse auto is used to load the table ‘auto.dta’. . sysuse auto twoway (scatter mpg length), by(foreign) . The code generates the following graph: .",
    "url": "/Presentation/faceted_graphs.html",
    "relUrl": "/Presentation/faceted_graphs.html"
  }
  ,"27": {
    "title": "Fixed Effects in Linear Regression",
    "content": "Fixed Effects in Linear Regression . Fixed effects is a statistical regression model in which the intercept of the regression model is allowed to vary freely across individuals or groups. It is often applied to panel data in order to control for any individual-specific attributes that do not vary across time. . For more information, see Wikipedia: Fixed Effects Model. . Keep in Mind . To use individual-level fixed effects, you must observe the same person multiple times (panel data). | In a linear regression context, fixed effects regression is relatively straightforward, and can be thought of as effectively adding a binary control variable for each individual, or subtracting the within-individual mean of each variable (the “within” estimator). However, you may want to apply fixed effects to other models like logit or probit. This is usually possible (depending on the model), but if you just add a set of binary controls or subtract within-individual means, it won’t work very well. Instead, look for a command specifically designed to implement fixed effects for that model. | If you are using fixed effects to estimate the causal effect of a variable , individuals with more variance in will be weighted more heavily (Gibbons, Serrano, &amp; Urbancic 2019, ungated copy here). You may want to consider weighting your regression by the inverse within-individual variance of . | . Also Consider . Instead of fixed effects you may want to use random effects, which requires additional assumptions but is statistically more efficient and also allows the individual effect to be modeled using covariates. See Linear Mixed-Effects Regression | You may want to consider clustering your standard errors at the same level as (some or more of) your fixed effects. | . Implementations . Julia . Julia provides support for estimating high-dimensional fixed effect models through the FixedEffectModels.jl package (link). Similarly to felm (R) and reghdfe (Stata), the package uses the method of alternating projections to sweep out fixed effects. However, the Julia implementation is typically quite a bit faster than these other two methods. It also offers further performance gains via GPU computation for users with a working CUDA installation (up to an order of magnitude faster for complicated problems). . # If necessary, install JuliaFixedEffects.jl and some ancilliary packages for reading in the data # ] add JuliaFixedEffects, CSVFiles, DataFrames # Read in the example CSV and convert to a data frame using CSVFiles, DataFrames df = DataFrame(load(&quot;https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv&quot;)) # Calculate proportion of graduates working df[!, :prop_working] = df[!, :count_working] ./ (df[!, :count_working ] .+ df[!, :count_not_working]) using JuliaFixedEffects # Regress median earnings on the proportion of working graduates. # We&#39;ll control for institution name and year as our fixed effects. # We&#39;ll also cluster our standard errors by institution name. reg(df, @formula(earnings_med ~ prop_working + fe(inst_name) + fe(year)), Vcov.cluster(:inst_name)) # Multithread example Threads.nthreads() ## See: https://docs.julialang.org/en/v1.2/manual/parallel-computing/#man-multithreading-1 reg(df, @formula(earnings_med ~ prop_working + fe(inst_name) + fe(year)), Vcov.cluster(:inst_name), method = :lsmr_threads) # GPU example (requires working CUDA installation) reg(df, @formula(earnings_med ~ prop_working + fe(inst_name) + fe(year)), Vcov.cluster(:inst_name), method = :lsmr_gpu) . R . There are numerous packages for estimating fixed effect models in R. We will limit our examples here to the two fastest implementations — lfe::felm and fixest::feols — both of which support high-dimensional fixed effects and standard error correction (multiway clustering, etc.). . We first demonstrate fixed effects in R using felm from the lfe package (link). lfe::felm uses the Method of Alternating Projections to “sweep out” the fixed effects and avoid estimating them directly. By default, this is automatically done in parallel, using all available cores on a user’s machine to maximize performance. (It is also possible to change this behaviour.) . # If necessary, install lfe # install.packages(&#39;lfe&#39;) library(lfe) # Read in data from the College Scorecard df &lt;- read.csv(&#39;https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv&#39;) # Calculate proportion of graduates working df$prop_working &lt;- df$count_working/(df$count_working + df$count_not_working) # A felm formula is constructed as: # outcome ~ # covariates | # fixed effects | # instrumental variables specification | # cluster variables for standard errors # Here let&#39;s regress earnings_med on prop_working # with institution name and year as our fixed effects # And clusters for institution name felm_model &lt;- felm(earnings_med ~ prop_working | inst_name + year | 0 | inst_name, data = df) # Look at our results summary(felm_model) . Next, we consider feols from the fixest package (link). The syntax is very similar to lfe::felm and again the estimation will be done in parallel by default. However, rather than the method of alternating projections, fixest::feols uses a concentrated maximum likelihood method to efficiently estimate models with an arbitrary number of fixed effects. Current benchmarks suggest that this can yield significant speed gains, especially for large problems. For the below example, we’ll continue with the same College Scorecard dataset already loaded into memory. . # If necessary, install fixest # install.packages(&#39;fixest&#39;) library(fixest) # Run the same regression as before feols_model &lt;- feols(earnings_med ~ prop_working | inst_name + year, data = df) # Look at our results # Standard errors are automatically clustered at the inst_name level summary(feols_model) # It is also possible to specify additional or different clustering of errors summary(feols_model, se = &quot;twoway&quot;) summary(feols_model, cluster = c(&quot;inst_name&quot;, &quot;year&quot;)) ## same as the above . As noted above, there are numerous other ways to implement fixed effect models in R. Users may also wish to look at the plm, lme4, and estimatr packages among others. For example, the latter’s estimatr::lm_robust function provides syntax that may be more familar syntax to new R users who are coming over from Stata. Note, however, that it will be less efficient for complicated models. . Stata . We will estimate fixed effects using Stata in two ways. First, using the built in xtreg command. Second, using the reghdfe package (link), which is more efficient and better handles multiple levels of fixed effects (as well as multiway clustering), but must be downloaded from SSC first. . * Load in College Scorecard data import delimited &quot;https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv&quot;, clear * The missings are written as &quot;NA&quot;, let&#39;s turn this numeric destring count_not_working count_working earnings_med, replace force * Calculate the proportion working g prop_working = count_working/(count_working + count_not_working) * xtset requires that the individual identifier be a numeric variable encode inst_name, g(name_number) * Set the data as panel data with xtset xtset name_number * Use xtreg with the &quot;fe&quot; option to run fixed effects * Regressing earnings_med on prop_working * with fixed effects for name_number (implied by fe) * and also year (which we&#39;ll add manually with i.year) * and standard errors clustered by name_number xtreg earnings_med prop_working i.year, fe vce(cluster name_number) * Now, let&#39;s demonstrate the same regression with reghdfe. * Install the package first if necessary. * ssc install reghdfe * For reghdfe we don&#39;t need to xtset the data. Let&#39;s undo that xtset, clear * We specify both sets of fixed effects in absorb() reghdfe earnings_med prop_working, absorb(name_number year) vce(cluster inst_name) .",
    "url": "/Model_Estimation/fixed_effects_in_linear_regression.html",
    "relUrl": "/Model_Estimation/fixed_effects_in_linear_regression.html"
  }
  ,"28": {
    "title": "Geocoding",
    "content": "Geocoding . Geocoding is taking an address (e.g. 1600 Pennsylvania Ave NW, Washington DC 20500) or a name of a place (e.g. The White House) and turning it into a geographic position on the earth’s surface. Commonly, the cooridinate system is longitude and latitude but there are other potential coordinate systems that can be used. . There are many different types of locations one can geocode including: . Cities | Landmarks | Geographic Locations * Mountains * Rivers | Addresses * Street Intersections * House Numbers with street names * Postal Codes | There are multiple ways to geocode. For instance, you could find the corrdinates of the Empire State building by flying to New York, riding an elevator to the top of the building, and then using your GPS to get the latitiude and longitude of where you were standing. A much more efficient way of geocoding is through interpolation. Interpolation uses other known geocoded locations to estimate the coordinates of the data that you wish to geocode. A computer uses an algorithm and the closest known geocodes to conduct this interpolation. However, the farther the “closest” known geocodes are to the data you are trying to geocode the less accurate the geocoding process is. smartystreets, a geocode platform, has a good explanation of this. . Additionally, Reverse Geocoding takes a latitude-longitude pair (or other global coordinates) and converts it into an address or a place. Depending on the data that is available reverse geocoding can be very useful. Similar to regular geocoding, reverse geocoding uses other known reverse geocoded locations to estimate the address of the inputted coordinates. . The Geocoding Process . Whenever you geocode data there is a 3 step process that is undergone: . Step 1: Input Data Descriptive or textual data is inputted to yield a desired corresponding spatial data | Step 2: Classification of Input Data Input data is sorted into two groups relative input data and absolute data | Step 3A: Relative Input Data Relative input data is the non-preferred type of data (most geocoding reject relative input data). Relative data are textual descriptions of locations that cannot be converted into precise spatial data on their own. Instead they are dependent on a other reference locations. For example, “across the street from the White House” has to use “the White House” as a reference point and then deduce what “across the street” means. | Step 3B: Absolute Data This is the sweet sweet data that geocoding platforms love. A spatial coordinate (lon, lat) can be defined for this data independently of other reference points. Examples: USPS ZIP codes; complete and partial postal addresses; PO boxes; cities; counties; intersections; and named places | . Why is geocoding helpful? . Odds are if you are on this page then you already have a reason to use geocoding, but here is a brief motivation for how geocoding can help with a project. Geocoding is helpful when you want to do spatial work. For example, maybe you have data on voter addresses and want to visualize party allegiance. Perhaps, you are wondering who is affected by a certain watershed. If you are limited to postal addresses without being able to visualize the actual location of those addresses the inference is limited. Commuter habits, crime trends, pandemic evolution, and (fill in your example here) analyses are all improved with geocoding. Thanks, geocoding! . Geocoding Services . It is important to recognize that there are many different geocoding platforms. There are others but here is a short list of platforms to consider: . Geocodio | Google’s geocode API service | IPUMS Geomarker | ArcGIS | This page will talk specifically about Geocodio and how to use the Goecodio platform in R studio. When you are deciding which geocode platform to use some important things to keep in mind are pricing structures and other specific features of the platfrom like bulk geocoding and coverage. For example, Geocodio is much more suited to geocode big data sets than Google’s platform. However, Geocodio is only able to geocode within the United States and Cananda whereas Google has international capabilities. Google is better at guessing what location you are trying to geocode (“the White House”) than Geocodio, but Geocodio offers census appends. The pricing sturcture is also nuanced across platforms. Here is a comparison chart provided by Geocodio that gives a flavor of what to consider when deciding which service to use (disclaimer, I find it to be a bit biased…) Lots to consider! In the end, which platform works best will depend on your preferences and the nature of your project. . Geocodio . This page will focus specifically on using Geocodio. Geocodio’s website is very straight forward, but I will briefly walk through the porcess: . Start by making an account. This account will allow you to do your geocoding with Geocodio as well as get a Geocodio API which we can use in R studio. . | To geocode on the website you can either upload a spreadsheet or copy and paste addresses into the input window. I highly recommend a spreadsheet which takes a specific format . | Geocodio will ask you to make edits if the data you have provided isn’t accurate enough . | Once your data is in satisfactory form Geodio allows you to make appends which allows you to include information pertaining to the addresses you wish to geocode (e.g. what State Legislative District the address is in or Census ACS Demographic information for the addresses you are geocoding) . | Finally Geocodio will geocode your addresses and return a downloadable csv file. The cost and the time of this process depends on the size of your data. For example, 250,000 addresses can be geocoded for $123.75 and will take about an hour to process. For estimates of both cost and time click here . | Keep in Mind . Be attentive to accruacy and accuracy type. Just because something is spit out doesn’t mean you should use/trust it | When you run rgeocodio commands in R it is accessing Geocodio’s server so you need to consider pricing (2,500 free lookups per day) | One size doesn’t necesarily fit all–tailor the geocode platform you choose to your project | . Implementations . rgeocodio (R + Geocodio) . rgeocodio allows you to access the Geocodio platform in R studio. Instead of the steps mentioned above you can use the rgeocodio to perform the same functions. . In order to install rgeocodio you will need to load the devtools package. Install it if you haven’t already install.packages(&quot;devtools&quot;). Once devtools is loaded run:devtools::install_github(&#39;hrbrmstr/rgeocodio&#39;). . rgeocodio uses an API that you can get from the geocodio website. To get an API visit geocodio’s website. Then save it in your Renviron. . To save the API in your Renvrion: . Open the Renviron by running usethis::edit_r_environ() | Once you are in the Renviron name and save the API you got from Geocodio. Maybe something like: #geocodio_API = &#39;your api` . | Save your Renviron and then restart your R session just to be sure that the API is saved. | Now that you have your API saved in R you still need to authorize the API in your R session. Do so by running gio_auth(). . # If necessary # install.packages(c(&#39;rgeocodio&#39;,&#39;readxl&#39;,&#39;tidyverse&#39;)) library(rgeocodio) gio_auth(force = F) . A quick note, force makes you set a new geocodio API key for the current environment. In general you will want to run force=F. Lets try a regeocodio example. Say you want to get the coordinates of the White House. You could run: . rgeocodio::gio_geocode(&#39;1600 Pennsylvania Ave NW, Washington DC 20500&#39;) . Most of these variables are intuitive but I want to spend a few seconds on accuracy and accuracy type which we can learn more about here. . Accuracy: because geocodio is interpolating the output will tell you how confident geocodio is in its estimation. Anything below 0.8 should be considered not accurate enough, but that is up to the user. . | Accuracy Type: interpolation uses the closest know geocodes. So if the closest geocodes are, for instance two ends of a street and you are trying to geocode a location somewhere on that street then the accuracy type will be “street.” In this case the accuracy type is “rooftop” which means the buildings on either side of the location were used to interpolate your query. Again, smartystreets has a good explanation of this. . | What if we want to geocode a bunch of addresses at once? To geocode multiple addresses at once we will use gio_batch_geocode. The data that we enter will need to be a character vector of addresses. . library(readxl) library(tidyverse) addresses&lt;- c(&#39;Yosemite National Park, California&#39;, &#39;1600 Pennsylvania Ave NW, Washington DC 20500&#39;, &#39;2975 Kincaide St Eugene, Oregon, 97405&#39;) gio_batch_geocode(addresses) . You will notice that the output is a list with dataframes of the results embedded. There are a number of ways to extract the relevant data but one approach would be: . addresses&lt;- c(&#39;Yosemite National Park, California&#39;, &#39;1600 Pennsylvania Ave NW, Washington DC 20500&#39;, &#39;2975 Kincaide St Eugene, Oregon, 97405&#39;) extract_function&lt;- function(addresses){ data&lt;-gio_batch_geocode(addresses) vector&lt;- (1: length(addresses)) df_function&lt;-function(vector){ df&lt;-data$response_results[vector] df&lt;-df%&gt;%as.data.frame() } geocode_data&lt;-do.call(bind_rows, lapply(vector, df_function)) return(geocode_data) } extract_function(addresses) . Reverse geocoding uses gio_reverse and gio_batch_reverse. . For gio_reverse you submit a longitude-latitude pair: . gio_reverse(38.89767, -77.03655) . For gio_batch_reverse we will submit a vector of numeric entries ordered by c(longitude, latitude): . #make a dataset data&lt;-data.frame( lat = c(35.9746000, 32.8793700, 33.8337100, 35.4171240), lon = c(-77.9658000, -96.6303900, -117.8362320, -80.6784760) ) gio_batch_reverse(data) . Notice that the output gives us multiple accuracy types. . What about geocoding the rest of the world, chico? . rgeocodio::gio_batch_geocode(&#39;523-303, 350 Mokdongdong-ro, Yangcheon-Gu, Seoul, South Korea 07987&#39;) . gasp Geocodio only works, from my understanding, in the United States and Canada. We would need to use a different service like Google’s geocoder to do the rest of the world. .",
    "url": "/Geo-Spatial/geocoding.html",
    "relUrl": "/Geo-Spatial/geocoding.html"
  }
  ,"29": {
    "title": "Heteroskedasticity-consistent standard errors",
    "content": "Heteroskedasticity-consistent (HC) standard errors . Heteroskedasticity is when the variance of a model’s error term is related to the predictors in that model. For more information, see Wikipedia: Heteroscedasticity. . Many regression models assume homoskedasticity (i.e. constant variance of the error term), especially when calculating standard errors. So in the presence of heteroskedasticity, standard errors will be incorrect. Heteroskedasticity-consistent (HC) standard errors — also called “heteroskedasticity-robust”, or sometimes just “robust” standard errors — are calculated without assuming such homoskedasticity. For more information, see Wikipedia: Heteroscedasticity-consistent standard errors. . Keep in Mind . Robust standard errors are a common way of dealing with heteroskedasticity. However, they make certain assumptions about the form of that heteroskedasticity which may not be true. You may instead want to use GMM instead. | For nonlinear models like Logit, heteroskedasticity can bias estimates in addition to messing up standard errors. Simply using a robust covariance matrix will not eliminate this bias. Check the documentation of your nonlinear regression command to see whether its robust-error options also adjust for this bias. If not, consider other ways of dealing with heteroskedasticity besides robust errors. | There are multiple kinds of robust standard errors, for example HC1, HC2, and HC3. Check in to the kind available to you in the commands you’re using. | . Also Consider . Generalized Method of Moments | Cluster-Robust Standard Errors | Bootstrap Standard Errors | Jackknife Standard Errors | . Implementations . R . The easiest way to obtain robust standard errors in R is with the estimatr package (link) and its family of lm_robust functions. These will default to “HC2” errors, but users can specify a variety of other options. . # If necessary, install estimatr # install.packages(c(&#39;estimatr&#39;)) library(estimatr) # Get mtcars data # data(mtcars) ## Optional: Will load automatically anyway # Default is &quot;HC2&quot;. Here we&#39;ll specify &quot;HC3&quot; just to illustrate. m1 &lt;- lm_robust(mpg ~ cyl + disp + hp, data = mtcars, se_type = &quot;HC3&quot;) summary(m1) . Alternately, users may consider the vcovHC function from the sandwich package (link), which is very flexible and supports a wide variety of generic regression objects. For inference (t-tests, etc.), use in conjunction with the coeftest function from the lmtest package (link). . # If necessary, install lmtest and sandwich # install.packages(c(&#39;lmtest&#39;,&#39;sandwich&#39;)) library(sandwich) library(lmtest) # Create a normal regression model (i.e. without robust standard errors) m2 &lt;- lm(mpg ~ cyl + disp + hp, data = mtcars) # Get the robust VCOV matrix using sandwich::vcovHC(). We can pick the kind of robust errors # with the &quot;type&quot; argument. Note that, unlike estimatr::lm_robust(), the default this time # is &quot;HC3&quot;. I&#39;ll specify it here anyway just to illustrate. vcovHC(m2, type = &quot;HC3&quot;) sqrt(diag(vcovHC(m2))) ## HAC SEs # For statistical inference, use together with lmtest::coeftest(). coeftest(m2, vcov = vcovHC(m2)) . Stata . Stata has robust standard errors built into most regression commands, and they generally work the same way for all commands. . * Load in auto data sysuse auto.dta, clear * Just add robust to the options of the regression * This will give you HC1 regress price mpg gear_ratio foreign, robust * For other kinds of robust standard errors use vce() regress price mpg gear_ratio foreign, vce(hc3) .",
    "url": "/Model_Estimation/Nonstandard_Errors/hc_se.html",
    "relUrl": "/Model_Estimation/Nonstandard_Errors/hc_se.html"
  }
  ,"30": {
    "title": "Heatmap Colored Correlation Matrix",
    "content": "Heatmap Colored Correlation Matrix . A correlation matrix shows the correlation between different variables in a matrix setting. However, because these matrices have so many numbers on them, they can be difficult to follow. Heatmap coloring of the matrix, where one color indicates a positive correlation, another indicates a negative correlation, and the shade indicates the strength of correlation, can make these matrices easier for the reader to understand. . Keep in Mind . Even with heatmap coloring, very large correlation matrices can still be difficult to read, as you must pinpoint which variable names go with which cell of the matrix. Consider breaking big correlation matrices up into smaller ones, or limiting the amount of data you’re trying to show in some other way. | . Also Consider . You may just want to create a correlation matrix | . Implementations . Python . We present two ways you can create a heatmap. First, the seaborn package has a great collection of premade plots, one of which is a heatmap we’ll use. The second we’ll only point you to, which is a “by hand” approach that will allow you more customization. . For the by hand approach, see this guide. . For the seaborn approach, you will need to pip install seaborn or conda install seaborn before continuing. Once you’ve done that, the follow code will produce the below plot. . # Ganked from https://seaborn.pydata.org/examples/many_pairwise_correlations.html # Assumes you have run `pip install numpy pandas matplotlib scikit-learn seaborn` # Standard imports import numpy as np import pandas as pd from matplotlib import pyplot as plt # For this example we&#39;ll use Seaborn, which has some nice built in plots import seaborn as sns # Grab a data set from scikit-learn from sklearn.datasets import fetch_california_housing data = fetch_california_housing() df = pd.DataFrame( np.c_[data[&#39;data&#39;], data[&#39;target&#39;]], columns=data[&#39;feature_names&#39;] + [&#39;target&#39;] ) # Create the correlation matrix corr = df.corr() # Generate a mask for the upper triangle; True = do NOT show mask = np.zeros_like(corr, dtype=np.bool) mask[np.triu_indices_from(mask)] = True # Set up the matplotlib figure f, ax = plt.subplots(figsize=(11, 9)) # Generate a custom diverging colormap cmap = sns.diverging_palette(220, 10, as_cmap=True) # Draw the heatmap with the mask and correct aspect ratio # More details at https://seaborn.pydata.org/generated/seaborn.heatmap.html sns.heatmap( corr, # The data to plot mask=mask, # Mask some cells cmap=cmap, # What colors to plot the heatmap as annot=True, # Should the values be plotted in the cells? vmax=.3, # The maximum value of the legend. All higher vals will be same color vmin=-.3, # The minimum value of the legend. All lower vals will be same color center=0, # The center value of the legend. With divergent cmap, where white is square=True, # Force cells to be square linewidths=.5, # Width of lines that divide cells cbar_kws={&quot;shrink&quot;: .5} # Extra kwargs for the legend; in this case, shrink by 50% ) # You can save this as a png with # f.savefig(&#39;heatmap_colored_correlation_matrix_seaborn_python.png&#39;) . . R . We will be creating our heatmap in two different ways. First, we will be using the corrplot package, which is tailor-made for the task and is very easy to use. Then, we will be using ggplot2 with geom_tile, which requires much more preprocessing to use, but then provides access to the entirety of the ggplot2 package for customization. . First, we will use corrplot: . # Install the corrplot package if necessary # install.packages(&#39;corrplot&#39;) # Load in the corrplot package library(corrplot) # Load in mtcars data data(mtcars) # Don&#39;t use too many variables or it will get messy! mtcars &lt;- mtcars[,c(&#39;mpg&#39;,&#39;cyl&#39;,&#39;disp&#39;,&#39;hp&#39;,&#39;drat&#39;,&#39;wt&#39;,&#39;qsec&#39;)] # Create a corrgram corrplot(cor(mtcars), # Using the color method for a heatmap method = &#39;color&#39;, # And the lower half only for easier readability type = &#39;lower&#39;, # Omit the 1&#39;s along the diagonal to bring variable names closer diag = FALSE, # Add the number on top of the color addCoef.col = &#39;black&#39; ) . This results in: . . Now we will make the graph using ggplot2. We will also make a little use of dplyr and tidyr, and so we’ll load them all as a part of the tidyverse. This example makes use of this guide. . # Install the tidyverse if necessary # install.packages(&#39;tidyverse&#39;) # Load in the tidyverse library(tidyverse) # Load in mtcars data data(mtcars) # Create a correlation matrix. C &lt;- mtcars %&gt;% # Don&#39;t use too many variables or it will get messy! # We use dplyr&#39;s select() here but there are other ways to limit variables, like [] select(cyl, disp, drat, hp, mpg, qsec, wt) %&gt;% # Correlation matrix cor() # At this point, we can limit the matrix to just its lower half # Note this will give weird results if you didn&#39;t select variables in alphabetical order earlier C[upper.tri(C)] &lt;- NA C &lt;- C %&gt;% # Turn it into a data frame as.data.frame() %&gt;% # with a column for the variable names. # We use dplyr&#39;s mutate to create this column but it could be made with $ # the . here means &quot;the data set we&#39;re working with&quot; mutate(Variable = row.names(.)) # Use tidyr&#39;s pivot_longer to reshape to long format # There are other ways to reshape too C_Long &lt;- pivot_longer(C, cols = c(mpg, cyl, disp, hp, drat, wt, qsec), # We will want this option for sure if we dropped the # upper half of the triangle earlier values_drop_na = TRUE) %&gt;% # Make both variables into factors mutate(Variable = factor(Variable), name = factor(name)) %&gt;% # Reverse the order of one of the variables so that the x and y variables have # Opposing orders, common for a correlation matrix mutate(Variable = factor(Variable, levels = rev(levels(.$Variable)))) # Now we graph! ggplot(C_Long, # Our x and y axis are Variable and name # And we want to fill each cell with the value aes(x = Variable, y = name, fill = value))+ # geom_tile to draw the graph geom_tile() + # Color the graph as we like # Here our negative correlations are red, positive are blue # gradient2 instead of gradient gives us a &quot;mid&quot; color which we can make white scale_fill_gradient2(low = &quot;red&quot;, high = &quot;blue&quot;, mid = &quot;white&quot;, midpoint = 0, limit = c(-1,1), space = &quot;Lab&quot;, name=&quot;Pearson nCorrelation&quot;) + # Axis names don&#39;t make much sense labs(x = NULL, y = NULL) + # We don&#39;t need that background theme_minimal() + # If we need more room for variable names at the bottom, rotate them theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1)) + # We want those cells to be square! coord_fixed() + # If you also want the correlations to be written directly on there, add geom_text geom_text(aes(label = round(value,3))) . This results in: . . SAS . See this guide. . Stata . Stata has the installable package corrtable which produces heatmap correlation tables. Handily, it puts the variable labels (or names, if labels aren’t available) along the diagonal where they are easy to read. Note that it does run quite slowly. . * Install corrtable if necessary * ssc install corrtable * Get auto data sysuse auto.dta, clear * Make correlation table * The half option just shows the lower triangle and puts variable names on the axis. * The flag1 and howflag1 options tell corrtable to plot positive correlations (r(rho &gt; 0)) * as blue (blue*.1) * and flag2 and howflag2 similarly tell it to plot negative correlations as pink. corrtable price-length, half flag1(r(rho) &gt; 0) howflag1(plotregion(color(blue * 0.1))) flag2(r(rho) &lt; 0) howflag2(plotregion(color(pink*0.1))) . This results in: . .",
    "url": "/Presentation/heatmap_colored_correlation_matrix.html",
    "relUrl": "/Presentation/heatmap_colored_correlation_matrix.html"
  }
  ,"31": {
    "title": "Heckman Correction Model",
    "content": "Heckman Correction Model . The Heckman correction for sample selection is a method designed to be used in cases where the model can only be run on a subsample of the data that is not randomly selected. For example, a regression using to predict cannot include people who don’t work, since we don’t observe their wage. The Heckman model views this sample selection process as a form of omitted variable bias. So, it (1) explicitly models the process of selecting into the sample, (2) transforms the predicted probability of being in the sample, and (3) adds a correction term to the model. . For more information, see Wikipedia: Heckman correction. . Keep in Mind . Conceptually, the Heckman model uses the regression covariates to predict selection, transforms the prediction, and then includes that transformation in the model. If there are no variables in the selection model that are excluded from the regression model, then the Heckman model is perfectly collinear, and is only statistically identified because the transformation is nonlinear (you may have heard the phrase “identified by nonlinearity” or “identified by the normality assumption”). That’s not ideal! You want to find an exclusion restriction - a variable that predicts selection, but does not belong in the final regression model - to avoid this collinearity. | . Also Consider . There are many ways to estimate a Heckman model. Maximum likelihood approaches generally have better statistical properties, but two-stage models are computationally simpler. Often you can look in the options of your Heckman estimator command to select an estimation method. | If your goal is to estimate the effect of a binary treatment by modeling selection into treatment, consider a Treatment Effect Model, or an Endogenous Switching Model which also allows predictors to work differently in different settings. | Standard Heckman models rely heavily on assumptions about the normality of error terms. You may want to consider Nonparametric Sample Selection Models. | . Implementations . Gretl . See here for a demonstration. . Python . See here for a demonstration. . R . # Install sampleSelection package if necessary # install.packages(&#39;sampleSelection&#39;) library(sampleSelection) # Get data from Mroz (1987, Econometrica) # which has Panel Study of Income Dynamics data for married women data(&quot;Mroz87&quot;) # First consider our selection model # We only observe wages for labor force participants (lfp == 1) # So we model that as a function of work experience (linear and squared), # income from the rest of the family, education, and number of kids 5 or younger. # lfp ~ exper + I(exper^2) + faminc + educ + kids5 # Then we model the regression of interest. We&#39;re interested in modeling # wage as a function of work experience, education, and whether you&#39;re in a city # Here, we don&#39;t include family income or number of kids, under the assumption that they # do not belong in a wage model. These are our exclusion restrictions # (note these particular exclusion restrictions might be a little dubious! But hey, this paper&#39;s from 1987.) # wage ~ exper + I(exper^2) + educ + city # Put them together in a selection() command heck_model &lt;- selection(lfp ~ exper + I(exper^2) + faminc + educ + kids5, wage ~ exper + I(exper^2) + educ + city, Mroz87) summary(heck_model) . Stata . * Get data from Mroz (1987, Econometrica) * which has Panel Study of Income Dynamics data for married women * (data via the sampleSelection package in R) import delimited &quot;https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Estimation/Data/Heckman_Correction_Model/Mroz87.csv&quot;, clear * First, consider the regression of interest. * First consider our selection model * We only observe wages for labor force participants (lfp == 1) * So we model that as a function of work experience (linear and squared), * income from the rest of the family, education, and number of kids 5 or younger. * select(lfp = c.exper##c.exper faminc educ kids5) * Then we model the regression of interest. We&#39;re interested in modeling * wage as a function of work experience, education, and whether you&#39;re in a city * Here, we don&#39;t include family income or number of kids, under the assumption that they * do not belong in a wage model. These are our exclusion restrictions * (note these particular exclusion restrictions might be a little dubious! But hey, 1987.) * wage c.exper##c.exper educ city * Now we run our Heckman model! heckman wage c.exper##c.exper educ city, select(lfp = c.exper##c.exper faminc educ kids5) .",
    "url": "/Model_Estimation/heckman_correction_model.html",
    "relUrl": "/Model_Estimation/heckman_correction_model.html"
  }
  ,"32": {
    "title": "Histograms",
    "content": "Histograms . Histograms are an indespensible tool of research across disciplines. They offer a helpful way to represent the distribution of a variable of interest. Specifically, their function is to record how frequently data values fall within pre-specified ranges called “bins.” Such visual representations can help researchers easily detect whether their data are distributed in a skewed or symmetric way, and can help detect outliers. . Despite being such a popular tool for scientific research, choosing the bin width (alternatively, number of bins) is ultimately a choice by the researcher. Histograms are intended to convey information about the variable, and choosing the “right” bin size to convey the information helpfully can be something of an art. . The relationship between bin width and the number of bins is given by: . For this reason, statistical softwares such as R and Stata will often accept either custom bin width specifications, or a number of bins. . Histogram vs. bar graph . Because histograms represent data frequency using rectangular bars, they might be mistaken for bar graphs at first glance. Whereas bar graphs (sometimes called bar charts) plot values for categorical data, histograms represent the distribution of continuous variables such as income, height, weight, etc. . Implementations . When feeding data to visualise using a histogram, one will notice that both R and Stata will attempt to “guess” what the “best” bin width/number of bins are. These may be overridden by user commands, as we will see. . R . Histograms can be represented using base R, or more elegantly with ggplot. R comes with a built-in states.x77 dataset containing per-capita income in the US states for the year 1974, which we will be using. . # loading the data incomes = data.frame( income = state.x77[,&#39;Income&#39;]) # first using base R hist(incomes$income) # now using ggplot if(!require(ggplot2)) install.packages(&#39;ggplot2&#39;) library(ggplot2) ggplot( data = incomes ) + geom_histogram( aes( x = income ) ) # showing how we can adjust number of bins... ggplot( data = incomes ) + geom_histogram( aes( x = income ) , bins = 15 ) # ...or the width of each bin ggplot( data = incomes ) + geom_histogram( aes( x = income ) , binwidth = 500 ) . Stata . To illustrate the basic histogram function in Stata we will use the “auto” dataset. . ** loading the data webuse auto * histogram with default bin width * The frequency option puts a count of observations on the y-axis * rather than a proportion histogram mpg, frequency * we can adjust the number of bins... histogram mpg, bin(15) frequency * ...or the bin width hist mpg, width(2) frequency .",
    "url": "/Presentation/histograms.html",
    "relUrl": "/Presentation/histograms.html"
  }
  ,"33": {
    "title": "Import a Foreign Data File",
    "content": "Import a Foreign Data File . Commonly, data will be distributed in a format that is not native to the software that you are using, such as Excel. How can you import it? . This page is specifically about importing data files from formats specific to particular foreign software. For importing standard shared formats, see Import a Delimited Data File (CSV, TSV) or Import a Fixed-Width Data File. . Keep in Mind . Check your data after it’s imported to make sure it worked properly. Sometimes special characters will have trouble converting, or variable name formats are inconsistent, and so on. It never hurts to check! | . Also Consider . Import a Delimited Data File (CSV, TSV) | Import a Fixed-Width Data File | Export Data to a Foreign Format | . Implementations . Because there are so many potential foreign formats, these implementations will be more about listing the appropriate commands with example syntax than providing full working examples. Make sure that you fill in the proper filename. The filename should include a filepath, or you should Set a Working Directory. . R . # Load Excel files with the readxl package # install.packages(&#39;readxl&#39;) library(readxl) data &lt;- read_excel(&#39;filename.xlsx&#39;) # Read Stata, SAS, and SPSS files with the haven package # install.packages(&#39;haven&#39;) library(haven) data &lt;- read_stata(&#39;filename.dta&#39;) data &lt;- read_spss(&#39;filename.sav&#39;) # read_sas also supports .sas7bcat, or read_xpt supports transport files data &lt;- read_sas(&#39;filename.sas7bdat&#39;) # Read lots of other types with the foreign package # install.packages(&#39;foreign&#39;) library(foreign) data &lt;- read.arff(&#39;filename.arff&#39;) data &lt;- read.dbf(&#39;filename.dbf&#39;) data &lt;- read.epiinfo(&#39;filename.epiinfo&#39;) data &lt;- read.mtb(&#39;filename.mtb&#39;) data &lt;- read.octave(&#39;filename.octave&#39;) data &lt;- read.S(&#39;filename.S&#39;) data &lt;- read.systat(&#39;filename.systat&#39;) . Stata . Stata can import foreign files using the File -&gt; Import menu. Alternately, you can use the import command: . import type using filename . where type can be excel, spss, sas, haver, or dbase (import can also be used to download data directly from sources like FRED). .",
    "url": "/Other/import_a_foreign_data_file.html",
    "relUrl": "/Other/import_a_foreign_data_file.html"
  }
  ,"34": {
    "title": "Instrumental Variables",
    "content": "Instrumental Variables . In the regression model . where is an error term, the estimated will not give the causal effect of on if is endogenous - that is, if is related to and so determined by forces within the model (endogenous). . One way to recover the causal effect of on is to use instrumental variables. If there exists a variable that is related to but is completely unrelated to (perhaps after adding some controls), then you can use instrumental variables estimation to isolate only the part of the variation in that is explained by . Naturally, then, this part of the variation is unrelated to because is unrelated to , and you can get the causal effect of that part of . . For more information, see Wikipedia: Instrumental variables estimation. . Keep in Mind . Technically, all the variables in the model except for the dependent variable and the endogenous variables are “instruments”, including controls. However, it is also common to refer to only the excluded instruments (i.e., variables that are only used to predict the endogenous variable, not the dependent variable) as instruments. This page will follow that convention. | For instrumental variables to work, it must be the case that the instrument is only related to the outcome variable through other variables already included in the model like the endogenous variables or the controls. This is called the “validity” assumption and it cannot be verified in the data, only theoretically. Give serious consideration as to whether validity applies to your instrument before using instrumental variables. | You can check for the relevance of your instrument, which is how strongly related it is to your endogenous variable. A rule of thumb is that an joint F-test of the instruments should be at least 10, but this is only a rule of thumb, and imprecise (see Stock and Yogo 2005 for a more precise version of this test). In general, if the instruments are not very strong predictors of the endogenous variables, you should consider whether your analysis fits the assumptions necessary to run a weak-instrument-robust estimation method. See Hahn &amp; Hausman 2003 for an overview. | Instrumental variables estimates a local average treatment effect - in other words, a weighted average of each individual observation’s treatment effect, where the weights are based on the strength of the effect of the instrument on the endogenous variable. Note both that this is not the same thing as an average treatment effect, which is an average of each individual’s treatment effect, which is usually what is desired, and also that if the instrumental variable has effects of different signs for different people (non-monotonicity), then the estimate isn’t really anything of interest. Be sure that monotonicity makes sense in your context before using instrumental variables. | Instrumental variables is a consistent estimator of a causal effect, but it is biased in finite samples. Be wary of using instrumental variables in small samples. | . Also Consider . Instrumental variables methods generally rely on linearity assumptions, and if your dependent or endogenous variables are not continuous, their assumptions may not hold. Consider methods specially designed for nonlinear instrumental variables estimation. | There are many ways to estimate instrumental variables, not just two stage least squares. Different estimators such as GMM or k-class limited-information maximum likelihood estimators perform better or worse depending on heterogeneous treatment effects, heteroskedasticity, and sample size. Many instrumental variables estimation commands allow for multiple different estimation methods, described below. Note that in the just-identified case (where the number of instruments is the same as the number of endogenous variables), several common estimators produce identical results. | . Implementations . R . There are several ways to run instrumental variables in R. Here we will cover two - AER::ivreg(), which is probably the most common, and lfe::felm(), which is more flexible and powerful. You may also want to consider looking at estimatr::iv_robust, which combines much of the flexibility of lfe::felm() with the simple syntax of AER::ivreg(), although it is not as powerful. . # If necessary, install both packages. # install.packages(c(&#39;AER&#39;,&#39;lfe&#39;)) # Load AER library(AER) # Load the Cigarettes data from ivreg, following the example data(CigarettesSW) # We will be using cigarette taxes as an instrument for cigarette prices # to evaluate the effect of cigarette price on log number of packs smoked # With income per capita as a control # Adjust everything for inflation CigarettesSW$rprice &lt;- CigarettesSW$price/CigarettesSW$cpi CigarettesSW$rincome &lt;- CigarettesSW$income/CigarettesSW$population/CigarettesSW$cpi CigarettesSW$tdiff &lt;- (CigarettesSW$taxs - CigarettesSW$tax)/CigarettesSW$cpi # The regression formula takes the format # dependent.variable ~ endogenous.variables + controls | instrumental.variables + controls ivmodel &lt;- ivreg(log(packs) ~ log(rprice) + log(rincome) | tdiff + log(rincome), data = CigarettesSW) summary(ivmodel) # Now we will run the same model with lfe::felm library(lfe) # The regression formula takes the format # dependent vairable ~ # controls | # fixed.effects | # (endogenous.variables ~ instruments) | # clusters.for.standard.errors # So if need be it is straightforward to adjust this example to account for # fixed effects and clustering. # Note the 0 indicating no fixed effects ivmodel2 &lt;- felm(log(packs) ~ log(rincome) | 0 | (log(rprice) ~ tdiff), data = CigarettesSW) summary(ivmodel2) # felm can also use several k-class estimation methods; see help(felm) for the full list. # Let&#39;s run it with a limited-information maximum likelihood estimator with # the fuller adjustment set to minimize squared error (4). ivmodel3 &lt;- felm(log(packs) ~ log(rincome) | 0 | (log(rprice) ~ tdiff), data = CigarettesSW, kclass = &#39;liml&#39;, fuller = 4) summary(ivmodel3) . Stata . Instrumental variables estimation in Stata typically uses the built-in ivregress command. This command can be used to implement linear instrumental variables regression using two-stage least squares, GMM, or LIML . * Get Stock and Watson Cigarette data import delimited &quot;https://vincentarelbundock.github.io/Rdatasets/csv/Zelig/CigarettesSW.csv&quot;, clear * Adjust everything for inflation g rprice = price/cpi g rincome = (income/population)/cpi g tdiff = (taxs - tax)/cpi * And take logs g lpacks = ln(packs) g lrincome = ln(rincome) g lrprice = ln(rprice) * The syntax for the regression is * name_of_estimator dependent_variable controls (endogenous_variable = instruments) * where name_of_estimator can be two stage least squares (2sls), * limited information maximum likelihood (liml, note that ivregress doesn&#39;t support k-class estimators), * or generalized method of moments (gmm) * Here we can run two stage least squares ivregress 2sls lpacks rincome (lrprice = tdiff) * Or gmm. ivregress gmm lpacks rincome (lrprice = tdiff) .",
    "url": "/Model_Estimation/instrumental_variables.html",
    "relUrl": "/Model_Estimation/instrumental_variables.html"
  }
  ,"35": {
    "title": "Interaction Terms and Polynomials",
    "content": "Interaction Terms and Polynomials . Regression models generally assume that the outcome variable is a function of an index, which is a linear function of the independent variables, for example in ordinary least squares: . However, if the independent variables have a nonlinear effect on the outcome, the model will be incorrectly specified. This is fine as long as that nonlinearity is modeled by including those nonlinear terms in the index. . The two most common ways this occurs is by including interactions or polynomial terms. With an interaction, the effect of one variable varies according to the value of another: . and with polynomial terms, the effect of one variable one the outcome is allowed to take a non-linear shape: . Keep in Mind . When you have interaction terms or polynomials, the effect of a variable can no longer be described with a single coefficient, and in some senses the individual coefficients lose meaning without the others. You can understand the effect of a single variable by taking the derivative of the index with respect to that variable. For example, in , the effect of on is . You must plug in the value of to get the effect of . Or in , the effect of is . You must plug in a value of to get the marginal effect of at that value. | In almost all cases, if you are including an interaction term, you should also include each of the interacted variables on their own. Otherwise, the coefficients become very difficult to interpret. | In almost all cases, if you are including a polynomial, you should include all terms of the polynomial. In other words, include the linear and squared term, not just the squared term. | . Also Consider . Interaction terms tend to have low statistical power. Consider performing a power analysis of interaction terms before running your analysis. | Polynomials are not the only way to model a nonlinear relationship. You could, for example, run one of many kinds of nonparametric regression. | You may want to get the average marginal effects or the marginal effects at the mean of your variables after running your model. | One common way to display the effects of a model with interactions is to graph them. See marginal effects plots for interactions with continuous variables and Marginal effects plots for interactions with continuous variables | . Implementations . Python . Using the statsmodels package, we can use a similar formulation as the R example below. . # Standard imports import numpy as np import pandas as pd import statsmodels.formula.api as sms from matplotlib import pyplot as plt # Load the R mtcars dataset from a URL df = pd.read_csv(&#39;https://raw.githubusercontent.com/LOST-STATS/lost-stats.github.io/source/Data/mtcars.csv&#39;) # Include a linear, squared, and cubic term using the I() function. # N.B. Python uses ** for exponentiation (^ means bitwise xor) model1 = sms.ols(&#39;mpg ~ hp + I(hp**2) + I(hp**3) + cyl&#39;, data=df) print(model1.fit().summary()) # Include an interaction term and the variables by themselves using * # The interaction term is represented by hp:cyl model2 = sms.ols(&#39;mpg ~ hp * cyl&#39;, data=df) print(model2.fit().summary()) # Equivalently, you can request &quot;all quadratic interaction terms&quot; by doing model3 = sms.ols(&#39;mpg ~ (hp + cyl) ** 2&#39;, data=df) print(model3.fit().summary()) # Include only the interaction term and not the variables themselves with : # Hard to interpret! Occasionally useful though. model4 = sms.ols(&#39;mpg ~ hp : cyl&#39;, data=df) print(model4.fit().summary()) . R . # Load mtcars data data(mtcars) # Include a linear, squared, and cubic term using the I() function model1 &lt;- lm(mpg ~ hp + I(hp^2) + I(hp^3) + cyl, data = mtcars) # Include a linear, squared, and cubic term using the poly() function # The raw = TRUE option will give the exact same result as model1 # Omitting this will give you orthogonal polynomial terms, # which are not correlated with each other but are more difficult to interpret model2 &lt;- lm(mpg ~ poly(hp, 3, raw = TRUE) + cyl, data = mtcars) # Include an interaction term and the variables by themselves using * model3 &lt;- lm(mpg ~ hp*cyl, data = mtcars) # Include only the interaction term and not the variables themselves with : # Hard to interpret! Occasionally useful though. model4 &lt;- lm(mpg ~ hp:cyl, data = mtcars) . Stata . Stata allows interaction and polynomial terms using hashtags ## to join together variables to make interactions, or joining a variable with itself to get a polynomial. You must also specify whether each variable is continuous (prefix the variable with c.) or a factor (prefix with i.). . * Load auto data sysuse auto.dta, clear * Use ## to interact variables together and also include the variables individually * foreign is a factor variable so we prefix it with i. * weight is continuous so we prefix it with c. reg mpg c.weight##i.foreign * Use # to include just the interaction term and not the variables themselves * If one is a factor, this will include the effect of the continuous variable * For each level of the factor reg mpg c.weight#i.foreign * Interact a variable with itself to create a polynomial term reg mpg c.weight##c.weight##c.weight foreign .",
    "url": "/Model_Estimation/interaction_terms_and_polynomials.html",
    "relUrl": "/Model_Estimation/interaction_terms_and_polynomials.html"
  }
  ,"36": {
    "title": "Line Graph with Labels at the Beginning or End of Lines",
    "content": "Line Graph with Labels at the Beginning or End of Lines . A line graph is a common way of showing how a value changes over time (or over any other x-axis where there’s only one observation per x-axis value). It is also common to put several line graphs on the same set of axes so you can see how multiple values are changing together. . When putting multiple line graphs on the same set of axes, a good idea is to label the different lines on the lines themselves, rather than in a legend, which generally makes things easier to read. . Keep in Mind . Check the resulting graph to make sure that labels are legible, visible in the graph area, and don’t overlap. | . Also Consider . More generally, see Line graph and Styling line graphs. In particular, consider Styling line graphs in order to distinguish the lines by color, pattern, etc. in addition to labels | If there are too many lines to be able to clearly follow them, labels won’t help too much. Instead, consider Faceted graphs. | . Implementations . R . # If necessary, install ggplot2, lubridate, and directlabels # install.packages(c(&#39;ggplot2&#39;,&#39;directlabels&#39;, &#39;lubridate&#39;)) library(ggplot2) library(directlabels) # Load in Google Trends Nobel Search Data # Which contains the Google Trends global search popularity index for the four # research-based Nobel prizes over a month. df &lt;- read.csv(&#39;https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Presentation/Data/Line_Graph_with_Labels_at_the_Beginning_or_End_of_Lines/Research_Nobel_Google_Trends.csv&#39;) # Properly treat our date variable as a date # Not necessary in all applications of this technique. df$date &lt;- lubridate::ymd(df$date) # Construct our standard ggplot line graph # Drawing separate lines by name # And using the log of hits for visibility ggplot(df, aes(x = date, y = log(hits), color = name)) + labs(x = &quot;Date&quot;, y = &quot;Log of Google Trends Index&quot;)+ geom_line()+ # Since we are about to add line labels, we don&#39;t need a legend theme(legend.position = &quot;none&quot;) + # Add, from the directlabels package, # geom_dl, using method = &#39;last.bumpup&#39; to put the # labels at the end, and make sure that if they intersect, # one is bumped up geom_dl(aes(label = name), method = &#39;last.bumpup&#39;) + # Extend the x axis so the labels are visible - # Try the graph a few times until you find a range that works scale_x_date(limits = c(min(df$date), lubridate::ymd(&#39;2019-10-25&#39;))) . This results in: . . Stata . Unfortunately, performing this technique in Stata requires placing each text() label on the graph. However, this can be automated with the use of a for loop to build the code using locals. . * Load in Google Trends Nobel Search Data * Which contains the Google Trends global search popularity index for the four * research-based Nobel prizes over a month. import delimited &quot;https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Presentation/Data/Line_Graph_with_Labels_at_the_Beginning_or_End_of_Lines/Research_Nobel_Google_Trends.csv&quot;, clear * Convert the date variable to an actual date * (not necessary in all implementations) g ymddate = date(date, &quot;YMD&quot;) * Format the new variable as a date so we see it properly on the x-axis format ymddate %td * Graph log(hits) for visibility g loghits = log(hits) * Get the different prize types to graph levelsof name, l(names) * Figure out the last time period in the data set quietly summarize ymddate local lastday = r(max) * Start constructing a local that contains all the line graphs to graph local lines * Start constructing a local that contains the text labels to add local textlabs * Loop through each one foreach n in `names&#39; { * Add in the line graph code * by building on the local we already have (`lines&#39;) and adding a new twoway segment local lines `lines&#39; (line loghits ymddate if name == &quot;`n&#39;&quot;) * Figure out the value this line hits on the last point on the graph quietly summ loghits if name == &quot;`n&#39;&quot; &amp; ymddate == `lastday&#39; * The text command takes the y-value (from the mean we just took) * the x-value (the last day on the graph), * and the text label (the name we are working with) * Plus place(r) to put it to the RIGHT of that point local textlabs `textlabs&#39; text(`r(mean)&#39; `lastday&#39; &quot;`n&#39;&quot;, place(r)) } * Finally, graph our lines * with the twoway lines we&#39;ve specified, followed by the text labels * We&#39;re sure to remove the legend with legend(off) * and extend the x-axis so we can see the labels with xscale(range()) quietly summarize ymddate local start = r(min) local end = r(max) + 5 twoway `lines&#39;, `textlabs&#39; legend(off) xscale(range(`start&#39; `end&#39;)) xtitle(&quot;Date&quot;) ytitle(&quot;Log of Google Trends Index&quot;) . This results in: . .",
    "url": "/Presentation/line_graph_with_labels_at_the_beginning_or_end.html",
    "relUrl": "/Presentation/line_graph_with_labels_at_the_beginning_or_end.html"
  }
  ,"37": {
    "title": "Linear Hypothesis Tests",
    "content": "Linear Hypothesis Tests . Most regression output will include the results of frequentist hypothesis tests comparing each coefficient to 0. However, in many cases, you may be interested in whether a linear sum of the coefficients is 0. For example, in the regression . You may be interested to see if and (both binary variables) cancel each other out. So you would want to do a test of . . Alternately, you may want to do a joint significance test of multiple linear hypotheses. For example, you may be interested in whether or are nonzero and so would want to jointly test the hypotheses and rather than doing them one at a time. Note the and here, since if either one or the other is rejected, we reject the null. . Keep in Mind . Be sure to carefully interpret the result. If you are doing a joint test, rejection means that at least one of your hypotheses can be rejected, not each of them. And you don’t necessarily know which ones can be rejected! | Generally, linear hypothesis tests are performed using F-statistics. However, there are alternate approaches such as likelihood tests or chi-squared tests. Be sure you know which on you’re getting. | Conceptually, what is going on with linear hypothesis tests is that they compare the model you’ve estimated against a more restrictive one that requires your restrictions (hypotheses) to be true. If the test you have in mind is too complex for the software to figure out on its own, you might be able to do it on your own by taking the sum of squared residuals in your original unrestricted model (), estimate the alternate model with the restriction in place () and then calculate the F-statistic for the joint test using . | . Also Consider . The process for testing a nonlinear combination of your coefficients, for example testing if or , is generally different. See Nonlinear hypothesis tests. | . Implementations . R . Linear hypothesis test in R can be performed for most regression models using the linearHypothesis() function in the car package. See this guide for more information. . # If necessary # install.packages(&#39;car&#39;) library(car) data(mtcars) # Run our model m1 &lt;- lm(mpg ~ hp + disp + am + wt, data = mtcars) # Test a linear combination of coefficients linearHypothesis(m1, c(&#39;hp + disp = 0&#39;)) # Test joint significance of multiple coefficients linearHypothesis(m1, c(&#39;hp = 0&#39;,&#39;disp = 0&#39;)) # Test joint significance of multiple linear combinations linearHypothesis(m1, c(&#39;hp + disp = 0&#39;,&#39;am + wt = 0&#39;)) . Stata . Tests of coefficients in Stata can generally be performed using the built-in test command. . * Load data sysuse auto.dta reg mpg headroom trunk prince rep78 * Make sure to run tests while the previous regression is still in memory * Test joint significance of multiple coefficients test headroom trunk * testparm does the same thing but allows wildcards to select coefficients * this will test the joint significance of every variable with an e in it testparm *e* * Test a linear combination of the coefficients test headroom + trunk = 0 * Test multiple linear combinations by accumulating them one at a time test headroom + trunk = 0 test price + rep78 = 0, accumulate .",
    "url": "/Model_Estimation/linear_hypothesis_tests.html",
    "relUrl": "/Model_Estimation/linear_hypothesis_tests.html"
  }
  ,"38": {
    "title": "Linear Mixed-Effects Regression",
    "content": "Linear Mixed-Effects Regression . Mixed-effects regression goes by many names, including hierarchical linear model, random coefficient model, and random parameter models. In a mixed-effects regression, some of the parameters are “random effects” which are allowed to vary over the sample. Others are “fixed effects”, which are not. Note that this use of the term “fixed effects” is not the same as in fixed effects regression. . For example, consider the model . The intercept has a $j$ subscript and is allowed to vary over the sample at the level, where may indicate individual or group, depending on context. The slope on , , is similarly allowed to vary over the sample. These are random effects. is not allowed to vary over the sample and so is fixed. . The random parameters have their own “level-two” equations, which may or may not include level-two covariates. . For more information see Wikipedia. . Keep in Mind . The assumptions necessary to use a mixed-effects model in general are the same as for most linear models. However, in addition, mixed-effects models assume that the error terms at different levels are unrelated. | At the second level, statistical power depends on the number of different values there are. Mixed-effects models may perform poorly if the coefficient is allowed to vary over only a few groups. | There’s no need to stop at two levels - the second-level coefficients can also be allowed to vary at a higher level. | . Also Consider . There are many variations of mixed-effects models for working with non-linear data, see nonlinear mixed-effects models. | If the goal is making predictions within subgroups, you may want to consider multi-level regression with poststratification. | . Implementations . R . One common way to fit mixed-effects models in R is with the lmer function in the lme4 package. To fit fully Bayesian models you may want to consider instead using STAN with the rstan package. See the multi-level regression with poststratification page for more information. . # Install lme4 if necessary # install.packages(&#39;lme4&#39;) # Load up lme4 library(lme4) # Load up university instructor evaluations data from lme4 data(InstEval) # We&#39;ll be treating lecture age as a numeric variable InstEval$lectage &lt;- as.numeric(InstEval$lectage) # Let&#39;s look at the relationship between lecture ratings andhow long ago the lecture took place # with a control for whether the lecture was a service lecture ols &lt;- lm(y ~ lectage + service, data = InstEval) summary(ols) # Now we will use lmer to allow the intercept to vary at the department level me1 &lt;- lmer(y ~ lectage + service + (1 | dept), data = InstEval) summary(me1) # Now we will allow the slope on lectage to vary at the department level me2 &lt;- lmer(y ~ lectage + service + (-1 + lectage | dept), data = InstEval) summary(me2) # Now both the intercept and lectage slope will vary at the department level me3 &lt;- lmer(y ~ lectage + service + (lectage | dept), data = InstEval) summary(me3) . Stata . Stata has a family of functions based around the mixed command that can estimate mixed-effects models. . * Load NLS-W data sysuse nlsw88.dta, clear * We are going to estimate the relationship between hourly wage and job tenure * with a contorl for marital status * Without mixed effects reg wage tenure married * Now we will allow the intercept to vary with occupation mixed wage tenure married || occupation: * Next we will allow the slope on tenure to vary with occupation mixed wage tenure married || occupation: tenure, nocons * Now, both! mixed wage tenure married || occupation: tenure * Finally we will allow the intercept and tenure slope to vary over both occupation * and age mixed wage tenure married || occupation: tenure || age: tenure .",
    "url": "/Model_Estimation/linear_mixed_effects_regression.html",
    "relUrl": "/Model_Estimation/linear_mixed_effects_regression.html"
  }
  ,"39": {
    "title": "Logit Model",
    "content": "Logit Regressions . A logistical regression (Logit) is a statistical method for a best-fit line between a binary [0/1] outcome variable and any number of independent variables. Logit regressions follow a logistical distribution and the predicted probabilities are bounded between 0 and 1. . For more information about Logit, see Wikipedia: Logit. . Keep in Mind . The beta coefficients from a logit model are maximum likelihood estimations. They are not the marginal effect, as you would see in an OLS estimation. So you cannot interpret the beta coefficient as a marginal effect of on . | To obtain the marginal effect, you need to perform a post-estimation command to discover the marginal effect. In general, you can ‘eye-ball’ the marginal effect by dividing the logit beta coefficient by 4. | . Implementations . Gretl . # Load auto data open auto.gdt # Run logit using the auto data, with mpg as the outcome variable # and headroom, trunk, and weight as predictors logit mpg const headroom trunk weight . R . R can run a logit regression using the glm() function. However, to get marginal effects you will need to calculate them by hand or use a package. We will use the mfx package, although the margins package is another good option, which produces tidy model output. . # If necessary, install the mfx package # install.packages(&#39;mfx&#39;) # mfx is only needed for the marginal effect, not the regression itself library(mfx) # Load mtcars data data(mtcars) # Use the glm() function to run logit # Here we are predicting engine type using # miles per gallon and number of cylinders as predictors my_logit &lt;- glm(vs ~ mpg + cyl, data = mtcars, family = binomial(link = &#39;logit&#39;)) # The family argument says we are working with binary data # and using a logit link function (rather than, say, probit) # The results summary(my_logit) # Marginal effects logitmfx(vs ~ mpg + cyl, data = mtcars) . Stata . * Load auto data sysuse auto.dta * Logit Estimation logit foreign mpg weight headroom trunk * Recover the Marginal Effects (Beta Coefficient in OLS) margins, dydx(*) .",
    "url": "/Model_Estimation/logit_model.html",
    "relUrl": "/Model_Estimation/logit_model.html"
  }
  ,"40": {
    "title": "Marginal Effects Plots for Interactions with Continuous Variables",
    "content": "Marginal Effects Plots for Interactions with Continuous Variables . In many contexts, the effect of one variable on another might be allowed to vary. For example, the relationship between income and mortality is nonlinear, so the effect of an additional dollar of income on mortality is different for someone earning $20,000/year than for someone earning $100,000/year. Or maybe the relationship between income and mortality differs depending on how many years of education you have. . A marginal effects plot displays the effect of on for different values of (or ). The plot will often include confidence intervals as well. The same code will often work if there’s not an explicit interaction, but you are, for example, estimating a logit model where the effect of one variable changes with the values of the others. . Keep in Mind . Interactions often have poor statistical power, and you will generally need a lot of observations to tell if the effect of $X$ on is different for two given different values of . | Make sure your graph has clearly labeled axes, so readers can tell whether your y-axis is the predicted value of $Y$ or the marginal effect of on . | . Also Consider . Consider performing a power analysis of interaction terms before running your analysis to see whether you have the statistical power for your interactions | Average marginal effects or marginal effects at the mean can be used to get a single marginal effect averaged over your sample, rather than showing how it varies across the sample. | Marginal effects plots for interactions with categorical variables | . Implementations . R . The interplot package can plot the marginal effect of a variable (y-axis) against different values of some variable. If instead you want the predicted values of on the y-axis, look at the ggeffects package. . # Install relevant packages, if necessary: # install.packages(c(&#39;ggplot2&#39;, &#39;interplot&#39;)) # Load in ggplot2 and interplot library(ggplot2) library(interplot) # Load in the txhousing data data(txhousing) # Estimate a regression with a nonlinear term cubic_model &lt;- lm(sales ~ listings + I(listings^2) + I(listings^3), data = txhousing) # Get the marginal effect of var1 (listings) # at different values of var2 (listings), with confidence ribbon. # This will return a ggplot object, so you can # customize using ggplot elements like labs(). interplot(cubic_model, var1 = &quot;listings&quot;, var2 = &quot;listings&quot;)+ labs(x = &quot;Number of Listings&quot;, y = &quot;Marginal Effect of Listings&quot;) # Try setting adding listings*date to the regression model # and then in interplot set var2 = &quot;date&quot; to get the effect of listings at different values of date . This results in: . . Stata . We will use the marginsplot command, which requires Stata 12 or higher. . * Load in the National Longitudinal Survey of Youth - Women sample sysuse nlsw88.dta * Perform a regression with a nonlinear term regress wage c.tenure##c.tenure * Use margins to calculate the marginal effects * Put the variable we&#39;re interested in getting the effect of in dydx() * And the values we want to evaluate it at in at() margins, dydx(tenure) at(tenure = (0(1)26)) * (If we had interacted with another variable, say age, we would specify similarly, * with at(age = (start(count-by)end))) * Then, marginsplot * The recast() and recastci() options make the effect/CI show up as a line/area * Remove to get points/lines instead. marginsplot, xtitle(&quot;Tenure&quot;) ytitle(&quot;Marginal Effect of Tenure&quot;) recast(line) recastci(rarea) . This results in: . .",
    "url": "/Presentation/marginal_effects_plots_for_interactions_with_continuous_variables.html",
    "relUrl": "/Presentation/marginal_effects_plots_for_interactions_with_continuous_variables.html"
  }
  ,"41": {
    "title": "McFadden's Choice Model (Alternative-Specific Conditional Logit)",
    "content": "McFadden’s Choice Model (Alternative-Specific Conditional Logit) . Discrete choice models are a regression method used to predict a categorical dependent variable with more than two categories. For example, a discrete choice model might be used to predict whether someone is going to take a train, car, or bus to work. . McFadden’s Choice Model is a discrete choice model that uses conditional logit, in which the variables that predict choice can vary either at the individual level (perhaps tall people are more likely to take the bus), or at the alternative level (perhaps the train is cheaper than the bus). . For more information, see Wikipedia: Discrete Choice . Keep in Mind . Just like other regression methods, the McFadden model does not guarantee that the estimates will be causal. Similarly, while the McFadden model is designed so that the results can be interpreted in terms of a “random utility” function, making inferences about utility functions does require additional assumptions. | The standard McFadden model assumes that the choice follows the Independence of Irrelevant Alternatives, which may be a strong assumption. There are variants of the McFadden model that relax this assumption. | If you are working with an estimation command that only allows alternative-specific predictors and not case-specific predictors, you can add them yourself by interacting the case-specific predictors with binary variables for the different alternatives. If is your case-specific variable and your alternatives are “train”, “bus”, and “car”, you’d add , , and to your model. These are your case-specific predictors. | Choice model regressions often have specific demands on how your data is structured. These vary across estimation commands and software packages. However, a common one is this (others will be pointed out in specific Implementations below): The data must contain a variable indicating the choice cases (i.e. you choose a car, that’s one case, then I choose a car, that’s a different case), a variable with the alternatives being chosen between, a binary variable equal to 1 for the alternative actually chosen (this should be 1 or TRUE exactly once within each choice case), and then variables that are case-specific or alternative-specific. | . In the below table, gives the choice case, gives the options, gives the choice, is a variable that varies at the alternative level, and is a variable that varies at the case level. . I Alts Chose X Y . 1 | A | 1 | 10 | 3 | . 1 | B | 0 | 20 | 3 | . 1 | C | 0 | 10.5 | 3 | . 2 | A | 0 | 8 | 5 | . 2 | B | 1 | 9 | 5 | . 3 | C | 0 | 1 | 5 | . This might be referred to as “long” choice data. “Wide” choice data is also common, and looks like: . I Chose Y XA XB XC . 1 | A | 3 | 10 | 20 | 10.5 | . 2 | B | 5 | 8 | 9 | 1 | . Also Consider . In order to relax the independence of irrelevant alternatives assumption and/or more closely model individual preferences, consider the mixed logit, nested logit or hierarchical Bayes conditional logit models. | . Implementations . R . We will implement McFadden’s choice model in R using the mlogit package, which can accept “wide” or “long” data in the mlogit.data function. . # If necessary, install mlogit package # install.packages(&#39;mlogit&#39;) library(mlogit) # Get Car data, in &quot;wide&quot; choice format data(Car) # For this we need to specify the choice variable with choice # whether it&#39;s currently in wide or long format with shape # the column numbers of the alternative-specific variables with varying. # We need alt.levels to tell us what our alternatives are (1-6, as seen in choice). # We also need sep = &quot;&quot; since our wide-format variable names are type1, type2, etc. # If the variable names were type_1, type_2, etc., we&#39;d need sep = &quot;_&quot;. # If this were long data we&#39;d also want: # the case identifier with id.var (for individuals) and/or chid.var # (for multiple choices within individuals) # And a variable indicating the alternatives with alt.var # But could skip the alt.levels and sep arguments mlogit.Car &lt;- mlogit.data(Car, choice = &#39;choice&#39;, shape = &#39;wide&#39;, varying = 5:70, alt.levels = 1:6, sep=&quot;&quot;) # mlogit.Car is now in &quot;long&quot; format # Note that if we did start with &quot;long&quot; format we could probably skip the mlogit.data() step. # Now we can run the regression with mlogit(). # We &quot;regress&quot; the choice on the alternative-specific variables like type, fuel, and price # Then put a pipe separator | # and add our case-specific variables like college model &lt;- mlogit(choice ~ type + fuel + price | college, data = mlogit.Car) # Look at the results summary(model) . Stata . Stata has the McFadden model built in. We will estimate the model using the older asclogit command as well as the cmclogit command that comes with Stata 16. These commands require “long” choice data, as described in the Keep in Mind section. . * Load in car choice data webuse carchoice * To use asclogit, we &quot;regress&quot; our choice variable (purchase) * on any alternative-specific variables (dealers) * then we put our case ID variable consumerid in case() * and our variable specifying alternatives, car, in alternatives() * then finally we put any case-specific variables like gender and income, in casevars() asclogit purchase dealers, case(consumerid) alternatives(car) casevars(gender income) * To use cmclogit, we first declare our data to be choice data with cmset * specifying our case ID variable and then the set of alternatives cmset consumerid car * Now that Stata knows the structure, we can omit those parts from the asclogit * specification, but the rest stays the same! cmclogit purchase dealers, casevars(gender income) . Why bother with the cmclogit version? cmset gives you a lot more information about your data, and makes it easy to transition between different choice model types, including those incorporating panel data (each person makes multiple choices). .",
    "url": "/Model_Estimation/mcfaddens_choice_model.html",
    "relUrl": "/Model_Estimation/mcfaddens_choice_model.html"
  }
  ,"42": {
    "title": "Merging Shape Files",
    "content": "Merging Shape Files . When we work with spatial anaylsis, it is quite often we need to deal with data in different format and at different scales. For example, I have nc data with global pm2.5 estimation with resolution. But I want to see the pm2.5 estimation in municipal level. I need to integrate my nc file into my municipality shp file so that I can group by the data into municipal level and calculate the mean. Then, I can make a map of it. . In this page, I will use Brazil’s pm2.5 estimation and its shp file in municipal level. . Keep in Mind . It doesn’t have to be nc file to map into the shp file, any format that can read in and convert to a sf object works. But the data has to have geometry coordinates(longitude and latitude). | . Implementations . R . Unusually for LOST, the example data files cannot be accessed from the code directly. Please visit this page and download both files to your working directory before running this code. . It is also strongly recommended that you find a high-powered computer or cloud service before attempting to run this code, as it requires a lot of memory. . # If necesary # install.packages(c(&#39;ncdf4&#39;,&#39;sp&#39;,&#39;raster&#39;,&#39;dplyr&#39;,&#39;sf&#39;,&#39;ggplot2&#39;,&#39;reprex&#39;,&#39;ggsn&#39;)) # Load packages library(ncdf4) library(sp) library(raster) library(dplyr) library(sf) library(ggplot2) library(reprex) ### Step 1: Read in nc file as a dataframe* pm2010 = nc_open(&quot;GlobalGWRwUni_PM25_GL_201001_201012-RH35_Median_NoDust_NoSalt.nc&quot;) nc.brick = brick(&quot;GlobalGWRwUni_PM25_GL_201001_201012-RH35_Median_NoDust_NoSalt.nc&quot;) # Check the dimensions dim(nc.brick) # Turn into a data frame for use nc.df = as.data.frame(nc.brick[[1]], xy = T) head(nc.df) ### Step 2: Filter out a specific country. # Global data is very big. I am going to focus only on Brazil. nc.brazil = nc.df %&gt;% filter(x &gt;= -73.59 &amp; x &lt;= 34.47 &amp; y &gt;= -33.45 &amp; y &lt;= 5.16) rm(nc.df) head(nc.brazil) ### Step 3: Change the dataframe to a sf object using the st_as_sf function pm25_sf = st_as_sf(nc.brazil, coords = c(&quot;x&quot;, &quot;y&quot;), crs = 4326, agr = &quot;constant&quot;) rm(nc.brazil) head(pm25_sf) ### Step 4: Read in the Brazil shp file. we plan to merge to Brazil_map_2010 = st_read(&quot;geo2_br2010.shp&quot;) head(Brazil_map_2010) ### Step 5: Intersect pm25 sf object with the shp file.* # Now let&#39;s use a sample from pm25 data and intersect it with the shp file. Since the sf object is huge, I recommend running the analysis on a cloud server pm25_sample = sample_n(pm25_sf, 1000, replace = FALSE) # Now look for the intersection between the pollution data and the Brazil map to merge them pm25_municipal_2010 = st_intersection(pm25_sample, Brazil_map_2010) head(pm25_municipal_2010) ### Step 6: Make a map using ggplot pm25_municipal_2010 = pm25_municipal_2010 %&gt;% select(1,6) pm25_municipal_2010 = st_drop_geometry(pm25_municipal_2010) Brazil_pm25_2010 = left_join(Brazil_map_2010, pm25_municipal_2010) ggplot(Brazil_pm25_2010) + # geom_sf creates the map we need geom_sf(aes(fill = -layer), alpha=0.8, lwd = 0, col=&quot;white&quot;) + # and we fill with the pollution concentration data scale_fill_viridis_c(option = &quot;viridis&quot;, name = &quot;PM25&quot;) + ggtitle(&quot;PM25 in municipals of Brazil&quot;)+ ggsn::blank() .",
    "url": "/Geo-Spatial/merging_shape_files.html",
    "relUrl": "/Geo-Spatial/merging_shape_files.html"
  }
  ,"43": {
    "title": "Nonstandard Errors",
    "content": "Nonstandard errors .",
    "url": "/Model_Estimation/Nonstandard_Errors/nonstandard_errors.html",
    "relUrl": "/Model_Estimation/Nonstandard_Errors/nonstandard_errors.html"
  }
  ,"44": {
    "title": "Ordinary Least Squares (Linear Regression)",
    "content": "Ordinary Least Squares (Linear Regression) . Ordinary Least Squares (OLS) is a statistical method that produces a best-fit line between some outcome variable and any number of predictor variables . These predictor variables may also be called independent variables or right-hand-side variables. . For more information about OLS, see Wikipedia: Ordinary Least Squares. . Keep in Mind . OLS assumes that you have specified a true linear relationship. | OLS results are not guaranteed to have a causal interpretation. Just because OLS estimates a positive relationship between and does not necessarily mean that an increase in will cause to increase. | OLS does not require that your variables follow a normal distribution. | . Also Consider . OLS standard errors assume that the model’s error term is IID, which may not be true. Consider whether your analysis should use heteroskedasticity-robust standard errors or cluster-robust standard errors. | If your outcome variable is discrete or bounded, then OLS is by nature incorrectly specified. You may want to use probit or logit instead for a binary outcome variable, or ordered probit or ordered logit for an ordinal outcome variable. | If the goal of your analysis is predicting the outcome variable and you have a very long list of predictor variables, you may want to consider using a method that will select a subset of your predictors. A common way to do this is a penalized regression method like LASSO. | In many contexts, you may want to include interaction terms or polynomials in your regression equation. | . Implementations . Gretl . # Load auto data open https://github.com/LOST-STATS/lost-stats.github.io/blob/master/Data/auto.gdt # Run OLS using the auto data, with mpg as the outcome variable # and headroom, trunk, and weight as predictors ols mpg const headroom trunk weight . Matlab . % Load auto data load(&#39;https://github.com/LOST-STATS/lost-stats.github.io/blob/master/Data/auto.mat&#39;) % Run OLS using the auto data, with mpg as the outcome variable % and headroom, trunk, and weight as predictors intercept = ones(length(headroom),1); X = [intercept headroom trunk weight]; [b,bint,r,rint,stats] = regress(mpg,X); . Python . &#39;&#39;&#39;Load R Datasets&#39;&#39;&#39; mtcars = sm.datasets.get_rdataset(&quot;mtcars&quot;).data # Fit OLS regression model to mtcars ols = smf.ols(formula= &#39;mpg ~ cyl + hp + wt&#39;, data= mtcars).fit() # Look at the OLS results print(ols.summary()) . R . # Load Data # data(mtcars) ## Optional: automatically loaded anyway # Run OLS using the mtcars data, with mpg as the outcome variable # and cyl, hp, and wt as predictors olsmodel &lt;- lm(mpg ~ cyl + hp + wt, data = mtcars) # Look at the results summary(olsmodel) . SAS . /* Load Data */ proc import datafile=&quot;C:mtcars.dbf&quot; out=fromr dbms=dbf; run; /* OLS regression */ proc reg; model mpg = cyl hp wt; run; . Stata . * Load auto data sysuse https://github.com/LOST-STATS/lost-stats.github.io/blob/master/Data/auto.dta * Run OLS using the auto data, with mpg as the outcome variable * and headroom, trunk, and weight as predictors regress mpg headroom trunk weight .",
    "url": "/Model_Estimation/ordinary_least_squares.html",
    "relUrl": "/Model_Estimation/ordinary_least_squares.html"
  }
  ,"45": {
    "title": "Penalized Regression",
    "content": "Penalized Regression . When running a regression, especially one with many predictors, the results have a tendency to overfit the data, reducing out-of-sample predictive properties. . Penalized regression eases this problem by forcing the regression estimator to shrink its coefficients towards 0 in order to avoid the “penalty” term imposed on the coefficients. This process is closely related to the idea of Bayesian shrinkage, and indeed standard penalized regression results are equivalent to regression performed using certain Bayesian priors. . Regular OLS selects coefficients to minimize the sum of squared errors: . Non-OLS regressions similarly select coefficients to minimize a similar objective function. Penalized regression adds a penalty term to that objective function, where is a tuning parameter that determines how harshly to penalize coefficients, and is the -norm of the coefficients, or . . Typically is set to 1 for LASSO regression (least absolute shrinkage and selection operator), which has the effect of tending to set coefficients to 0, i.e. model selection, or to 2 for Ridge Regression. Elastic net regression provides a weighted mix of LASSO and Ridge penalties, commonly referring to the weight as . . Keep in Mind . To avoid being penalized for a constant term, or by differences in scale between variables, it is a very good idea to standardize each variable (subtract the mean and divide by the standard deviation) before running a penalized regression. | Penalized regression can be run for logit and other kinds of regression, not just linear regression. Using penalties with general linear models like logit is common. | Penalized regression coefficients are designed to improve out-of-sample prediction, but they are biased. If the goal is estimation of a parameter, rather than prediction, this should be kept in mind. A common procedure is to use LASSO to select variables, and then run regular regression models with the variables that LASSO has selected. | The parameter is often chosen using cross-validation. Many penalized regression commands include an option to select by cross-validation automatically. | LASSO models commonly include variables along with polynomial transformation of those variables and interactions, allowing LASSO to determine which transformations are worth keeping. | . Also Consider . If it is not important to estimate coefficients but the goal is simply to predict an outcome, then there are many other machine learning methods that do so, and in some cases can handle higher dimensionality or work with smaller samples. | . Implementations . R . We will use the glmnet package. . # Install glmnet and tidyverse if necessary # install.packages(&#39;glmnet&#39;, &#39;tidyverse&#39;) # Load glmnet library(glmnet) # Load iris data data(iris) # Create a matrix with all variables other than our dependent vairable, Sepal.Length # and interactions. # -1 to omit the intercept M &lt;- model.matrix(lm(Sepal.Length ~ (.)^2 - 1, data = iris)) # Add squared terms of numeric variables numeric.var.names &lt;- names(iris)[2:4] M &lt;- cbind(M,as.matrix(iris[,numeric.var.names]^2)) colnames(M)[16:18] &lt;- paste(numeric.var.names,&#39;squared&#39;) # Create a matrix for our dependent variable too Y &lt;- as.matrix(iris$Sepal.Length) # Standardize all variables M &lt;- scale(M) Y &lt;- scale(Y) # Use glmnet to estimate penalized regression # We pick family = &quot;gaussian&quot; for linear regression; # other families work for other kinds of data, like binomial for binary data # In each case, we use cv.glmnet to pick our lambda value using cross-validation # using nfolds folds for cross-validation # Note that alpha = 1 picks LASSO cv.lasso &lt;- cv.glmnet(M, Y, family = &quot;gaussian&quot;, nfolds = 20, alpha = 1) # We might want to see how the choice of lambda relates to out-of-sample error with a plot plot(cv.lasso) # After doing CV, we commonly pick the lambda.min for lambda, # which is the lambda that minimizes out-of-sample error # or lambda.1se, which is one standard error above lambda.min, # which penalizes more harshly. The choice depends on context. lasso.model &lt;- glmnet(M, Y, family = &quot;gaussian&quot;, alpha = 1, lambda = cv.lasso$lambda.min) # coefficients are shown in the beta element. . means LASSO dropped it lasso.model$beta # Running Ridge, or mixing the two with elastic net, simply means picking # alpha = 0 (Ridge), or 0 &lt; alpha &lt; 1 (Elastic Net) cv.ridge &lt;- cv.glmnet(M, Y, family = &quot;gaussian&quot;, nfolds = 20, alpha = 0) ridge.model &lt;- glmnet(M, Y, family = &quot;gaussian&quot;, alpha = 0, lambda = cv.ridge$lambda.min) cv.elasticnet &lt;- cv.glmnet(M, Y, family = &quot;gaussian&quot;, nfolds = 20, alpha = .5) elasticnet.model &lt;- glmnet(M, Y, family = &quot;gaussian&quot;, alpha = .5, lambda = cv.elasticnet$lambda.min) . Stata . Penalized regression is one of the few machine learning algorithms that Stata does natively. This requires Stata 16. If you do not have Stata 16, you can alternately perform some forms of penalized regression by installing the lars package using ssc install lars. . * Use NLSY-W data sysuse nlsw88.dta, clear * Construct all squared and interaction terms by loop so we don&#39;t have to specify them all * by hand in the regression function local numeric_vars = &quot;age grade hours ttl_exp tenure&quot; local factor_vars = &quot;race married never_married collgrad south smsa c_city industry occupation union&quot; * Add all squares foreach x in `numeric_vars&#39; { g sq_`x&#39; = `x&#39;^2 } * Turn all factors into dummies so we can standardize them local faccount = 1 local dummy_vars = &quot;&quot; foreach x in `factor_vars&#39; { xi i.`x&#39;, pre(f`count&#39;_) local count = `count&#39; + 1 } * Add all numeric-numeric interactions; these are easy * factor interactions would need a more thorough loop forvalues i = 1(1)5 { local next_i = `i&#39;+1 forvalues j = `next_i&#39;(1)5 { local namei = word(&quot;`numeric_vars&#39;&quot;,`i&#39;) local namej = word(&quot;`numeric_vars&#39;&quot;,`j&#39;) g interact_`i&#39;_`j&#39; = `namei&#39;*`namej&#39; } } * Standardize everything foreach var of varlist `numeric_vars&#39; f*_* interact_* { qui summ `var&#39; qui replace `var&#39; = (`var&#39; - r(mean))/r(sd) } * Use the lasso command to run LASSO * using sel(cv) to select lambda using cross-validation * we specify a linear model here, but logit/probit/poisson would work lasso linear wage `numeric_vars&#39; f*_* interact_*, sel(cv) * get list of included coefficients lassocoef * We can use elasticnet to run Elastic Net * By default, alpha will be selected by cross-validation as well elasticnet linear wage `numeric_vars&#39; f*_* interact_*, sel(cv) .",
    "url": "/Machine_Learning/penalized_regression.html",
    "relUrl": "/Machine_Learning/penalized_regression.html"
  }
  ,"46": {
    "title": "Probit Model",
    "content": "Probit Regressions . A Probit regression is a statistical method for a best-fit line between a binary [0/1] outcome variable and any number of independent variables. Probit regressions follow a standard normal probability distribution and the predicted values are bounded between 0 and 1. . For more information about Probit, see Wikipedia: Probit. . Keep in Mind . The beta coefficients from a probit model are maximum likelihood estimations. They are not the marginal effect, as you would see in an OLS estimation. So you cannot interpret the beta coefficient as a marginal effect of on . | To obtain the marginal effect, you need to perform a post-estimation command to discover the marginal effect. In general, you can ‘eye-ball’ the marginal effect by dividing the probit beta coefficient by 2.5. | . Implementations . R . R can run a probit regression using the glm() function. However, to get marginal effects you will need to calculate them by hand or use a package. We will use the mfx package, although the margins package is another good option, which produces tidy model output. . # If necessary, install the mfx package # install.packages(&#39;mfx&#39;) # mfx is only needed for the marginal effect, not the regression itself library(mfx) # Load mtcars data data(mtcars) # Use the glm() function to run probit # Here we are predicting engine type using # miles per gallon and number of cylinders as predictors my_probit &lt;- glm(vs ~ mpg + cyl, data = mtcars, family = binomial(link = &#39;probit&#39;)) # The family argument says we are working with binary data # and using a probit link function (rather than, say, logit) # The results summary(my_probit) # Marginal effects probitmfx(vs ~ mpg + cyl, data = mtcars) . Stata . * Load auto data sysuse auto.dta * Probi Estimation probit foreign mpg weight headroom trunk * Recover the Marginal Effects (Beta Coefficient in OLS) margins, dydx(*) . Gretl . # Load auto data open auto.gdt # Run probit using the auto data, with mpg as the outcome variable # and headroom, trunk, and weight as predictors probit mpg const headroom trunk weight .",
    "url": "/Model_Estimation/probit_model.html",
    "relUrl": "/Model_Estimation/probit_model.html"
  }
  ,"47": {
    "title": "Random/Mixed Effects in Linear Regression",
    "content": "Random/Mixed Effects in Linear Regression . In panel data, we often have to deal with unobserved heterogeneity among the units of observation that are observed over time. If we assume that the unobserved heterogeneity is uncorrelated with the independent variables, we can use random effects model. Otherwise, we may consider fixed effects. In practice, random effects and fixed effects are often combined to implement a mixed effects model. Mixed refers to the fact that these models contain both fixed, and random effects. . For more information, see Wikipedia: Random Effects Model . Keep in Mind . To use random effects model, you must observe the same person multiple times (panel data). | If unobserved heterogeneity is correlated with independent variables, the random effects estimator is biased and inconsistent. | However, even if unobserved heterogeneity is expected to be correlated with independent variables, the fixed effects model may have high standard errors if the number of observation per unit of observation is very small. Random effects maybe considered in such cases. | Additionally, modeling the correlation between the indepdendent variables and the random effect by using variables in predicting the random effect can account for this problem | . Also Consider . Consider Fixed effects if unobserved heterogeneity and independent variables are correlated or if only within-variation is desired. | Hauman Tests are often used to inform us about the appropiateness of fixed effects models vs. random effects models in which only the intercept is random. | Clustering your error | . Implementations . We continue from our the example in Fixed effects. In that example we estimated a fixed effect model of the form: . That is, average earnings of graduates of an institution depends on proportion employed, after controlling for time and institution fixed effects. But, some institutions have one observation, and the average number of observations is 5.1. We may be worried about the precision of our estimates. So, we may choose to use random effects for intercepts by institution to estimate the model even if we think . That is, we choose possiblity of bias over variance. . R . Several packages can be used to implement a random effects model in R - such as lme4 and nlme. lme4 is more widely used. The example that follows uses the lme4 package. . # If necessary, install lme4 if(!require(lme4)){install.packages(&quot;lme4&quot;)} library(lme4) # Read in data from the College Scorecard df &lt;- read.csv(&#39;https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv&#39;) # Calculate proportion of graduates working df$prop_working &lt;- df$count_working/(df$count_working + df$count_not_working) # We write the mixed effect formula for estimation in lme4 as: # dependent_var ~ # covariates (that can include fixed effects) + # random effects - we need to specify if our model is random effects in intercepts or in slopes. In our example, we suspect random effects in intercepts at institutions. So we write &quot;...+(1 | inst_name), ....&quot; If we wanted to specify a model where the coefficient on prop_working was also varying by institution - we would use (1 + open | inst_name). # Here we regress average earnings graduates in an institution on prop_working, year fixed effects and random effects in intercepts for institutions. relm_model &lt;- lmer(earnings_med ~ prop_working + factor(df$year) + (1 | inst_name), data = df) # Display results summary(relm_model) # We note that comparing with the fixed effects model, our estimates are more precise. But, the correlation between X`s and errors suggest bias in our mixed effect model, and we do see a large increase in estimated beta. . Stata . We will estimate a mixed effects model using Stata using the built in xtreg command. . * Obtain same data from Fixed Effect tutorial import delimited &quot;https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Fix ed_Effects_in_Linear_Regression/Scorecard.csv&quot;, clear * Data cleaning * We are turning missings are written as &quot;NA&quot; into numeric destring count_not_working count_working earnings_med, replace force * Calculate the proportion working g prop_working = count_working/(count_working + count_not_working) * xtset requires that the individual identifier be a numeric variable encode inst_name, g(name_number) * Set the data as panel data with xtset xtset name_number * Use xtreg with the &quot;re&quot; option to run random effects on institution intercepts * Regressing earnings_med on prop_working * with random effects for name_number (implied by re) * and also year fixed effects (which we&#39;ll add manually with i.year) xtreg earnings_med prop_working i.year, re * We note that comparing with the fixed effects model, our estimates are more precise. But, correlation between X`s and errors suggest bias in our random effect model, and we do see a large increase in estimated beta. .",
    "url": "/Model_Estimation/random_mixed_effects_estimation.html",
    "relUrl": "/Model_Estimation/random_mixed_effects_estimation.html"
  }
  ,"48": {
    "title": "Regression Discontinuity Design",
    "content": "Regression Discontinuity Design . Regression discontinuity (RDD) is a research design for the purposes of causal inference. It can be used in cases where treatment is assigned based on a cutoff value of a “running variable”. For example, perhaps students in a school take a test in 8th grade. Students who score 30 or below are assigned to remedial classes, while students scoring above 30 stay in regular classes. Regression discontinuity could be applied to this setting with test score as a running variable and 30 as the cutoff to look at the effects of remedial classes. . Regression discontinuity works by focusing on the cutoff. It makes an estimate of what the outcome is within a narrow bandwidth to the left of the cutoff, and also makes an estimate of what the outcome is to the right of the cutoff. Then it compares them to generate a treatment effect estimate. . See Wikpedia: Regression Discontinuity Design for more information. . Regression discontinuity receives a lot of attention because it relies on what some consider to be plausible assumptions. If the running variable is finely measured and is not being manipulated, then one can argue that being just to the left or the right of a cutoff is effectively random (someone getting a 30 or 31 on the test can basically be down to bad luck on the day) and so this approach by itself can remove confounding from lots of factors. . Keep in Mind . There are many, many options to choose when performing an RDD. Bandwidth selection procedure, polynomial terms, bias correction, etc. etc.. Please check the help file for your command of choice closely, and ensure you know what kind of analysis you’re about to run. Don’t assume the defaults are correct. | Regression discontinuity relies on the absence of manipulation of the running variable. In the test score example, if the teachers scoring the exam nudge a few students from 30 to 31 so they can avoid remedial classes, RDD doesn’t work any more. | Because the method relies on isolating a narrow bandwidth around the cutoff, RDD doesn’t work quite the same if the running variable is discrete and split into a small number of groups. You want a running variable with a lot of different values! See Kolesár and Rothe (2018) for more information. | In order to improve statistical performance, regression discontinuity designs often incorporate information from data points far away from the cutoff to improve the estimate of what the outcome is near the cutoff. This can be done nonparametrically, but is most often done by fitting a separate polynomial function for the running variable on either side of the cutoff. A temptation is to use a very high-order polynomial (say, and ) to improve fit. However, in general a low-order polynomial is probably a better idea. See Gelman and Imbens 2019 for more information. | Regression discontinuity designs are very well-suited to graphical demonstrations of the method. Software packages designed for RDD specifically will almost always provide an easy method for creating these graphs, and it is rare that you will not want to do this. However, do keep in mind that graphs can sometimes obscure meaningfully large effects. See Kirabo Jackson for an explanation. | Regression discontinuities can be sharp, where everyone to one side of the cutoff is treated and nobody on the other side is, or fuzzy, where the probability of treatment changes across the cutoff but assignment isn’t perfect. Most RDD packages can handle both. The intuition for both is similar, but the statistical properties of sharp designs are generally stronger. Fuzzy RDD can be thought of as similar to using an instrumental variables estimator in a case of imperfect random assignment in an experiment. Covariates are generally not necessary in a sharp RDD but may be advisable in a fuzzy one. | . Also Consider . The Regression Kink Design is an extension of RDD that looks for a change in a relationship between the running variable and the outcome, i.e. the slope, at the cutoff, rather than a change in the predicted outcome. | Regression discontinuity designs are often accompanied by placebo tests, where the same RDD is run again, but with a covariate or some other non-outcome measure used as the outcome. If the RDD shows a significant effect for the covariates, this suggests that balancing did not occur properly and there may be an issue with the RDD assumptions. | Part of performing an RDD is selecting a bandwidth around the cutoff to focus on. This can be done by context, but more commonly there are data-based methods for selecting a bandwidth Check your RDD command of choice to see what methods are available for selecting a bandwidth. | . Implementations . Stata . A standard package for performing regression discontinuity in Stata is rdrobust, installable from scc. . * If necessary * ssc install rdrobust * Load RDD of house elections from the R package rddtools, * and originally from Lee (2008) https://www.sciencedirect.com/science/article/abs/pii/S0304407607001121 import delimited &quot;https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Estimation/Data/Regression_Discontinuity_Design/house.csv&quot;, clear * x is &quot;vote margin in the previous election&quot; and y is &quot;vote margin in this election&quot; * If we want to specify options for bandwidth selection, we can run rdbwselect directly. * Otherwise, rdrobust will run it with default options by itself * c(0) indicates that treatment is assigned at 0 (i.e. someone gets more votes than the opponent) rdbwselect y x, c(0) * Run a sharp RDD with a second-order polynomial term rdrobust y x, c(0) p(2) * Run a fuzzy RDD * We don&#39;t have a fuzzy RDD in this data, but let&#39;s create one, where * probability of treatment jumps from 20% to 60% at the cutoff g treatment = (runiform() &lt; .2)*(x &lt; 0) + (runiform() &lt; .6)*(x &gt;= 0) rdrobust y x, c(0) fuzzy(treatment) * Generate a standard RDD plot with a polynomial of 2 (default is 4) rdplot y x, c(0) p(2) . R . There are several packages in R designed for the estimation of RDD. Three prominent options are rdd, rddtools, and rdrobust. See this article for comparisons between them in terms of their strengths and weaknesses. The article, considering the verisons of the packages available in 2017, recommends rddtools for assumption and sensitivity checks, and rdrobust for bandwidth selection and treatment effect estimation. We will consider rdrobust here. See the rddtools walkthrough for a detailed example of the use of rddtools. . # If necessary # install.packages(&#39;rdrobust&#39;) library(rdrobust) # Load RDD of house elections from the R package rddtools, # and originally from Lee (2008) https://www.sciencedirect.com/science/article/abs/pii/S0304407607001121 df &lt;- read.csv(&quot;https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Estimation/Data/Regression_Discontinuity_Design/house.csv&quot;) # x is &quot;vote margin in the previous election&quot; and y is &quot;vote margin in this election&quot; # If we want to specify options for bandwidth selection, we can run rdbwselect directly. # Otherwise, rdrobust will run it with default options by itself # c(0) indicates that treatment is assigned at 0 (i.e. someone gets more votes than the opponent) bandwidth &lt;- rdbwselect(df$y, df$x, c=0) # Run a sharp RDD with a second-order polynomial term rdd &lt;- rdrobust(df$y, df$x, c=0, p=2) summary(rdd) # Run a fuzzy RDD # We don&#39;t have a fuzzy RDD in this data, but let&#39;s create one, where # probability of treatment jumps from 20% to 60% at the cutoff N &lt;- nrow(df) df$treatment &lt;- (runif(N) &lt; .2)*(df$x &lt; 0) + (runif(N) &lt; .6)*(df$x &gt;= 0) rddfuzzy &lt;- rdrobust(df$y, df$x, c=0, p=2, fuzzy = df$treatment) summary(rddfuzzy) # Generate a standard RDD plot with a polynomial of 2 (default is 4) rdplot(df$y, df$x, c = 0, p = 2) .",
    "url": "/Model_Estimation/regression_discontinuity_design.html",
    "relUrl": "/Model_Estimation/regression_discontinuity_design.html"
  }
  ,"49": {
    "title": "Reshaping Data",
    "content": "Reshaping Data .",
    "url": "/Data_Manipulation/Reshaping/reshape.html",
    "relUrl": "/Data_Manipulation/Reshaping/reshape.html"
  }
  ,"50": {
    "title": "Reshape Panel Data from Long to Wide",
    "content": "Reshape Panel Data from Long to Wide . Panel data is data in which individuals are observed at multiple points in time. There are two standard ways of storing this data: . In wide format, there is one row per individual. Then, for each variable in the data set that varies over time, there is one column per time period. For example: . Individual FixedCharacteristic TimeVarying1990 TimeVarying1991 TimeVarying1992 . 1 | C | 16 | 20 | 22 | . 2 | H | 23.4 | 10 | 14 | . This format makes it easy to perform calculations across multiple years. . In long format, there is one row per individual per time period: . Individual FixedCharacteristic Year TimeVarying . 1 | C | 1990 | 16 | . 1 | C | 1991 | 20 | . 1 | C | 1992 | 22 | . 2 | H | 1990 | 23.4 | . 2 | H | 1991 | 10 | . 2 | H | 1992 | 14 | . This format makes it easy to run models like fixed effects. . Reshaping is the method of converting wide-format data to long and vice versa. . Keep in Mind . If your data has multiple observations per individual/time, then standard reshaping techniques generally won’t work. | It’s a good idea to check your data by directly looking at it both before and after a reshape to check that it worked properly. | . Also Consider . To go in the other direction, reshape from wide to long. | Determine the observation level of a data set. | . Implementations . R . There are many ways to reshape in R, including base-R reshape and the deprecated reshape2::melt and cast and tidyr::gather and spread. We will be using the tidyr package function pivot_wider, which requires tidyr version 1.0.0 or later. . # install.packages(&#39;tidyr&#39;) library(tidyr) # Load in population, which has one row per country per year data(&quot;population&quot;) # If we look at the data, we&#39;ll see that we have: # identifying information in &quot;country&quot;, # a time indicator in &quot;year&quot;, # and our values in &quot;population&quot; head(population) . Now we think: . Think about the set of variables that contain the values we’re interested in reshaping. Here’s it’s population. This list of variable names will be our values_from argument. | Think about what we want the new variables to be called. The variable variable says which variable we’re looking at. So that will be our names_from argument. And we want to specify that each variable represents population in a given year (rather than some other variable, so we’ll add “pop_” as our names_prefix. | pop_wide &lt;- pivot_wider(population, names_from = year, values_from = population, names_prefix = &quot;pop_&quot;) . Stata . * Load blood pressure data in long format, which contains * blood pressure both before and after a treatment for some patients sysuse bplong.dta . The next steps involve thinking: . Think about the set of variables that identify individuals. Here it’s patient. This will go in i(), so we have i(patient). | Think about the set of variables that vary across time. Here’s it’s bp. This will be one of our “stub”s. | Think about which variable separates the different time periods within individual. Here we have “when”, and this goes in j(), so we have j(when). | * Syntax is: * reshape wide stub, i(individualvars) j(newtimevar) * So we have reshape wide bp i(patient) j(when) * Note that simply typing reshape * will show the syntax for the function . With especially large datasets, the Gtools package provides a much faster version of reshape known as greshape. The syntax can function exactly the same, though they provide alternative syntax that you may find more intuitive. . * First, we will create a toy dataset that is very large to demonstrate the speed gains * If necessary, first install gtools: * ssc install gtools * Clear memory clear all * Turn on return message to see command run time set rmsg on * Set data size to 15 million observations set obs 15000000 * Create ten observations per person generate person_id = floor((_n-1)/10) * Number time periods from 1 to 10 for each person generate time_id = mod((_n-1), 10) + 1 *Create an income in each period generate income = round(rnormal(100, 20)) * Demonstrate the comparative speed of these two reshape approaches. * preserve and restore aren&#39;t a part of the reshape command; * they just store the current state of the data and then restore it, * so we can try our different reshape commands on the same data. *The traditional reshape command preserve reshape wide income, i(person_id) j(time_id) restore *The Gtools reshape command preserve greshape wide income, i(person_id) j(time_id) restore *The Gtools reshape command, alternative syntax preserve greshape wide income, by(person_id) keys(time_id) restore . Note: there is much more guidance to the usage of greshape on the Gtools reshape page. .",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html",
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html"
  }
  ,"51": {
    "title": "Reshape Panel Data from Wide to Long",
    "content": "Reshape Panel Data from Wide to Long . Panel data is data in which individuals are observed at multiple points in time. There are two standard ways of storing this data: . In wide format, there is one row per individual. Then, for each variable in the data set that varies over time, there is one column per time period. For example: . Individual FixedCharacteristic TimeVarying1990 TimeVarying1991 TimeVarying1992 . 1 | C | 16 | 20 | 22 | . 2 | H | 23.4 | 10 | 14 | . This format makes it easy to perform calculations across multiple years. . In long format, there is one row per individual per time period: . Individual FixedCharacteristic Year TimeVarying . 1 | C | 1990 | 16 | . 1 | C | 1991 | 20 | . 1 | C | 1992 | 22 | . 2 | H | 1990 | 23.4 | . 2 | H | 1991 | 10 | . 2 | H | 1992 | 14 | . This format makes it easy to run models like fixed effects. . Reshaping is the method of converting wide-format data to long and vice versa.. . Keep in Mind . If your data has multiple observations per individual/time, then standard reshaping techniques generally won’t work. | It’s a good idea to check your data by directly looking at it both before and after a reshape to check that it worked properly. | . Also Consider . To go in the other direction, reshape from long to wide. | Determine the observation level of a data set. | . Implementations . R . There are many ways to reshape in R, including base-R reshape and the deprecated reshape2::melt and cast and tidyr::gather and spread. We will be using the tidyr package function pivot_longer, which requires tidyr version 1.0.0 or later. . # install.packages(&#39;tidyr&#39;) library(tidyr) # Load in billboard, which has one row per song # and one variable per week, for its chart position each week data(&quot;billboard&quot;) # If we look at the data, we&#39;ll see that we have: # identifying information in &quot;artist&quot; and &quot;track&quot; # A variable consistent within individuals &quot;date.entered&quot; # and a bunch of variables containing position information # all named wk and then a number names(billboard) . Now we think: . Think about the set of variables that contain time-varying information. Here’s it’s wk1-wk76. So we can give a list of all the variables we want to widen using the tidyselect helper function starts_with(): starts_with(&quot;wk&quot;). This list of variable names will be our col argument. | Think about what we want the new variables to be called. I’ll call the week time variable “week” (this will be the names_to argument), and the data values currently stored in wk1-wk76 is the “position” (values_to). | Think about the values you want to be in your new time variable. The column names are wk1-wk76 but we want the variable to have 1-76 instead, so we’ll take out the “wk” with names_prefix = &quot;wk&quot;. | billboard_long &lt;- pivot_longer(billboard, col = starts_with(&quot;wk&quot;), names_to = &quot;week&quot;, names_prefix = &quot;wk&quot;, values_to = &quot;position&quot;, values_drop_na = TRUE) # values_drop_na says to drop any rows containing missing values of position. # If reshaping to create multiple variables, see the names_sep or names_pattern options. . Stata . * Load blood pressure data in wide format, which contains * bp_before and bp_after sysuse bpwide.dta . The next steps involve thinking: . Think about the set of variables that identify individuals. Here it’s patient. This will go in i(), so we have i(patient). | Think about the set of variables that vary across time. Here’s it’s bp_. Note the inclusion of the _, so that “before” and “after” will be our time periods. This will be one of our “stub”s. | Think about what we want the new time variable to be called. I’ll just call it “time”, and this goes in j(), so we have j(time). | * Syntax is: * reshape long stub, i(individualvars) j(newtimevar) * So we have reshape long bp_ i(patient) j(time) s * Where the s indicates that our time variable is a string (&quot;before&quot;, &quot;after&quot;) * Note that simply typing reshape * will show the syntax for the function . With especially large datasets, the Gtools package provides a much faster version of reshape known as greshape. The syntax can function exactly the same, though they provide alternative syntax that you may find more intuitive. . * If necessary, install gtools * ssc install gtools * First, we will create a toy dataset that is very large to demonstrate the speed gains * Clear memory clear all * Turn on return message to see command run time set rmsg on * Set data size to 15 million observations set obs 15000000 * Create an ID variable generate person_id = _n * Create 4 separate fake test scores per student generate test_score1 = round(rnormal(180, 30)) generate test_score2 = round(rnormal(180, 30)) generate test_score3 = round(rnormal(180, 30)) generate test_score4 = round(rnormal(180, 30)) * Demonstrate the comparative speed of these two reshape approaches * preserve and restore aren&#39;t a part of the reshape command; * they just store the current state of the data and then restore it, * so we can try our different reshape commands on the same data. * The traditional reshape command preserve reshape long test_score, i(person_id) j(test_number) restore *The Gtools reshape command preserve greshape long test_score, i(person_id) j(test_number) restore *The Gtools reshape command, alternative syntax preserve greshape long test_score, by(person_id) keys(test_number) restore . Note: there is much more guidance to the usage of greshape on the Gtools reshape page. .",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html",
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html"
  }
  ,"52": {
    "title": "Rowwise Calculations",
    "content": "Rowwise Calculations . When working with a table of data, it’s not uncommon to want to perform a calculations across many columns. For example, taking the mean of a bunch of columns for each row. . This is generally not difficult to do by hand if the number of variables being handled is small. For example, in most software packages, you could take the mean of columns A and B for each row by just asking for (A+B)/2. . This becomes more difficult, though, when the list of variables gets too long to type out by hand, or when the calculation doesn’t play nicely with being given columns. In these cases, approaches explicitly designed for rowwise calculations are necessary. . Keep in Mind . When incorporating lots of variables, rowwise calculations often allow you to select those variables by group, such as “all variables starting with r_”. When doing this, check ahead of time to make sure you aren’t accidentally incorporating unintended variables. | . Implementations . Stata . Stata has a series of built-in row operations that use the egen command. See help egen for the full list, and look for functions beginning with row like rowmean. . The full list includes: rowfirst and rowlast (first or last non-missing observation), rowmean, rowmedian, rowmax, rowmin, rowpctile, and rowtotal (the mean, median, max, min, given percentile, or sum of all the variables), and rowmiss and rownonmiss (the count of the number of missing or nonmissing observations across the variables). . The egenmore package, which can be installed with ssc install egenmore, adds rall, rany, and rcount (checks a condition for each variable and returns whether all are true, any are true, or the number that are true), rownvals and rowsvals (number of unique values for numeric and string variables, respectively), and rsum2 (rowtotal with some additional options). . * Get data on midwestern states import delimited using &quot;https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2/midwest.csv&quot; * There are three sets of variables starting with &quot;perc&quot; - let&#39;s make sure they * add up to 300 as they should * Use * as a wildcard for variable names egen total_perc = rowtotal(perc*) summ total_perc * They don&#39;t! Uh oh. * Let&#39;s just check the education variables - should add up to 100 * Use - to include all variables from one to the other * based on their current order in the data egen total_ed = rowtotal(perchsd-percprof) * Oh that explains it... * These aren&#39;t exclusive categories (HSD, college overlap) * and also leaves out non-HS graduates. summ total_ed . R . There are a few ways to perform rowwise operations in R. If you are summing the columns or taking their mean, rowSums and rowMeans in base R are great. . For something more complex, apply in base R can perform any necessary rowwise calculation, but pmap in the purrr package is likely to be faster. . In all cases, the tidyselect helpers in the dplyr package can help you to select many variables by name. . # If necessary # install.packages(c(&#39;purrr&#39;,&#39;ggplot2&#39;,&#39;dplyr&#39;)) # ggplot2 is only for the data data(midwest, package = &#39;ggplot2&#39;) # dplyr is for the tidyselect functions, the pipe %&gt;%, and select() to pick columns library(dplyr) # There are three sets of variables starting with &quot;perc&quot; - let&#39;s make sure they # add up to 300 as they maybe should # Use starts_with to select the variables # First, do it with rowSums, # either by picking column indices or using tidyselect midwest$rowsum_rowSums1 &lt;- rowSums(midwest[,c(12:16,18:20,22:26)]) midwest$rowsum_rowSums2 &lt;- midwest %&gt;% select(starts_with(&#39;perc&#39;)) %&gt;% rowSums() # Next, with apply - we&#39;re doing sum() here for the function # but it could be anything midwest$rowsum_apply &lt;- apply( midwest %&gt;% select(starts_with(&#39;perc&#39;)), MARGIN = 1, sum) # Next, two ways with purrr: library(purrr) # First, using purrr::reduce, which is good for some functions like summing # Note that . is the data set being sent by %&gt;% midwest &lt;- midwest %&gt;% mutate(rowsum_purrrReduce = reduce(select(., starts_with(&#39;perc&#39;)), `+`)) # More flexible, purrr::pmap, which works for any function # using pmap_dbl here to get a numeric variable rather than a list midwest &lt;- midwest %&gt;% mutate(rowsum_purrrpmap = pmap_dbl( select(.,starts_with(&#39;perc&#39;)), sum)) # So do we get 300? summary(midwest$rowsum_rowSums2) # Uh-oh... looks like we didn&#39;t understand the data after all. .",
    "url": "/Data_Manipulation/rowwise_calculations.html",
    "relUrl": "/Data_Manipulation/rowwise_calculations.html"
  }
  ,"53": {
    "title": "Scatterplot by Group on Shared Axes",
    "content": "Scatterplot by Group on Shared Axes . Scatterplots are a standard data visualization tool that allows you to look at the relationship between two variables and . If you want to see how the relationship between and might be different for Group A as opposed to Group B, then you might want to plot the scatterplot for both groups on the same set of axes, so you can compare them. . Keep in Mind . Scatterplots may not work well if the data is discrete, or if there are a large number of data points. | . Also Consider . Sometimes, instead of putting both Group A and Group B on the same set of axes, it makes more sense to plot them separately, and put the plots next to each other. See Faceted Graphs. | There are many ways to make the scatterplots of the two groups distinct. See Styling Scatterplots. | . Implementations . R . library(ggplot2) # Load auto data data(mtcars) # Make sure that our grouping variable is a factor # and labeled properly mtcars$Transmission &lt;- factor(mtcars$am, labels = c(&quot;Automatic&quot;, &quot;Manual&quot;)) # Put wt on the x-axis, mpg on the y-axis, ggplot(mtcars, aes(x = wt, y = mpg, # distinguish the Transmission values by color, color = Transmission)) + # make it a scatterplot with geom_point() geom_point()+ # And label properly labs(x = &quot;Car Weight&quot;, y = &quot;MPG&quot;) . This results in: . . Stata . * Load auto data sysuse auto.dta * Start a twoway command * Then, for each group, put its scatter command in () * Using if to plot each group separately * And specifying mcolor or msymbol (etc.) to differentiate them twoway (scatter weight mpg if foreign == 0, mcolor(black)) (scatter weight mpg if foreign == 1, mcolor(blue)) * Add a legend option so you know what the colors mean twoway (scatter weight mpg if foreign == 0, mcolor(black)) (scatter weight mpg if foreign == 1, mcolor(blue)), legend(lab(1 Domestic) lab(2 Foreign)) xtitle(&quot;Weight&quot;) ytitle(&quot;MPG&quot;) . This results in: . .",
    "url": "/Presentation/scatterplot_by_group_on_shared_axes.html",
    "relUrl": "/Presentation/scatterplot_by_group_on_shared_axes.html"
  }
  ,"54": {
    "title": "Styling Line Graphs",
    "content": "Styling Line Graphs . There are several ways of styling line graphs. The following examples demonstrate how to modify the appearances of the lines (type and sizes), as well chart titles and axes labels. . Keep in Mind . To get started on how to plot line graphs, see here. | Elements for customization include line thickness, line type (solid, dashed, etc.), shade, transparency, and color. | Color is one of the easiest ways to distinguish a large number of line graphs. If you have many line graphs overlaid and have to use black-and-white, consider different shades of black/gray. | . Implementation . R . ## If necessary ## install.packages(c(&#39;ggplot2&#39;,&#39;cowplot&#39;)) ## load packages library(ggplot2) ## Cowplot is just to join together the four graphs at the end library(cowplot) ## load data (the Economics dataset comes with ggplot2) eco_df &lt;- economics ## basic plot p1 &lt;- ggplot() + geom_line(aes(x=date, y = uempmed), data = eco_df) p1 ## Change line color and chart labels ## Note here that color is *outside* of the aes() argument, and so this will color the line ## If color were instead *inside* aes() and set to a factor variable, ggplot would create ## a different line for each value of the factor variable, colored differently. p2 &lt;- ggplot() + ## choose a color of preference geom_line(aes(x=date, y = uempmed), color = &quot;navyblue&quot;, data = eco_df) + ## add chart title and change axes labels labs( title = &quot;Median Duration of Unemployment&quot;, x = &quot;Date&quot;, y = &quot;&quot;) + ## Add a ggplot theme theme_light() ## center the chart title theme(plot.title = element_text(hjust = 0.5)) + p2 ## plotting multiple charts (of different line types and sizes) p3 &lt;-ggplot() + geom_line(aes(x=date, y = uempmed), color = &quot;navyblue&quot;, size = 1.5, data = eco_df) + geom_line(aes(x=date, y = psavert), color = &quot;red2&quot;, linetype = &quot;dotted&quot;, size = 0.8, data = eco_df) + labs( title = &quot;Unemployment Duration (Blue) and Savings Rate (Red)&quot;, x = &quot;Date&quot;, y = &quot;&quot;) + theme_light() + theme(plot.title = element_text(hjust = 0.5)) p3 ## Plotting a different line type for each group ## There isn&#39;t a natural factor in this data so let&#39;s just duplicate the data and make one up eco_df$fac &lt;- factor(1, levels = c(1,2)) eco_df2 &lt;- eco_df eco_df2$fac &lt;- 2 eco_df2$uempmed &lt;- eco_df2$uempmed - 2 + rnorm(nrow(eco_df2)) eco_df &lt;- rbind(eco_df, eco_df2) p4 &lt;- ggplot() + ## This time, color goes inside aes geom_line(aes(x=date, y = uempmed, color = fac), data = eco_df) + ## add chart title and change axes labels labs( title = &quot;Median Duration of Unemployment&quot;, x = &quot;Date&quot;, y = &quot;&quot;) + ## Add a ggplot theme theme_light() + ## center the chart title theme(plot.title = element_text(hjust = 0.5), ## Move the legend onto some blank space on the diagram legend.position = c(.25,.8), ## And put a box around it legend.background = element_rect(color=&quot;black&quot;)) + ## Retitle the legend that pops up to explain the discrete (factor) difference in colors ## (note if we just want a name change we could do guides(color = guide_legend(title = &#39;Random Factor&#39;)) instead) scale_color_manual(name = &quot;Random Factor&quot;, # And specify the colors for the factor levels (1 and 2) by hand if we like values = c(&quot;1&quot; = &quot;red&quot;, &quot;2&quot; = &quot;blue&quot;)) p4 # Put them all together with cowplot for LOST upload plot_grid(p1,p2,p3,p4, nrow=2) . The four plots generated by the code are (in order p1, p2, then p3 and p4): . .",
    "url": "/Presentation/styling_line_graphs.html",
    "relUrl": "/Presentation/styling_line_graphs.html"
  }
  ,"55": {
    "title": "Home",
    "content": "Home . Welcome to the Library of Statistical Techniques (LOST)! . LOST is a publicly-editable website with the goal of making it easy to execute statistical techniques in statistical software. . Each page of the website contains a statistical technique — which may be an estimation method, a data manipulation or cleaning method, a method for presenting or visualizing results, or any of the other kinds of things that statistical software typically does. . For each of those techniques, the LOST page will contain code for performing that method in a variety of packages and languages. It may also contain information (or links) with thorough descriptions of the method, but the focus here is on implementation. How can you do it in your language of choice? If there are multiple ways, how are those ways different? Is the way you used to do it outdated, or does it do something unexpected? What’s the R equivalent of that command you know about in Stata or SAS, or vice versa? . In short, LOST is a Rosetta Stone for statistical software. . If you are interested in contributing to LOST, please see the Contributing page. . LOST was originated in 2019 by Nick Huntington-Klein and is maintained by volunteer contributors. The project’s GitHub page is here. .",
    "url": "/",
    "relUrl": "/"
  }
  ,"56": {
    "title": "",
    "content": "Folder for adding user contributed data. .",
    "url": "/Data/",
    "relUrl": "/Data/"
  }
  
}