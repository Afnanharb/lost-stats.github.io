{
  
  "0": {
    "title": "Balance Table",
    "content": "Balance Tables . Balance Tables are a method by which you can statistically compare differences in characteristics between a treatment and control group. Common in experimental work and when using matching estimators, balance tables show if the treatment and control group are ‘balanced’ and can be seen as similarly ‘identical’ for comparison of a causal effect. . Keep in Mind . When a characteristic is statistically different between control and treatment, your study is unbalanced in respect to that attribute. | When a characteristic is unbalanced in your study, you may want to consider controlling for that attribute as a variable in your model specification. | Balance tables can only report numeric differences and are not suitable for string value comparisions | . Implementations . R . # Import Dependency library(&quot;cobalt&quot;) # Load Data data(mtcars) # Create Balance Table bal.tab(am ~ mpg + hp, data = mtcars) . Stata . * Import Dependency: &#39;ssc install table1&#39; * Load Data sysuse auto, clear * Create Balance Table * You need to declare the kind of variable for each, as well as the variable by which you define treatment and control. * Adding test gives the statistical difference between the two groups. The ending saves your output as an .xls file table1, by(foreign) vars(price conts mpg conts weight contn length conts) test saving(bal_tab.xls, replace) . Also Consider . The World Bank’s very useful ietoolkit for Stata has a very flexible command for creating balance tables, iebaltab. You can learn more about how to use it on their Wiki page on the command. .",
    "url": "/Summary_Statistics/Balance_Tables.html",
    "relUrl": "/Summary_Statistics/Balance_Tables.html"
  }
  ,"1": {
    "title": "Contributing",
    "content": "HOW TO CONTRIBUTE . Get a GitHub account. You do not need to know Git to contribute to LOST, but you do need a GitHub account. | Read the Guide to GitHub Markdown which will show the syntax that is used on LOST pages. | Read the below LOST Writing Guide, which shows what a good LOST page looks like from top to bottom. Even if you are just adding another language to an existing page, be sure to read the Implementations section at the bottom. | Explore LOST using the navigation bar on the left, find a page that needs to be expanded, and add more content. Or find one that doesn’t exist but should (perhaps on the Desired Nonexistent Pages list, and write it yourself! Go to the GitHub Repository for LOST to find the appropriate file to edit or folder to create your new file in. | If you are a “Contributor” to the project, you can make your edits and changes directly to the repository. If not, you will need to issue a pull request to get your work on LOST. We will add you as a contributor after your first accepted pull request. If you don’t know Git or how to do a pull request, please post in Issues asking to be added as a contributor so you can edit LOST directly. | If you’ve made a new page, make sure it’s saved as a .md file, put it in the appropriate folder, and add Navigation Information at the top (see below). If you’ve written a Desired Nonexistent Page, be sure to remove it from the list. Or, if your page links to some new nonexistent pages, add those to the list! Also, try to see if other pages have attempted to link to the page you’re working on, and update their links so they go to the right place. | LOST WRITING GUIDE . A LOST page is intended to be a set of instructions for performing a statistical technique, where “statistical technique” is broadly defined as “the things you do in statistical software”, which includes everything from loading data to estimating models to cleaning data to visualization to reproducible notebooks. . After someone reads a LOST page, they should have a decent idea of: . How to implement at least a basic version of what they want to do in the language/software they’re using | What common pitfalls there might be in what they’re doing | What are the pros and cons of meaningfully-different ways of doing the same thing, if relevant | Given what they’re doing, what else should they consider doing (for example, if they’re running a regression discontinuity design, you might suggest they also run a test for bunching on either side of the cutoff) | . Things to remember while writing: . Be as clear as possible. You’re writing a set of instructions, in effect. People should be able to follow them. | The technical ability of the reader may vary by page. People reading a LOST page about how to calculate a mean probably have little experience with their software and will need a lot of hand-holding. You can assume that people reading a LOST page about Markov-Chain Monte Carlo methods probably already have a fairly solid background. | . Markdown . LOST pages are written in Markdown. Markdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for . Syntax highlighted code block # Header 1 ## Header 2 ### Header 3 - Bulleted - List 1. Numbered 2. List **Bold** and _Italic_ and `Code` text [Link](url) and ![Image](src) . For more details see GitHub Flavored Markdown. . Math . Math is rendered with MathJax, which provides support for math formatting. To use on a specific page, make sure that the YAML at the top on the underlying Markdown (i.e. .md) file includes a line saying mathjax: true. This should already be the default on most existing pages, but it is worth emphasising. For example, here is a screenshot of the “Contributing.md” file that you are reading right now. . . After that, equations and other math sections can be delimited with two dollar symbol pairs. For example, $$x = frac{1}{2}$$ is rendered inline as . Similarly, we can render math in display mode (i.e. as a distinct block) by wrapping the dollar symbol pairs on separate lines. For example, . $$ y = beta_0 + beta_1 x + beta_2 x^2 + epsilon $$ . is rendered as display math: . While we don’t include such examples here, note that standard math environments such as begin{equation} ... end{equation} (for numbered equations) and begin{align} ... end{align} (for aligned equation systems) are all supported. Just remember to wrap them between a pair of dollar symbols. More information about MathJax can be found here. . STRUCTURE OF A LOST PAGE . When starting a LOST page, you should copy the New Page Template. There are four main sections of a LOST page: . Navigation Information . Your page will begin with what’s known as YAML, i.e. something that looks like this: . title: Observation level parent: Data Manipulation has_children: false nav_order: 1 mathjax: true . You don’t need to worry too much about YAML syntax (here’s the Wikipedia entry for those interested). The important thing is that the YAML provides a set of very basic instructions for the website navigation and page structure. Make sure to fill in the title with a relevant and brief title. Also be sure to put the appropriate name for the parent — this will ensure that your page shows up in the appropriate spot in the navigation structure. Options for parent include: . Data Manipulation | Geo-Spatial | Machine Learning | Model Estimation | Presentation | Summary Statistics | Time Series | Other | . For the most part, you should generally ignore has_children. (An exception is if you are creating a new section that does have new child pages, but then you are probably better off filing an issue with us to make sure this is done correctly.) You can also ignore nav_order — leaving this at 1 for everything will put everything in alphabetical order. . Introduction . This is an introduction to the technique. Most of the time this will be just a few sentences about what it is and does, and perhaps why it is used. However, in cases of more niche or complex material, there may be a reason to include more detailed information or general non-language-specific instructions here. In general, however, for more detailed explanations or discussions of statistical properties, you can always just link to an outside trusted source like Wikipedia or a (non-paywalled) academic paper. . Keep in Mind . This is a list of details and reminders for people using the method, especially if they are not yet an expert at it or if the detail is not well-known. This may include: . Important assumptions that an estimation method makes. | Notes about interpreting the results. | Settings where the technique seems like it might be a good idea, but actually isn’t. | Features of the technique that might surprise users or be unexpected. | Rules of thumb for use (“you will want to set the number of bootstrap samples to at least 1,000 (citation)”) | . Also Consider . This is a list of other techniques that are commonly used in addition to this page’s technique, or as an alternative to this page’s technique. If not obvious, include a very brief explanation of why you might want to use that other technique in addition to/instead of the current one. Note that you can link to another LOST page even if that page doesn’t exist yet. Maybe it will inspite someone to write it! . For example, pages about estimation techniques might list standard robustness tests to be used in addition to the technique, or adjustments to standard errors they might want to use. A page about a data visualization technique might include a link to a page about setting color palettes to be used in addition. . Or, they might list an alternative technique that might be used if a certain assumption fails (“This technique requires continuous variables. So if your data is discrete, use this other method.”). . To link to other LOST pages (even if they don’t exist yet — don’t forget to add these to Desired Nonexistent Pages!, use the Markdown [Link name](url) structure, where the URL is of the format https://lost-stats.github.io/Category_Name/page_name.html. . Implementations . Implementations contains multiple subsections, one for each statistical software environment/programming language that the technique can be implemented in. . Implementations should be listed in alphabetical order of software/language. eViews, then Python, then R, then SAS, then Stata, etc. | For each language, include well-commented and as-brief-as-reasonably-possible example code that provides an example of performing the technique. Readers should be able to copy the code and have it run the technique from beginning to end, including steps like loading in data if necessary. See existing pages for examples. | If someone else on the internet has already written a clear, thorough, and general implementation example, then linking to it is perfectly fine! This includes StackOverflow/StackExchange answers, which you can link to using the share button. Extremely long demonstrations for super-complex methods may be better left as links only (perhaps with a tiny example pulled out and put on LOST). If the example is short enough, though, including the example directly in the LOST page is preferable, with link attribution of the source, so readers don’t have to go elsewhere. | Avoid creating a long list of examples showing every variant or optional setting of the technique. Instead, focus on one main example, with variants included only if they are especially important. If you like, you can mention in comments additional useful options/settings the reader might want to look into and what they do. | If the technique requires that a package or library be installed, include the code for installing the package in a comment (or if you are using a language where libraries cannot be installed inside the code, include a comment directing the user to install the library). | If a given language has multiple ways of performing the same technique, ideally report only one “best” method, whatever that might be. If other methods are only different in trivial ways, then you can describe them as being alternatives, but avoid providing examples for them. If other methods are different in important ways, then include an example for each, with text explanations of what is different about them. If two contributors seriously disagree about which way is best, then they’re probably different in some meaningful way so you can include both as long as you can explain what that difference is. | It is fine to add implementations for software that only has a graphical interface rather than code (such as Excel) using screenshots. Be sure to keep images well-cropped and small so they don’t crowd the page. If your graphical instructions are necessarily very long, consider posting them as a blog post somewhere and just put a link to that post in Implementations. | . Images . Images can be added to Implementation sections if relevant, for example if you’re working with GUI-only software, or demonstrating the output of a data visualization technique. . How can you add these images? You can upload the images somewhere on the internet that allows image linking, and include the image in your instructions with ![Image](src). Ideally, upload the image directly to the Images/name-of-your-page/ subfolder of whatever directory you’re working in, and link to the images there. . Please be sure to add alt text to images for sight-impaired users. Image filenames should make reference to the language used to make them, i.e. python_scatterplot.png. . Data . Ideally, the same data set will be uploaded to LOST directly in a format accessible by many languages (like CSV) in the Data/name-of-your-page/ subfolder of whatever directory you’re wokring in, and then that data can be used for implementation in all languages on the page. This is not required, but is encouraged. . FREQUENTLY ASKED QUESTIONS . What techniques are important enough to be their own page? This is a little subjective, but if you’re writing about X, which is a minor option/variant of Y, then you can just include it on the Y page. If X is a different technique or a variant of Y that is used in different circumstances or produces meaningfully different output, then give X its own page. | How should I title my page? Pick a single, concise description of the technique you’re talking about. If there are multiple ways to refer to the technique you’re doing, pick one. You will also need to select a file name, which should be in lower_case_with_underscores.md and you might want to make a bit shorter. So Ordinary Least Squares (Linear Regression) might be the title and H1 heading, and ordinary_least_squares.md might be the file name. | What languages can I include in Implementations? Any language is valid as long as it’s something people actually do statistical analysis in. Don’t include something just because you can (I mean, you can technically do OLS in assembly but is that useful for anyone?), but because you think someone will find it useful. | Should I include the output of my code? For data visualization, yes! Just keep the images relatively small so they don’t crowd the page. See the Implementations section above for how to add images. If your output is not visual, there’s probably no need to include output unless you think that it is especially important for some technique. | How can I discuss what I’m doing with other contributors? Head to the Issues page and find (or post) a thread with the title of the page you’re talking about. | How can I [add an image/link to another LOST page/add an external link/bold text] in the LOST wiki? See the Markdown section above. | I want to contribute but I do not like all the rules and structure on this page. I don’t even want my FAQ entry to be a question. Just let me write what I want. If you have valuable knowledge about statistical techniques to share with people and are able to explain things clearly, I don’t want to stop you. So go for it. Maybe post something in Issues when you’re done and perhaps someone else will help make your page more consistent with the rest of the Wiki. I mean, it would be nicer if you did that yourself, but hey, we all have different strengths, right? | .",
    "url": "/Contributing/Contributing.html",
    "relUrl": "/Contributing/Contributing.html"
  }
  ,"2": {
    "title": "Model Estimation",
    "content": "Model Estimation .",
    "url": "/Model_Estimation/Estimation.html",
    "relUrl": "/Model_Estimation/Estimation.html"
  }
  ,"3": {
    "title": "Machine Learning",
    "content": "Machine Learning .",
    "url": "/Machine_Learning/Machine_Learning.html",
    "relUrl": "/Machine_Learning/Machine_Learning.html"
  }
  ,"4": {
    "title": "Title of page",
    "content": "Name of Page . INTRODUCTION SECTION . Remember that you can use inline math, e.g. . In general, you should render variables in math mode (, , etc.) . You can also render math in display mode: . Keep in Mind . LIST OF IMPORTANT THINGS TO REMEMBER ABOUT USING THE TECHNIQUE | . Also Consider . LIST OF OTHER TECHNIQUES THAT WILL COMMONLY BE USED ALONGSIDE THIS PAGE’S TECHNIQUE | (E.G. LINEAR REGRESSION LINKS TO ROBUST STANDARD ERRORS), | OR INSTEAD OF THIS TECHNIQUE | (E.G. PIE CHART LINKS TO A BAR PLOT AS AN ALTERNATIVE) | WITH EXPLANATION | INCLUDE LINKS TO OTHER LOST PAGES WITH THE FORMAT Description. Categories include Data_Manipulation, Geo-Spatial, Machine_Learning, Model_Estimation, Presentation, Summary_Statistics, Time_Series, and Other | . Implementations . NAME OF LANGUAGE/SOFTWARE 1 . identifier for language type, see this page: https://github.com/jmm/gfm-lang-ids/wiki/GitHub-Flavored-Markdown-%28GFM%29-language-IDs Commented code demonstrating the technique . ## NAME OF LANGUAGE/SOFTWARE 2 WHICH HAS MULTIPLE APPROACHES There are two ways to perform this technique in language/software 2. First, explanation of what is different about the first way: identifier for language type, see this page: https://github.com/jmm/gfm-lang-ids/wiki/GitHub-Flavored-Markdown-%28GFM%29-language-IDs Commented code demonstrating the technique . Second, explanation of what is different about the second way: . identifier for language type, see this page: https://github.com/jmm/gfm-lang-ids/wiki/GitHub-Flavored-Markdown-%28GFM%29-language-IDs Commented code demonstrating the technique .",
    "url": "/NewPageTemplate.html",
    "relUrl": "/NewPageTemplate.html"
  }
  ,"5": {
    "title": "Other",
    "content": "Other .",
    "url": "/Other/Other.html",
    "relUrl": "/Other/Other.html"
  }
  ,"6": {
    "title": "Presentation",
    "content": "Presentation .",
    "url": "/Presentation/Presentation.html",
    "relUrl": "/Presentation/Presentation.html"
  }
  ,"7": {
    "title": "Summary Statistics",
    "content": "Summary Statistics .",
    "url": "/Summary_Statistics/Summary_Statistics.html",
    "relUrl": "/Summary_Statistics/Summary_Statistics.html"
  }
  ,"8": {
    "title": "Geo-Spatial",
    "content": "Geo-Spatial .",
    "url": "/Geo-Spatial/Summary_Techniques.html",
    "relUrl": "/Geo-Spatial/Summary_Techniques.html"
  }
  ,"9": {
    "title": "Time Series",
    "content": "Time Series .",
    "url": "/Time_Series/Time_Series.html",
    "relUrl": "/Time_Series/Time_Series.html"
  }
  ,"10": {
    "title": "Cluster-Robust Standard Errors",
    "content": "Cluster-Robust Standard Errors (a.k.a. Clustered Standard Errors) . Data is considered to be clustered when there are subsamples within the data that are related to each other. For example, if you had data on test scores in a school, those scores might be correlated within classroom because classrooms share the same teacher. When error terms are correlated within clusters but independent across clusters, then regular standard errors, which assume independence between all observations, will be incorrect. Cluster-robust standard errors are designed to allow for correlation between observations within cluster. For more information, see A Practitioner’s Guide to Cluster-Robust Inference. . Keep in Mind . Just because there are likely to be clusters in your data is not necessarily a good justification for using cluster-robust inference. Generally, clustering is advised only if either sampling or treatment assignment is performed at the level of the clusters. See Abadie, Athey, Imbens, &amp; Wooldridge (2017), or this simple summary of the paper. | There are multiple kinds of cluster-robust standard errors, for example CR0, CR1, and CR2. Check in to the kind available to you in the commands you’re using. | . Also Consider . Cluster Bootstrap Standard Errors, which are another way of performing cluster-robust inference that will work even outside of a standard regression context. | . Implementations . Note: Clustering of standard errors is especially common in panel models, such as linear fixed effects. For this reason, software routines for these particular models typically offer built-in support for (multiway) clustering. The implementation pages for these models should be hyperlinked in the relevant places below. Here, we instead concentrate on providing implementation guidelines for clustering in general. . Julia . For cluster-robust estimation of (high-dimensional) fixed effect models in Julia, see here. . R . For cluster-robust estimation of (high-dimensional) fixed effect models in R, see here. . Cluster-robust standard errors for many different kinds of regression objects in R can be obtained using the coeftest function in the lmtest package combined with the vcovCL function in the sandwich package. Alternately, while it does not handle as many types of regressions, the lm_robust function in estimatr can provide cluster-robust standard errors much more easily. . # If necessary, install lmtest, sandwich, and estimatr # install.packages(c(&#39;lmtest&#39;,&#39;sandwich&#39;,&#39;estimatr&#39;)) # Read in data from the College Scorecard df &lt;- read.csv(&#39;https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv&#39;) # Get robust errors using vcovCL and lmtest library(lmtest) library(sandwich) # Create a regression model without cluster-robust standard errors my_model &lt;- lm(repay_rate ~ earnings_med + state_abbr, data = df) # Put the model into vcovCL() to get a robust covariance matrix # and then put that in lmtest() to get the regression results with robust errors # Pick the kind of robust errors with &quot;type&quot; # It refers to the heteroskedasticity-consistent error types. # HC1 is the default but I&#39;ve specified it here anyway coeftest(my_model, vcov = vcovCL(my_model, cluster = df$inst_name, type = &quot;HC1&quot;)) # Alternately, just use lm_robust. # Standard error types are referred to as CR0, CR1 (&quot;stata&quot;), CR2 here. # Here, CR2 is the default library(estimatr) my_model2 &lt;- lm_robust(repay_rate ~ earnings_med + state_abbr, data = df, clusters = inst_name, se_type = &quot;stata&quot;) summary(my_model2) . Stata . Stata has clustered standard errors built into most regression commands, and they generally work the same way for all commands. . * Load in College Scorecard data import delimited &quot;https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv&quot;, clear * The missings are written as &quot;NA&quot;, let&#39;s turn this numeric destring earnings_med repay_rate, replace force * If we want to cluster on a variable or include it as a factor it must not be a string encode inst_name, g(inst_name_encoded) encode state_abbr, g(state_encoded) * Just add vce(cluster) to the options of the regression * This will give you CR1 regress repay_rate earnings_med i.state_encoded, vce(cluster inst_name_encoded) .",
    "url": "/Model_Estimation/Nonstandard_Errors/clustered_se.html",
    "relUrl": "/Model_Estimation/Nonstandard_Errors/clustered_se.html"
  }
  ,"11": {
    "title": "Collapse a Data Set",
    "content": "Collapse a Data Set . The observation level of a data set is the set of case-identifying variables which, in combination, uniquely identify every row of the data set. For example, in the data set . I J X . 1 | 1 | 3 | . 1 | 2 | 3.5 | . 2 | 1 | 2 | . 2 | 2 | 4.5 | . the variables and uniquely identify rows. The first row has and , and there is no other row with that combination. We could also say that uniquely identifies rows, but in this example is not a case-identifying variable, it’s actual data. . It is common to want to collapse a data set from one level to another, coarser level. For example, perhaps instead of one row per combination of and , we simply want one row per , perhaps with the average across all observations. This would result in: . I X . 1 | 3.25 | . 2 | 3.25 | . This can be one useful way to produce summary statistics, but can also be used to rid the data of unnecessary or unusable detail, or to change one data set to match the observation level of another. . Keep in Mind . Collapsing a data set almost by definition requires losing some information. Make sure that you actually want to lose this information, rather than, for example, doing a horizontal merge, which can match data sets with different observation levels without losing information. | Make sure that, for each variable you plan to retain in your new, collapsed data, you know the correct procedure that should be used to figure out the new, summarized value. Should the collapsed data for variable use the mean of all the observations you started with? The median? The mode? The first value found in the data? Think through these decisions. | . Also Consider . For more information about observation levels and how to determine what the current observation level is, see determine the observation level of a data set. | . Implementations . R . # If necessary, install dplyr # install.packages(&#39;dplyr&#39;) library(dplyr) # Get data on storms from dplyr data(&quot;storms&quot;) # We would like each storm to be identified by # name, year, month, and day # However, currently, they are also identified by hour, # And even then there are sometimes multiple observations per hour # To construct the collapsed data, we start with the original storms_collapsed &lt;- storms %&gt;% # group by the variables that make the new observation level group_by(name, year, month, day) %&gt;% # And use summarize() to pick the variables to keep, as well as # the functions we want to use to collapse each variable. # Let&#39;s get the mean wind and pressure, and the first category value summarize(wind = mean(wind), pressure = mean(pressure), category = first(category)) # Note that if we wanted to collapse every variable in the data with the # same function, we could instead use summarize_all() . Stata . ** Load surface.dta, which contains temperature recordings in different locations sysuse surface.dta, clear * Currently, there is one observation per latitude per longitude per date * I would like this to just be one observation per latitude/longitude * So I construct a collapse command. * I take my new target observation level and put it in by() * and then take each variable I&#39;d like to keep around and tell * Stata what function to use to create the new collapsed value, here (mean) collapse (mean) temperature, by(latitude longitude) .",
    "url": "/Data_Manipulation/collapse_a_data_set.html",
    "relUrl": "/Data_Manipulation/collapse_a_data_set.html"
  }
  ,"12": {
    "title": "Horizontal Combination (Deterministic)",
    "content": "Combining Datasets: Horizontal Combination (Deterministic) . A deterministic merge is when there is some variable(s) that uniquely and dependably identifies individual units (be it people, firms, teams, etc.) across datasets. For example, we might have two datasets containing information about the same set of people, one with their financial information, the other with their educational information. To analyze the relationship between the education and financial measures, we need them in the same dataset and so would want to combine them. If both datasets had a unique identification field for each person, such as a social security number or other national id, we could use this to match the records, so that all information from the same person appeared on the same line. . Because we expect such identifiers to be unique to an individual (unlike many names, such as John Smith) and appear exactly the same in each dataset, we can use just this field to do the match, and don’t anticipate in ambiguity in determining which records match to each other. Thus, it is a deterministic merge. . Keep in mind . For any number of reasons, one or both of the datasets may have more than one observation per unit or individual. That may be for a good reason – such as havinng multiple test scores for the same student because they took exams at different points in time – or it may be redundant information. Understanding the structure of your data is key before embarking on a deterministic merge. | It is a good idea to have a clear sense of how much overlap you anticipate across your datasets. It is important to examine the results of your merge and see if it matches the amount the overlap you expected. Subtle differences in a matching variable (e.g. if leading zeroes are present in an ID variable for one variable but not another) can be a source of major headaches for your analysis if not caught early. If something looks weird in your results later in the project, trouble with a merge is a common cause. So check your merge results early and often. | . Also Consider . Determine the observation level of a data set. | . Implementations . R . There are several ways to combine data sets horizontally in R, including base-R merge and several different approaches in the data.table package. We will be using the join functions in the dplyr package. . # If necessary, install dplyr # install.packages(&#39;dplyr&#39;) library(dplyr) # This data set contains information on GDP in local currency GDP2018 &lt;- data.frame(Country = c(&quot;UK&quot;, &quot;USA&quot;, &quot;France&quot;), Currency = c(&quot;Pound&quot;, &quot;Dollar&quot;, &quot;Euro&quot;), GDPTrillions = c(2.1, 20.58, 2.78)) # This data set contains dollar exchange rates DollarValue2018 &lt;- data.frame(Currency = c(&quot;Euro&quot;, &quot;Pound&quot;, &quot;Yen&quot;, &quot;Dollar&quot;), InDollars = c(1.104, 1.256, .00926, 1)) . Next we want to join together GDP2018 and DollarValue2018 so we can convert all the GDPs to dollars and compare them. There are three kinds of observations we could get - observations in GDP2018 but not DollarValue2018, observations in DollarValue2018 but not GDP2018, and observations in both. Use help(join) to pick the variant of join that keeps the observations we want. The “Yen” observation won’t have a match, and we don’t need to keep it. So let’s do a left_join and list GDP2018 first, so it keeps matched observations, plus any observations only in GDP2018. . GDPandExchange &lt;- left_join(GDP2018, DollarValue2018) . The join function will automatically detect that the Currency variable is shared in both data sets and use it to join them. Generally, you will want to be sure that the set of variables you are joining by uniquely identifies observations in at least one of the data sets you are joining. If you’re not sure whether that’s true, see Determine the observation level of a data set, or run join through the safe_join from the pmdplyr package. . Stata . A Quick Prelude About “Master” And “Using” Datasets . When merging two datasets together, there are two relevant datasets to consider. The first is the one currently in Stata’s memory, the other is whatever dataset (not currently loaded into Stata) that you would like to merge onto the dataset in memory. For ease of reference, Stata calls the dataset in memory the “master” dataset and the other file the “using” dataset. When you see the syntax of the merge command, the reason for calling it the “using” dataset will become clear. . In Stata, there are 3 types of deterministic merges: . 1-to-1 . A one-to-one merge expects there to be no more than one row in each dataset to have a matched pair in the other dataset. If there is more than one observation with the same identifying variable(s) in either the master or using datasets when attempting to do a one-to-one merge, Stata will throw an error. (Note: you can check to see if there is more than one observation per identifying variable by using the “duplicates report” command, or the Gtools variant for especially large datasets, called “gduplicates report.”) . *Import Data with Car Sizes and Merge it to Data on Car Prices using the ID variable “Make” webuse autosize.dta, clear merge 1:1 make using http://www.stata-press.com/data/r14/autoexpense.dta . Note that the syntax specifies “make” as the identifying variable after the merge type (1:1) and before the using statement (thus why we call the data not in memory the “using” data. The result of this merge shows 5 successful matches and one observation from the master dataset that did not have a match. . Result # of obs. _merge value . not matched from master | 1 | (_merge==1) | . not matched from using | 0 | (_merge==2) | .   |   |   | . matched | 5 | (_merge==3) | . Note that Stata creates a new variable (_merge) during the merge that stores the merge status of each observation, where a value of 1 means that the observation was only found in the master dataset, 2 means it was found only in the using dataset, and 3 means it was found in both and successfully merged. . Many-to-1 . A many-to-one merge occurs when the master dataset contains multiple observations of the same unit or individual (say, multiple test scores for the same student), while the using dataset has only one observation per unit or individual (say, the age of each student). Here is the syntax for a many-to-1 merge. . *Import Data with Car Sizes and Merge it to Data on Car Prices using the ID variable “Make” webuse sforce.dta, clear merge m:1 region using http://www.stata-press.com/data/r14/dollars.dta . Note that in this case that the syntax changes from merge 1:1 to merge m:1 where m stands for many. In this case, the identifying variable is “region.” . 1-to-Many . A one-to-many merge is the opposite of a many to one merge, with multiple observations for the same unit or individual in the using rather than the master data. The only different in the syntax is that it becomes merge 1:m rather than merge m:1. . Many-to-Many . A many-to-many merge is intended for use when there are multiple observations for each combination of the set of merging variables in both master and using data. However, merge m:m has strange behavior that is effectively never what you want, and it is not recommended. .",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html",
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_horizontal_merge_deterministic.html"
  }
  ,"13": {
    "title": "Combining Datasets",
    "content": "Combining Datasets Overview . There are two main ways to combine data: vertically and horizontally. That is, you can want to combine observations (adding new variables) or combine variables (adding new observations). This is perhaps easiest to show visually: . Individual Name Info |Name| ID | |–|–| |John Smith|A63240| |Desiree Thomas|B78242| . Individual Age Info |ID | Age | |–|–| |B78242|22| |A63240|27| . In the case above, we would like to combine two datasets, the Individual Name Info and the Individual Date Info, that have different information about the same people, who are identified by the ID variable. The result from the merge would be to have a new dataset with more columns than the original datasets because it contains all of the information for each individual from both of the original datasets. Here we have to combine the files according to the ID variable, placing the information from observations with the same ID on the same row in the combined dataset. . Alternatively, the below example has two datasets that collect the same information about different people. We would like to combine these datasets vertically, with the result containing more rows than the original dataset, because it contains all of the people that are present in each of the original datasets. Here we combine the files based on the name or position of the columns in the dataset. . Name ID Age . John Smith | A63240 | 22 | . Desiree Thomas | B78242 | 27 | . Name ID Age . Teresa Suarez | Y34208 | 19 | . Donald Akliberti | B72197 | 34 | . These ways of combining data are referred to by different names across different programming languages, but will largely be referred to by one common set of terms (used by Stata and Python’s Pandas): merge for horizontal combination and append for for vertical combination. .",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_overview.html",
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_overview.html"
  }
  ,"14": {
    "title": "Vertical Combination",
    "content": "Combining Datasets: Vertical Combination . When combining two datasets that collect the same information about different people, they get combined vertically because they have variables in common but different observations. The result of this combination will more rows than the original dataset because it contains all of the people that are present in each of the original datasets. Here we combine the files based on the name or position of the columns in the dataset. It is a “vertical” combination in the sense that one set of observations gets added to the bottom of the other set of observations. . Keep in Mind . Vertical combinations require datasets to have variables in common to be of much use. That said, it may not be necessary for the two datasets to have exactly the same variables. Be aware of how your statistical package handles observations for a variable that is in one dataset but not another (e.g. are such observations set to missing?). | It may be the case that the datasets you are combining have the same variables but those variables are stored differently (numeric vs. string storage types). Be aware of how the variables are stored across datasets and how your statistical package handles attempts to combine the same variable with different storage types (e.g. Stata throws an error and will now allow the combination, unless the “, force” option is specified.) | . Implementations . R . There are several ways to vertically combine data sets in R, including rbind. We will use the dplyr package function bind_rows, which allows the two data sets to combine even if they don’t contain the exact same set of variables. . # If necessary, install dplyr # install.packages(&#39;dplyr&#39;) library(dplyr) # Load in mtcars data data(mtcars) # Split it in two, so we can combine them back together mtcars1 &lt;- mtcars[1:10,] mtcars2 &lt;- mtcars[11:32,] # Use bind_rows to vertically combine the data sets mtcarswhole &lt;- bind_rows(mtcars1, mtcars2) . Stata . * Load California Population data webuse http://www.stata-press.com/data/r14/capop.dta // Import data from the web append using http://www.stata-press.com/data/r14/ilpop.dta // Merge on Illinois population data from the web . You can also append multiple datasets at once, by simply listing both datasets separated by a space: . * Load California Population data * Import data from the web webuse http://www.stata-press.com/data/r14/capop.dta * Merge on Illinois and Texas population data from the web append using http://www.stata-press.com/data/r14/ilpop.dta http://www.stata-press.com/data/r14/txpop.dta . Note that, if there are columns in one but not the other of the datasets, Stata will still append the two datasets, but observations from the dataset that did not contain those columns will have their values for that variable set to missing. . * Load Even Number Data webuse odd.dta, clear append using http://www.stata-press.com/data/r14/even.dta .",
    "url": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html",
    "relUrl": "/Data_Manipulation/Combining_Datasets/combining_datasets_vertical_combination.html"
  }
  ,"15": {
    "title": "Data Manipulation",
    "content": "Data Manipulation Techniques .",
    "url": "/Data_Manipulation/data_manipulation.html",
    "relUrl": "/Data_Manipulation/data_manipulation.html"
  }
  ,"16": {
    "title": "Desired Nonexistent Pages",
    "content": "Desired Nonexistent Pages . This is a manually-maintained list of pages that have been linked from elsewhere, or that it would be nice to have, but do not exist yet. Feel free to edit with your own wishes! . A page does not have to be listed here for you to add it! These are just the things that we thought of. There’s certainly plenty more in the world of statistical techniques to be added. . If you create one of these pages, please remove it from this list. . Data Manipulation . Aggregating Statistics | Collapse a Data Set | . Model Estimation . Random Effects | Nonlinear Instrumental Variables Estimation | Differences in Differences | Regression Discontinuity Design | Propensity Score Matching | Event Study Estimation | Nonparametric regression | Generalized Method of Moments (GMM) | Bootstrap Standard Errors | Cluster Bootstrap Standard Errors | Treatment Effect Model | Endogenous Switching Model | Nonparametric Sample Selection Models | Heteroskedasticity-Robust Standard Errors | Jackknife Standard Errors | Tobit | Average Marginal Effects | Marginal Effects at the Mean | Conditional Logit | Mixed Logit | Nested Logit | Hierarchical Bayes Conditional Logit | Multilevel Regression with Poststratification | Nonlinear Mixed Effects Models | Cluster-robust Standard Errors | Ordered Probit | Ordered Logit | Joint F-tests | . Geo-Spatial . Geo-coding | Handling Raster Data | Handling Vector Data | Spatial Joins | Spatial Regression Model | . Machine Learning . A-B Testing | Artificial Neural Networks | Random Forest | Decision Trees | Nearest Neighbors Matching | . Presentation . Line Graph | Styling Line Graphs | Scatterplot | Styling Scatterplots | Faceted Graphs | Marginal Effects Plots for Discrete Variables | . Summary Statistics . Summary Statistics by Group | Summary Statistics Tables | Cross-Tabulations | Correlation Matrix | . Time Series . Serial Correlation | Stationarity and Weak Dependence | Granger Causality | Auto-regressive Model | Moving Average Model | ARMA Model | ARIMA Model | ARIMAX Model | ARCH Model | GARCH Model | TARCH Model | Rolling Regressions | Recursive Regressions | State Dependent Regression | Structural Break (Chow Test) | Dynamic Panel | Ex-Post Forecasting | Ex-Ante Forecasting | . Other . Import a Delimited Data File (CSV, TSV) | Import a Fixed-Width Data File | Export Data to a Foreign Format | Set a Working Directory | Power Analysis of Interaction Terms | .",
    "url": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html",
    "relUrl": "/Desired_Nonexistent_Pages/desired_nonexistent_pages.html"
  }
  ,"17": {
    "title": "Determine the Observation Level of a Data Set",
    "content": "Determine the Observation Level of a Data Set . The observation level of a data set is the set of case-identifying variables which, in combination, uniquely identify every row of the data set. For example, in the data set . I J X . 1 | 1 | 3 | . 1 | 2 | 3.5 | . 2 | 1 | 2 | . 2 | 2 | 4.5 | . the variables and uniquely identify rows. The first row has and , and there is no other row with that combination. We could also say that uniquely identifies rows, but in this example is not a case-identifying variable, it’s actual data. . When working with data that has case-identifier variables, like panel data, it’s generally a good idea to know what set of them makes up the observation level of a data set. Otherwise you might perform merges or case-level calculations incorrectly. . Keep in Mind . As in the above example, it’s easy to uniquely identify rows using continuous data. But the goal is to figure out which case-identifying variables, like an individual’s ID code, or a country code, or a time code, uniquely identify rows. Make sure you only try these variables. | Even if you think you know what the observation level is, it’s good to check. Lots of data is poorly behaved! | . Also Consider . You can collapse a data set to switch from one observation level to another, coarser one. | . Implementations . R . # If necessary, install dplyr # install.packages(&#39;dplyr&#39;) # We do not need dplyr to detect the observation level # But we will use it to get data, and for our alternate approach library(dplyr) # Get data on storms from dplyr data(&quot;storms&quot;) # Each storm should be identified by # name, year, month, day, and hour # anyDuplicated will return 0 if there are no duplicate combinations of these # so if we get 0, the variables in c() are our observation level. anyDuplicated(storms[,c(&#39;name&#39;,&#39;year&#39;,&#39;month&#39;,&#39;day&#39;,&#39;hour&#39;)]) # We get 2292, telling us that row 2292 is a duplicate (and possibly others!) # We can pick just the rows that are duplicates of other rows for inspection # (note this won&#39;t get the first time that duplicate shows up, just the subsequent times) duplicated_rows &lt;- storms[duplicated(storms[,c(&#39;name&#39;,&#39;year&#39;,&#39;month&#39;,&#39;day&#39;,&#39;hour&#39;)]),] # Alternately, we can use dplyr storms %&gt;% group_by(name, year, month, day, hour) %&gt;% # Add a variable with the number of times that particular combination shows up mutate(number_duplicates = n()) %&gt;% # Then take that variable out pull(number_duplicates) %&gt;% # And get the maximum of it max() # If the result is 1, then we have found the observation level. If not, we have duplicates. # We can pick out the rows that are duplicated for inspection # by filtering on n(). This approach will give you every time the duplicate appears. duplicated_rows &lt;- storms %&gt;% group_by(name, year, month, day, hour) %&gt;% # Add a variable with the number of times that particular combination shows up filter(n() &gt; 1) . Stata . * Load surface.dta, which contains temperature recordings in different locations sysuse surface.dta, clear * duplicates report followed by a variable list will show how many times * each combination shows up. * I think there is one observation level for each location, so I&#39;ll check that duplicates report latitude longitude * If I am correct, then the only number in the &quot;Copies&quot; column will be 1. * But it looks like I was not correct. * duplicates tag will create a binary variable with 1 for all duplicates * so I can examine the problem more closely * (duplicates examples is another option) duplicates tag latitude longitude, g(duplicated_data) * If I want to know not just whether there are duplicates but how many * of each there are for when I look more closely, I can instead do by latitude longitude, sort: g number_of_duplicates_in_this_group = _N . For especially large datasets the Gtools version of the various duplicates commands, gduplicates, is a great option . * Install gtools if necessary * ssc install gtools * Recreate the two duplicates tasks from above gduplicates report latitude longitude gduplicates tag latitude longitude, g(g_duplicated_data) .",
    "url": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html",
    "relUrl": "/Data_Manipulation/determine_the_observation_level_of_a_data_set.html"
  }
  ,"18": {
    "title": "Export a Formatted Regression Table",
    "content": "Export a Formatted Regression Table . Statistical packages often report regression results in a way that is not how you would want to display them in a paper or on a website. Additionally, they rarely provide an option to display multiple regression results in the same table. . Two (bad) options for including regression results in your paper include copying over each desied number by hand, or taking a screenshot of your regression output. Much better is using a command that outputs regression results in a nice format, in a way you can include in your presentation. . Keep in Mind . Any good regression table exporting command should include an option to limit the number of significant digits in your result. You should almost always make use of this option. It is very rare that the seventh or eighth decimal place (commonly reported in statistics packages) is actually meaningful, and it makes it difficult to read your table. | Variable names serve different purposes in statistical coding and in papers. Variable names in papers should be changed to be readable in the language of the paper. So for example, while employment may be recorded as EMP_STAT in your statistics package, you should rename it Employment for your paper. Most table exporting commands include options to perform this renaming. But if it doesn’t, you can always change it by hand after exporting. | If you use asterisks to indicate significance, be sure to check the significance levels that different numbers of asterisks indicate in the command you’re using, as standards for what significance levels the asterisks mean vary across fields (and so vary across commands as well). Most commands include an option to change the significance levels used. On that note, always include a table note saying what the different asterisk indicators mean! These commands should all include one by default - don’t take it out! | . Also Consider . If you are a Word user, and the command you are using does not export to Word or RTF, you can get the table into Word by exporting an HTML, CSV, or LaTeX, then opening up the result in your browser, Excel, or TtH, respectively. Excel and HTML tables can generally be copy/pasted directly into Word (and then formatted within Word). You may at that point want to use Word’s “Convert Text to Table” command, especially if you’ve pasted in HTML. | By necessity, regression-output commands often have about ten million options, and they can’t all be covered on this page. If you want it to do something, it probably can. To reduce errors, it is probably a good idea to do as little formatting and copy/pasting by hand as possible. So if you want to do something it doesn’t do by default, like adding additional calculations, check out the help file for your command to see how you can do it automatically. | . Implementations . R . There are many, many packages for exporting regression results in R, including RStudio’s gt, texreg, and xtable. Here we will focus on two: stargazer, which is probably the easiest to use, and huxtable, which is slightly more up-to-date and offers advanced formatting options, outlined on its website. . # Install stargazer if necessary # install.packages(&#39;stargazer&#39;) library(stargazer) # Get mtcars data data(mtcars) # Let&#39;s give it two regressions to output lm1 &lt;- lm(mpg ~ cyl, data = mtcars) lm2 &lt;- lm(mpg ~ cyl + hp, data = mtcars) # Let&#39;s output an HTML table, perhaps for pasting into Word # We could instead set type = &#39;latex&#39; for LaTeX or type = &#39;text&#39; for a text-only table. stargazer(lm1, lm2, type = &#39;html&#39;, out = &#39;my_reg_table.html&#39;) # In line with good practices, we should use readable names for our variables stargazer(lm1, lm2, type = &#39;html&#39;, out = &#39;my_reg_table.html&#39;, covariate.labels = c(&#39;Cylinders&#39;,&#39;Horsepower&#39;), dep.var.labels = &#39;Miles per Gallon&#39;) . This produces: . | (1) | (2) | . Cylinders | -2.876 *** | -2.265 *** | . | (0.322)&nbsp;&nbsp;&nbsp; | (0.576)&nbsp;&nbsp;&nbsp; | . Horsepower | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | -0.019&nbsp;&nbsp;&nbsp;&nbsp; | . | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | (0.015)&nbsp;&nbsp;&nbsp; | . N | 32&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | 32&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | . R2 | 0.726&nbsp;&nbsp;&nbsp;&nbsp; | 0.741&nbsp;&nbsp;&nbsp;&nbsp; | . logLik | -81.653&nbsp;&nbsp;&nbsp;&nbsp; | -80.781&nbsp;&nbsp;&nbsp;&nbsp; | . AIC | 169.306&nbsp;&nbsp;&nbsp;&nbsp; | 169.562&nbsp;&nbsp;&nbsp;&nbsp; | . *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. | . Now we will do the same thing with huxtable, using mostly defaults. . # Install huxtable and magrittr if necessary # install.packages(&#39;huxtable&#39;, &#39;magrittr&#39;) # huxtable works more easily with the pipe %&gt;% # which can come from magrittr or dplyr or tidyverse, etc. library(huxtable) library(magrittr) # First we build a huxreg object, using readable names huxreg(lm1, lm2, coefs=c(&#39;Cylinders&#39; = &#39;cyl&#39;, &#39;Horsepower&#39; = &#39;hp&#39;)) %&gt;% # We can send it to the screen to view it instantly print_screen() # Or we can send it to a file with the quick_ functions, which can # output to pdf, docx, html, xlsx, pptx, rtf, or latex. huxreg(lm1, lm2, coefs=c(&#39;Cylinders&#39; = &#39;cyl&#39;, &#39;Horsepower&#39; = &#39;hp&#39;)) %&gt;% # Let&#39;s make an HTML file quick_html(file = &#39;my_reg_output.html&#39;) . Which produces (note the different asterisks behavior, which can be changed with huxreg’s stars option): . | . | Dependent variable: | . | | . | Miles per Gallon | . | (1) | (2) | . | . Cylinders | -2.876*** | -2.265*** | . | (0.322) | (0.576) | . | | | . Horsepower | | -0.019 | . | | (0.015) | . | | | . Constant | 37.885*** | 36.908*** | . | (2.074) | (2.191) | . | | | . | . Observations | 32 | 32 | . R2 | 0.726 | 0.741 | . Adjusted R2 | 0.717 | 0.723 | . Residual Std. Error | 3.206 (df = 30) | 3.173 (df = 29) | . F Statistic | 79.561*** (df = 1; 30) | 41.422*** (df = 2; 29) | . | . Note: | *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 | . Stata . There are two main ways of outputting regression results in Stata, both of which must be installed from ssc install: outreg2 and estout. We will use estout here, as it is more flexible. More detail is available on the estout website. . Also note that, in a pinch, if you’re using a strange command that does not play nicely with estout, you can often select any Stata regression output, select the output, right-click, do “Copy Table”, and paste the result into Excel. This is only if all else fails. . * Install estout if necessary * ssc install estout * Load auto data sysuse auto.dta, clear * Let&#39;s provide it two regressions * Making sure to store the results each time reg mpg weight estimates store weightonly reg mpg weight foreign estimates store weightandforeign * Now let&#39;s export the table using estout * while renaming the variables for readability using the variable labels already in Stata * replacing any table we&#39;ve already made * and making an HTML table with style(html) * style(tex) also works, and the default is tab-delimited data for use in Excel. * Note also the default is to display t-statistics in parentheses. If we want * standard errors instead, we say so with se esttab weightonly weightandforeign using my_reg_output.html, label replace style(html) se . Which produces: . . | . | (1) | (2) | . | Mileage (mpg) | Mileage (mpg) | . . | . Weight (lbs.) | -0.00601*** | -0.00659*** | . | (0.000518) | (0.000637) | . &nbsp; | . Car type | | -1.650 | . | | (1.076) | . &nbsp; | . Constant | 39.44*** | 41.68*** | . | (1.614) | (2.166) | . . | . Observations | 74 | 74 | . . | . Standard errors in parentheses* p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001 | .",
    "url": "/Presentation/export_a_formatted_regression_table.html",
    "relUrl": "/Presentation/export_a_formatted_regression_table.html"
  }
  ,"19": {
    "title": "Fixed Effects in Linear Regression",
    "content": "Fixed Effects in Linear Regression . Fixed effects is a statistical regression model in which the intercept of the regression model is allowed to vary freely across individuals or groups. It is often applied to panel data in order to control for any individual-specific attributes that do not vary across time. . For more information, see Wikipedia: Fixed Effects Model. . Keep in Mind . To use individual-level fixed effects, you must observe the same person multiple times (panel data). | In a linear regression context, fixed effects regression is relatively straightforward, and can be thought of as effectively adding a binary control variable for each individual, or subtracting the within-individual mean of each variable (the “within” estimator). However, you may want to apply fixed effects to other models like logit or probit. This is usually possible (depending on the model), but if you just add a set of binary controls or subtract within-individual means, it won’t work very well. Instead, look for a command specifically designed to implement fixed effects for that model. | If you are using fixed effects to estimate the causal effect of a variable , individuals with more variance in will be weighted more heavily (Gibbons, Serrano, &amp; Urbancic 2019, ungated copy here. You may want to consider weighting your regression by the inverse within-individual variance of . | . Also Consider . Instead of fixed effects you may want to use random effects, which requires additional assumptions but is statistically more efficient and also allows the individual effect to be modeled using covariates. See Linear Mixed-Effects Regression | You may want to consider clustering your standard errors at the same level as (some or more of) your fixed effects. | . Implementations . Julia . Julia provides support for estimating high-dimensional fixed effect models through the FixedEffectModels.jl package (link). Similarly to felm (R) and reghdfe (Stata), the package uses the method of alternating projections to sweep out fixed effects. However, the Julia implementation is typically quite a bit faster than these other two methods. It also offers further performance gains via GPU computation for users with a working CUDA installation (up to an order of magnitude faster for complicated problems). . # If necessary, install JuliaFixedEffects.jl and some ancilliary packages for reading in the data # ] add JuliaFixedEffects, CSVFiles, DataFrames # Read in the example CSV and convert to a data frame using CSVFiles, DataFrames df = DataFrame(load(&quot;https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv&quot;)) # Calculate proportion of graduates working df[!, :prop_working] = df[!, :count_working] ./ (df[!, :count_working ] .+ df[!, :count_not_working]) using JuliaFixedEffects # Regress median earnings on the proportion of working graduates. # We&#39;ll control for institution name and year as our fixed effects. # We&#39;ll also cluster our standard errors by institution name. reg(df, @formula(earnings_med ~ prop_working + fe(inst_name) + fe(year)), Vcov.cluster(:inst_name)) # Multithread example Threads.nthreads() ## See: https://docs.julialang.org/en/v1.2/manual/parallel-computing/#man-multithreading-1 reg(df, @formula(earnings_med ~ prop_working + fe(inst_name) + fe(year)), Vcov.cluster(:inst_name), method = :lsmr_threads) # GPU example (requires working CUDA installation) reg(df, @formula(earnings_med ~ prop_working + fe(inst_name) + fe(year)), Vcov.cluster(:inst_name), method = :lsmr_gpu) . R . There are numerous packages for estimating fixed effect models in R. We will limit our examples here to the two fastest implementations — lfe::felm and fixest::feols — both of which support high-dimensional fixed effects and standard error correction (multiway clustering, etc.). . We first demonstrate fixed effects in R using felm from the lfe package (link). lfe::felm uses the Method of Alternating Projections to “sweep out” the fixed effects and avoid estimating them directly. By default, this is automatically done in parallel, using all available cores on a user’s machine to maximize performance. (It is also possible to change this behaviour.) . # If necessary, install lfe # install.packages(&#39;lfe&#39;) library(lfe) # Read in data from the College Scorecard df &lt;- read.csv(&#39;https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv&#39;) # Calculate proportion of graduates working df$prop_working &lt;- df$count_working/(df$count_working + df$count_not_working) # A felm formula is constructed as: # outcome ~ # covariates | # fixed effects | # instrumental variables specification | # cluster variables for standard errors # Here let&#39;s regress earnings_med on prop_working # with institution name and year as our fixed effects # And clusters for institution name felm_model &lt;- felm(earnings_med ~ prop_working | inst_name + year | 0 | inst_name, data = df) # Look at our results summary(felm_model) . Next, we consider feols from the fixest package (link). The syntax is very similar to lfe::felm and again the estimation will be done in parallel by default. However, rather than the method of alternating projections, fixest::feols uses a concentrated maximum likelihood method to efficiently estimate models with an arbitrary number of fixed effects. Current benchmarks suggest that this can yield significant speed gains, especially for large problems. For the below example, we’ll continue with the same College Scorecard dataset already loaded into memory. . # If necessary, install fixest # install.packages(&#39;fixest&#39;) library(fixest) # Run the same regression as before feols_model &lt;- feols(earnings_med ~ prop_working | inst_name + year, data = df) # Look at our results # Standard errors are automatically clustered at the inst_name level summary(feols_model) # It is also possible to specify additional or different clustering of errors summary(feols_model, se = &quot;twoway&quot;) summary(feols_model, cluster = c(&quot;inst_name&quot;, &quot;year&quot;)) ## same as the above . As noted above, there are numerous other ways to implement fixed effect models in R. Users may also wish to look at the plm, lme4, and estimatr packages among others. For example, the latter’s estimatr::lm_robust function provides syntax that may be more familar syntax to new R users who are coming over from Stata. Note, however, that it will be less efficient for complicated models. . Stata . We will estimate fixed effects using Stata in two ways. First, using the built in xtreg command. Second, using the reghdfe package (link), which is more efficient and better handles multiple levels of fixed effects (as well as multiway clustering), but must be downloaded from SSC first. . * Load in College Scorecard data import delimited &quot;https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Model_Estimation/Data/Fixed_Effects_in_Linear_Regression/Scorecard.csv&quot;, clear * The missings are written as &quot;NA&quot;, let&#39;s turn this numeric destring count_not_working count_working earnings_med, replace force * Calculate the proportion working g prop_working = count_working/(count_working + count_not_working) * xtset requires that the individual identifier be a numeric variable encode inst_name, g(name_number) * Set the data as panel data with xtset xtset name_number * Use xtreg with the &quot;fe&quot; option to run fixed effects * Regressing earnings_med on prop_working * with fixed effects for name_number (implied by fe) * and also year (which we&#39;ll add manually with i.year) * and standard errors clustered by name_number xtreg earnings_med prop_working i.year, fe vce(cluster name_number) * Now, let&#39;s demonstrate the same regression with reghdfe. * Install the package first if necessary. * ssc install reghdfe * For reghdfe we don&#39;t need to xtset the data. Let&#39;s undo that xtset, clear * We specify both sets of fixed effects in absorb() reghdfe earnings_med prop_working, absorb(name_number year) vce(cluster inst_name) .",
    "url": "/Model_Estimation/fixed_effects_in_linear_regression.html",
    "relUrl": "/Model_Estimation/fixed_effects_in_linear_regression.html"
  }
  ,"20": {
    "title": "Heteroskedasticity-consistent standard errors",
    "content": "Heteroskedasticity-consistent (HC) standard errors . Heteroskedasticity is when the variance of a model’s error term is related to the predictors in that model. For more information, see Wikipedia: Heteroscedasticity. . Many regression models assume homoskedasticity (i.e. constant variance of the error term), especially when calculating standard errors. So in the presence of heteroskedasticity, standard errors will be incorrect. Heteroskedasticity-consistent (HC) standard errors — also called “heteroskedasticity-robust”, or sometimes just “robust” standard errors — are calculated without assuming such homoskedasticity. For more information, see Wikipedia: Heteroscedasticity-consistent standard errors. . Keep in Mind . Robust standard errors are a common way of dealing with heteroskedasticity. However, they make certain assumptions about the form of that heteroskedasticity which may not be true. You may instead want to use GMM instead. | For nonlinear models like Logit, heteroskedasticity can bias estimates in addition to messing up standard errors. Simply using a robust covariance matrix will not eliminate this bias. Check the documentation of your nonlinear regression command to see whether its robust-error options also adjust for this bias. If not, consider other ways of dealing with heteroskedasticity besides robust errors. | There are multiple kinds of robust standard errors, for example HC1, HC2, and HC3. Check in to the kind available to you in the commands you’re using. | . Also Consider . Generalized Method of Moments | Cluster-Robust Standard Errors | Bootstrap Standard Errors | Jackknife Standard Errors | . Implementations . R . The easiest way to obtain robust standard errors in R is with the estimatr package (link) and its family of lm_robust functions. These will default to “HC2” errors, but users can specify a variety of other options. . # If necessary, install estimatr # install.packages(c(&#39;estimatr&#39;)) library(estimatr) # Get mtcars data # data(mtcars) ## Optional: Will load automatically anyway # Default is &quot;HC2&quot;. Here we&#39;ll specify &quot;HC3&quot; just to illustrate. m1 &lt;- lm_robust(mpg ~ cyl + disp + hp, data = mtcars, se_type = &quot;HC3&quot;) summary(m1) . Alternately, users may consider the vcovHC function from the sandwich package (link), which is very flexible and supports a wide variety of generic regression objects. For inference (t-tests, etc.), use in conjunction with the coeftest function from the lmtest package (link). . # If necessary, install lmtest and sandwich # install.packages(c(&#39;lmtest&#39;,&#39;sandwich&#39;)) library(sandwich) library(lmtest) # Create a normal regression model (i.e. without robust standard errors) m2 &lt;- lm(mpg ~ cyl + disp + hp, data = mtcars) # Get the robust VCOV matrix using sandwich::vcovHC(). We can pick the kind of robust errors # with the &quot;type&quot; argument. Note that, unlike estimatr::lm_robust(), the default this time # is &quot;HC3&quot;. I&#39;ll specify it here anyway just to illustrate. vcovHC(m2, type = &quot;HC3&quot;) sqrt(diag(vcovHC(m2))) ## HAC SEs # For statistical inference, use together with lmtest::coeftest(). coeftest(m2, vcov = vcovHC(m2)) . Stata . Stata has robust standard errors built into most regression commands, and they generally work the same way for all commands. . * Load in auto data sysuse auto.dta, clear * Just add robust to the options of the regression * This will give you HC1 regress price mpg gear_ratio foreign, robust * For other kinds of robust standard errors use vce() regress price mpg gear_ratio foreign, vce(hc3) .",
    "url": "/Model_Estimation/Nonstandard_Errors/hc_se.html",
    "relUrl": "/Model_Estimation/Nonstandard_Errors/hc_se.html"
  }
  ,"21": {
    "title": "Heatmap Colored Correlation Matrix",
    "content": "Heatmap Colored Correlation Matrix . A correlation matrix shows the correlation between different variables in a matrix setting. However, because these matrices have so many numbers on them, they can be difficult to follow. Heatmap coloring of the matrix, where one color indicates a positive correlation, another indicates a negative correlation, and the shade indicates the strength of correlation, can make these matrices easier for the reader to understand. . Keep in Mind . Even with heatmap coloring, very large correlation matrices can still be difficult to read, as you must pinpoint which variable names go with which cell of the matrix. Consider breaking big correlation matrices up into smaller ones, or limiting the amount of data you’re trying to show in some other way. | . Also Consider . You may just want to create a correlation matrix | . Implementations . Python . We present two ways you can create a heatmap. First, the seaborn package has a great collection of premade plots, one of which is a heatmap we’ll use. The second we’ll only point you to, which is a “by hand” approach that will allow you more customization. . For the by hand approach, see this guide. . For the seaborn approach, you will need to pip install seaborn or conda install seaborn before continuing. Once you’ve done that, the follow code will produce the below plot. . # Ganked from https://seaborn.pydata.org/examples/many_pairwise_correlations.html # Assumes you have run `pip install numpy pandas matplotlib scikit-learn seaborn` # Standard imports import numpy as np import pandas as pd from matplotlib import pyplot as plt # For this example we&#39;ll use Seaborn, which has some nice built in plots import seaborn as sns # Grab a data set from scikit-learn from sklearn.datasets import fetch_california_housing data = fetch_california_housing() df = pd.DataFrame( np.c_[data[&#39;data&#39;], data[&#39;target&#39;]], columns=data[&#39;feature_names&#39;] + [&#39;target&#39;] ) # Create the correlation matrix corr = df.corr() # Generate a mask for the upper triangle; True = do NOT show mask = np.zeros_like(corr, dtype=np.bool) mask[np.triu_indices_from(mask)] = True # Set up the matplotlib figure f, ax = plt.subplots(figsize=(11, 9)) # Generate a custom diverging colormap cmap = sns.diverging_palette(220, 10, as_cmap=True) # Draw the heatmap with the mask and correct aspect ratio # More details at https://seaborn.pydata.org/generated/seaborn.heatmap.html sns.heatmap( corr, # The data to plot mask=mask, # Mask some cells cmap=cmap, # What colors to plot the heatmap as annot=True, # Should the values be plotted in the cells? vmax=.3, # The maximum value of the legend. All higher vals will be same color vmin=-.3, # The minimum value of the legend. All lower vals will be same color center=0, # The center value of the legend. With divergent cmap, where white is square=True, # Force cells to be square linewidths=.5, # Width of lines that divide cells cbar_kws={&quot;shrink&quot;: .5} # Extra kwargs for the legend; in this case, shrink by 50% ) # You can save this as a png with # f.savefig(&#39;heatmap_colored_correlation_matrix_seaborn_python.png&#39;) . . R . We will be creating our heatmap in two different ways. First, we will be using the corrplot package, which is tailor-made for the task and is very easy to use. Then, we will be using ggplot2 with geom_tile, which requires much more preprocessing to use, but then provides access to the entirety of the ggplot2 package for customization. . First, we will use corrplot: . # Install the corrplot package if necessary # install.packages(&#39;corrplot&#39;) # Load in the corrplot package library(corrplot) # Load in mtcars data data(mtcars) # Don&#39;t use too many variables or it will get messy! mtcars &lt;- mtcars[,c(&#39;mpg&#39;,&#39;cyl&#39;,&#39;disp&#39;,&#39;hp&#39;,&#39;drat&#39;,&#39;wt&#39;,&#39;qsec&#39;)] # Create a corrgram corrplot(cor(mtcars), # Using the color method for a heatmap method = &#39;color&#39;, # And the lower half only for easier readability type = &#39;lower&#39;, # Omit the 1&#39;s along the diagonal to bring variable names closer diag = FALSE, # Add the number on top of the color addCoef.col = &#39;black&#39; ) . This results in: . . Now we will make the graph using ggplot2. We will also make a little use of dplyr and tidyr, and so we’ll load them all as a part of the tidyverse. This example makes use of this guide. . # Install the tidyverse if necessary # install.packages(&#39;tidyverse&#39;) # Load in the tidyverse library(tidyverse) # Load in mtcars data data(mtcars) # Create a correlation matrix. C &lt;- mtcars %&gt;% # Don&#39;t use too many variables or it will get messy! # We use dplyr&#39;s select() here but there are other ways to limit variables, like [] select(cyl, disp, drat, hp, mpg, qsec, wt) %&gt;% # Correlation matrix cor() # At this point, we can limit the matrix to just its lower half # Note this will give weird results if you didn&#39;t select variables in alphabetical order earlier C[upper.tri(C)] &lt;- NA C &lt;- C %&gt;% # Turn it into a data frame as.data.frame() %&gt;% # with a column for the variable names. # We use dplyr&#39;s mutate to create this column but it could be made with $ # the . here means &quot;the data set we&#39;re working with&quot; mutate(Variable = row.names(.)) # Use tidyr&#39;s pivot_longer to reshape to long format # There are other ways to reshape too C_Long &lt;- pivot_longer(C, cols = c(mpg, cyl, disp, hp, drat, wt, qsec), # We will want this option for sure if we dropped the # upper half of the triangle earlier values_drop_na = TRUE) %&gt;% # Make both variables into factors mutate(Variable = factor(Variable), name = factor(name)) %&gt;% # Reverse the order of one of the variables so that the x and y variables have # Opposing orders, common for a correlation matrix mutate(Variable = factor(Variable, levels = rev(levels(.$Variable)))) # Now we graph! ggplot(C_Long, # Our x and y axis are Variable and name # And we want to fill each cell with the value aes(x = Variable, y = name, fill = value))+ # geom_tile to draw the graph geom_tile() + # Color the graph as we like # Here our negative correlations are red, positive are blue # gradient2 instead of gradient gives us a &quot;mid&quot; color which we can make white scale_fill_gradient2(low = &quot;red&quot;, high = &quot;blue&quot;, mid = &quot;white&quot;, midpoint = 0, limit = c(-1,1), space = &quot;Lab&quot;, name=&quot;Pearson nCorrelation&quot;) + # Axis names don&#39;t make much sense labs(x = NULL, y = NULL) + # We don&#39;t need that background theme_minimal() + # If we need more room for variable names at the bottom, rotate them theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1)) + # We want those cells to be square! coord_fixed() + # If you also want the correlations to be written directly on there, add geom_text geom_text(aes(label = round(value,3))) . This results in: . . SAS . See this guide. . Stata . Stata has the installable package corrtable which produces heatmap correlation tables. Handily, it puts the variable labels (or names, if labels aren’t available) along the diagonal where they are easy to read. Note that it does run quite slowly. . * Install corrtable if necessary * ssc install corrtable * Get auto data sysuse auto.dta, clear * Make correlation table * The half option just shows the lower triangle and puts variable names on the axis. * The flag1 and howflag1 options tell corrtable to plot positive correlations (r(rho &gt; 0)) * as blue (blue*.1) * and flag2 and howflag2 similarly tell it to plot negative correlations as pink. corrtable price-length, half flag1(r(rho) &gt; 0) howflag1(plotregion(color(blue * 0.1))) flag2(r(rho) &lt; 0) howflag2(plotregion(color(pink*0.1))) . This results in: . .",
    "url": "/Presentation/heatmap_colored_correlation_matrix.html",
    "relUrl": "/Presentation/heatmap_colored_correlation_matrix.html"
  }
  ,"22": {
    "title": "Heckman Correction Model",
    "content": "Heckman Correction Model . The Heckman correction for sample selection is a method designed to be used in cases where the model can only be run on a subsample of the data that is not randomly selected. For example, a regression using to predict cannot include people who don’t work, since we don’t observe their wage. The Heckman model views this sample selection process as a form of omitted variable bias. So, it (1) explicitly models the process of selecting into the sample, (2) transforms the predicted probability of being in the sample, and (3) adds a correction term to the model. . For more information, see Wikipedia: Heckman correction. . Keep in Mind . Conceptually, the Heckman model uses the regression covariates to predict selection, transforms the prediction, and then includes that transformation in the model. If there are no variables in the selection model that are excluded from the regression model, then the Heckman model is perfectly collinear, and is only statistically identified because the transformation is nonlinear (you may have heard the phrase “identified by nonlinearity” or “identified by the normality assumption”). That’s not ideal! You want to find an exclusion restriction - a variable that predicts selection, but does not belong in the final regression model - to avoid this collinearity. | . Also Consider . There are many ways to estimate a Heckman model. Maximum likelihood approaches generally have better statistical properties, but two-stage models are computationally simpler. Often you can look in the options of your Heckman estimator command to select an estimation method. | If your goal is to estimate the effect of a binary treatment by modeling selection into treatment, consider a Treatment Effect Model, or an Endogenous Switching Model which also allows predictors to work differently in different settings. | Standard Heckman models rely heavily on assumptions about the normality of error terms. You may want to consider Nonparametric Sample Selection Models. | . Implementations . Gretl . See here for a demonstration. . Python . See here for a demonstration. . R . # Install sampleSelection package if necessary # install.packages(&#39;sampleSelection&#39;) library(sampleSelection) # Get data from Mroz (1987, Econometrica) # which has Panel Study of Income Dynamics data for married women data(&quot;Mroz87&quot;) # First consider our selection model # We only observe wages for labor force participants (lfp == 1) # So we model that as a function of work experience (linear and squared), # income from the rest of the family, education, and number of kids 5 or younger. # lfp ~ exper + I(exper^2) + faminc + educ + kids5 # Then we model the regression of interest. We&#39;re interested in modeling # wage as a function of work experience, education, and whether you&#39;re in a city # Here, we don&#39;t include family income or number of kids, under the assumption that they # do not belong in a wage model. These are our exclusion restrictions # (note these particular exclusion restrictions might be a little dubious! But hey, this paper&#39;s from 1987.) # wage ~ exper + I(exper^2) + educ + city # Put them together in a selection() command heck_model &lt;- selection(lfp ~ exper + I(exper^2) + faminc + educ + kids5, wage ~ exper + I(exper^2) + educ + city, Mroz87) summary(heck_model) . Stata . * Get data from Mroz (1987, Econometrica) * which has Panel Study of Income Dynamics data for married women * (data via the sampleSelection package in R) import delimited &quot;https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Estimation/Data/Heckman_Correction_Model/Mroz87.csv&quot;, clear * First, consider the regression of interest. * First consider our selection model * We only observe wages for labor force participants (lfp == 1) * So we model that as a function of work experience (linear and squared), * income from the rest of the family, education, and number of kids 5 or younger. * select(lfp = c.exper##c.exper faminc educ kids5) * Then we model the regression of interest. We&#39;re interested in modeling * wage as a function of work experience, education, and whether you&#39;re in a city * Here, we don&#39;t include family income or number of kids, under the assumption that they * do not belong in a wage model. These are our exclusion restrictions * (note these particular exclusion restrictions might be a little dubious! But hey, 1987.) * wage c.exper##c.exper educ city * Now we run our Heckman model! heckman wage c.exper##c.exper educ city, select(lfp = c.exper##c.exper faminc educ kids5) .",
    "url": "/Model_Estimation/heckman_correction_model.html",
    "relUrl": "/Model_Estimation/heckman_correction_model.html"
  }
  ,"23": {
    "title": "Import a Foreign Data File",
    "content": "Import a Foreign Data File . Commonly, data will be distributed in a format that is not native to the software that you are using, such as Excel. How can you import it? . This page is specifically about importing data files from formats specific to particular foreign software. For importing standard shared formats, see Import a Delimited Data File (CSV, TSV) or Import a Fixed-Width Data File. . Keep in Mind . Check your data after it’s imported to make sure it worked properly. Sometimes special characters will have trouble converting, or variable name formats are inconsistent, and so on. It never hurts to check! | . Also Consider . Import a Delimited Data File (CSV, TSV) | Import a Fixed-Width Data File | Export Data to a Foreign Format | . Implementations . Because there are so many potential foreign formats, these implementations will be more about listing the appropriate commands with example syntax than providing full working examples. Make sure that you fill in the proper filename. The filename should include a filepath, or you should Set a Working Directory. . R . # Load Excel files with the readxl package # install.packages(&#39;readxl&#39;) library(readxl) data &lt;- read_excel(&#39;filename.xlsx&#39;) # Read Stata, SAS, and SPSS files with the haven package # install.packages(&#39;haven&#39;) library(haven) data &lt;- read_stata(&#39;filename.dta&#39;) data &lt;- read_spss(&#39;filename.sav&#39;) # read_sas also supports .sas7bcat, or read_xpt supports transport files data &lt;- read_sas(&#39;filename.sas7bdat&#39;) # Read lots of other types with the foreign package # install.packages(&#39;foreign&#39;) library(foreign) data &lt;- read.arff(&#39;filename.arff&#39;) data &lt;- read.dbf(&#39;filename.dbf&#39;) data &lt;- read.epiinfo(&#39;filename.epiinfo&#39;) data &lt;- read.mtb(&#39;filename.mtb&#39;) data &lt;- read.octave(&#39;filename.octave&#39;) data &lt;- read.S(&#39;filename.S&#39;) data &lt;- read.systat(&#39;filename.systat&#39;) . Stata . Stata can import foreign files using the File -&gt; Import menu. Alternately, you can use the import command: . import type using filename . where type can be excel, spss, sas, haver, or dbase (import can also be used to download data directly from sources like FRED). .",
    "url": "/Other/import_a_foreign_data_file.html",
    "relUrl": "/Other/import_a_foreign_data_file.html"
  }
  ,"24": {
    "title": "Instrumental Variables",
    "content": "Instrumental Variables . In the regression model . where is an error term, the estimated XYXX epsilon$$ and so determined by forces within the model (endogenous). . One way to recover the causal effect of on is to use instrumental variables. If there exists a variable that is related to but is completely unrelated to (perhaps after adding some controls), then you can use instrumental variables estimation to isolate only the part of the variation in that is explained by . Naturally, then, this part of the variation is unrelated to because is unrelated to , and you can get the causal effect of that part of . . For more information, see Wikipedia: Instrumental variables estimation. . Keep in Mind . Technically, all the variables in the model except for the dependent variable and the endogenous variables are “instruments”, including controls. However, it is also common to refer to only the excluded instruments (i.e., variables that are only used to predict the endogenous variable, not the dependent variable) as instruments. This page will follow that convention. | For instrumental variables to work, it must be the case that the instrument is only related to the outcome variable through other variables already included in the model like the endogenous variables or the controls. This is called the “validity” assumption and it cannot be verified in the data, only theoretically. Give serious consideration as to whether validity applies to your instrument before using instrumental variables. | You can check for the relevance of your instrument, which is how strongly related it is to your endogenous variable. A rule of thumb is that an joint F-test of the instruments should be at least 10, but this is only a rule of thumb, and imprecise (see Stock and Yogo 2005 for a more precise version of this test). In general, if the instruments are not very strong predictors of the endogenous variables, you should consider whether your analysis fits the assumptions necessary to run a weak-instrument-robust estimation method. See Hahn &amp; Hausman 2003 for an overview. | Instrumental variables estimates a local average treatment effect - in other words, a weighted average of each individual observation’s treatment effect, where the weights are based on the strength of the effect of the instrument on the endogenous variable. Note both that this is not the same thing as an average treatment effect, which is an average of each individual’s treatment effect, which is usually what is desired, and also that if the instrumental variable has effects of different signs for different people (non-monotonicity), then the estimate isn’t really anything of interest. Be sure that monotonicity makes sense in your context before using instrumental variables. | Instrumental variables is a consistent estimator of a causal effect, but it is biased in finite samples. Be wary of using instrumental variables in small samples. | . Also Consider . Instrumental variables methods generally rely on linearity assumptions, and if your dependent or endogenous variables are not continuous, their assumptions may not hold. Consider methods specially designed for nonlinear instrumental variables estimation. | There are many ways to estimate instrumental variables, not just two stage least squares. Different estimators such as GMM or k-class limited-information maximum likelihood estimators perform better or worse depending on heterogeneous treatment effects, heteroskedasticity, and sample size. Many instrumental variables estimation commands allow for multiple different estimation methods, described below. Note that in the just-identified case (where the number of instruments is the same as the number of endogenous variables), several common estimators produce identical results. | . Implementations . R . There are several ways to run instrumental variables in R. Here we will cover two - AER::ivreg(), which is probably the most common, and lfe::felm(), which is more flexible and powerful. You may also want to consider looking at estimatr::iv_robust, which combines much of the flexibility of lfe::felm() with the simple syntax of AER::ivreg(), although it is not as powerful. . # If necessary, install both packages. # install.packages(c(&#39;AER&#39;,&#39;lfe&#39;)) # Load AER library(AER) # Load the Cigarettes data from ivreg, following the example data(CigarettesSW) # We will be using cigarette taxes as an instrument for cigarette prices # to evaluate the effect of cigarette price on log number of packs smoked # With income per capita as a control # Adjust everything for inflation CigarettesSW$rprice &lt;- CigarettesSW$price/CigarettesSW$cpi CigarettesSW$rincome &lt;- CigarettesSW$income/CigarettesSW$population/CigarettesSW$cpi CigarettesSW$tdiff &lt;- (CigarettesSW$taxs - CigarettesSW$tax)/CigarettesSW$cpi # The regression formula takes the format # dependent.variable ~ endogenous.variables + controls | instrumental.variables + controls ivmodel &lt;- ivreg(log(packs) ~ log(rprice) + log(rincome) | tdiff + log(rincome), data = CigarettesSW) summary(ivmodel) # Now we will run the same model with lfe::felm library(lfe) # The regression formula takes the format # dependent vairable ~ # controls | # fixed.effects | # (endogenous.variables ~ instruments) | # clusters.for.standard.errors # So if need be it is straightforward to adjust this example to account for # fixed effects and clustering. # Note the 0 indicating no fixed effects ivmodel2 &lt;- felm(log(packs) ~ log(rincome) | 0 | (log(rprice) ~ tdiff), data = CigarettesSW) summary(ivmodel2) # felm can also use several k-class estimation methods; see help(felm) for the full list. # Let&#39;s run it with a limited-information maximum likelihood estimator with # the fuller adjustment set to minimize squared error (4). ivmodel3 &lt;- felm(log(packs) ~ log(rincome) | 0 | (log(rprice) ~ tdiff), data = CigarettesSW, kclass = &#39;liml&#39;, fuller = 4) summary(ivmodel3) . Stata . Instrumental variables estimation in Stata typically uses the built-in ivregress command. This command can be used to implement linear instrumental variables regression using two-stage least squares, GMM, or LIML . * Get Stock and Watson Cigarette data import delimited &quot;https://vincentarelbundock.github.io/Rdatasets/csv/Zelig/CigarettesSW.csv&quot;, clear * Adjust everything for inflation g rprice = price/cpi g rincome = (income/population)/cpi g tdiff = (taxs - tax)/cpi * And take logs g lpacks = ln(packs) g lrincome = ln(rincome) g lrprice = ln(rprice) * The syntax for the regression is * name_of_estimator dependent_variable controls (endogenous_variable = instruments) * where name_of_estimator can be two stage least squares (2sls), * limited information maximum likelihood (liml, note that ivregress doesn&#39;t support k-class estimators), * or generalized method of moments (gmm) * Here we can run two stage least squares ivregress 2sls lpacks rincome (lrprice = tdiff) * Or gmm. ivregress gmm lpacks rincome (lrprice = tdiff) .",
    "url": "/Model_Estimation/instrumental_variables.html",
    "relUrl": "/Model_Estimation/instrumental_variables.html"
  }
  ,"25": {
    "title": "Interaction Terms and Polynomials",
    "content": "Interaction Terms and Polynomials . Regression models generally assume that the outcome variable is a function of an index, which is a linear function of the independent variables, for example in ordinary least squares: . However, if the independent variables have a nonlinear effect on the outcome, the model will be incorrectly specified. This is fine as long as that nonlinearity is modeled by including those nonlinear terms in the index. . The two most common ways this occurs is by including interactions or polynomial terms. With an interaction, the effect of one variable varies according to the value of another: . and with polynomial terms, the effect of one variable one the outcome is allowed to take a non-linear shape: . Keep in Mind . When you have interaction terms or polynomials, the effect of a variable can no longer be described with a single coefficient, and in some senses the individual coefficients lose meaning without the others. You can understand the effect of a single variable by taking the derivative of the index with respect to that variable. For example, in , the effect of on is . You must plug in the value of to get the effect of . Or in , the effect of is . You must plug in a value of to get the marginal effect of at that value. | In almost all cases, if you are including an interaction term, you should also include each of the interacted variables on their own. Otherwise, the coefficients become very difficult to interpret. | In almost all cases, if you are including a polynomial, you should include all terms of the polynomial. In other words, include the linear and squared term, not just the squared term. | . Also Consider . Interaction terms tend to have low statistical power. Consider performing a power analysis of interaction terms before running your analysis. | Polynomials are not the only way to model a nonlinear relationship. You could, for example, run one of many kinds of nonparametric regression. | You may want to get the average marginal effects or the marginal effects at the mean of your variables after running your model. | One common way to display the effects of a model with interactions is to graph them. See marginal effects plots for interactions with continuous variables and Marginal effects plots for interactions with continuous variables | . Implementations . Python . Using the statsmodels package, we can use a similar formulation as the R example below. . # Standard imports import numpy as np import pandas as pd import statsmodels.formula.api as sms from matplotlib import pyplot as plt # Load the R mtcars dataset from a URL df = pd.read_csv(&#39;https://raw.githubusercontent.com/LOST-STATS/lost-stats.github.io/source/Data/mtcars.csv&#39;) # Include a linear, squared, and cubic term using the I() function. # N.B. Python uses ** for exponentiation (^ means bitwise xor) model1 = sms.ols(&#39;mpg ~ hp + I(hp**2) + I(hp**3) + cyl&#39;, data=df) print(model1.fit().summary()) # Include an interaction term and the variables by themselves using * # The interaction term is represented by hp:cyl model2 = sms.ols(&#39;mpg ~ hp * cyl&#39;, data=df) print(model2.fit().summary()) # Equivalently, you can request &quot;all quadratic interaction terms&quot; by doing model3 = sms.ols(&#39;mpg ~ (hp + cyl) ** 2&#39;, data=df) print(model3.fit().summary()) # Include only the interaction term and not the variables themselves with : # Hard to interpret! Occasionally useful though. model4 = sms.ols(&#39;mpg ~ hp : cyl&#39;, data=df) print(model4.fit().summary()) . R . # Load mtcars data data(mtcars) # Include a linear, squared, and cubic term using the I() function model1 &lt;- lm(mpg ~ hp + I(hp^2) + I(hp^3) + cyl, data = mtcars) # Include a linear, squared, and cubic term using the poly() function # The raw = TRUE option will give the exact same result as model1 # Omitting this will give you orthogonal polynomial terms, # which are not correlated with each other but are more difficult to interpret model2 &lt;- lm(mpg ~ poly(hp, 3, raw = TRUE) + cyl, data = mtcars) # Include an interaction term and the variables by themselves using * model3 &lt;- lm(mpg ~ hp*cyl, data = mtcars) # Include only the interaction term and not the variables themselves with : # Hard to interpret! Occasionally useful though. model4 &lt;- lm(mpg ~ hp:cyl, data = mtcars) . Stata . Stata allows interaction and polynomial terms using hashtags ## to join together variables to make interactions, or joining a variable with itself to get a polynomial. You must also specify whether each variable is continuous (prefix the variable with c.) or a factor (prefix with i.). . * Load auto data sysuse auto.dta, clear * Use ## to interact variables together and also include the variables individually * foreign is a factor variable so we prefix it with i. * weight is continuous so we prefix it with c. reg mpg c.weight##i.foreign * Use # to include just the interaction term and not the variables themselves * If one is a factor, this will include the effect of the continuous variable * For each level of the factor reg mpg c.weight#i.foreign * Interact a variable with itself to create a polynomial term reg mpg c.weight##c.weight##c.weight foreign .",
    "url": "/Model_Estimation/interaction_terms_and_polynomials.html",
    "relUrl": "/Model_Estimation/interaction_terms_and_polynomials.html"
  }
  ,"26": {
    "title": "Line Graph with Labels at the Beginning or End of Lines",
    "content": "Line Graph with Labels at the Beginning or End of Lines . A line graph is a common way of showing how a value changes over time (or over any other x-axis where there’s only one observation per x-axis value). It is also common to put several line graphs on the same set of axes so you can see how multiple values are changing together. . When putting multiple line graphs on the same set of axes, a good idea is to label the different lines on the lines themselves, rather than in a legend, which generally makes things easier to read. . Keep in Mind . Check the resulting graph to make sure that labels are legible, visible in the graph area, and don’t overlap. | . Also Consider . More generally, see Line graph and Styling line graphs. In particular, consider Styling line graphs in order to distinguish the lines by color, pattern, etc. in addition to labels | If there are too many lines to be able to clearly follow them, labels won’t help too much. Instead, consider Faceted graphs. | . Implementations . R . # If necessary, install ggplot2, lubridate, and directlabels # install.packages(c(&#39;ggplot2&#39;,&#39;directlabels&#39;, &#39;lubridate&#39;)) library(ggplot2) library(directlabels) # Load in Google Trends Nobel Search Data # Which contains the Google Trends global search popularity index for the four # research-based Nobel prizes over a month. df &lt;- read.csv(&#39;https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Presentation/Data/Line_Graph_with_Labels_at_the_Beginning_or_End_of_Lines/Research_Nobel_Google_Trends.csv&#39;) # Properly treat our date variable as a date # Not necessary in all applications of this technique. df$date &lt;- lubridate::ymd(df$date) # Construct our standard ggplot line graph # Drawing separate lines by name # And using the log of hits for visibility ggplot(df, aes(x = date, y = log(hits), color = name)) + labs(x = &quot;Date&quot;, y = &quot;Log of Google Trends Index&quot;)+ geom_line()+ # Since we are about to add line labels, we don&#39;t need a legend theme(legend.position = &quot;none&quot;) + # Add, from the directlabels package, # geom_dl, using method = &#39;last.bumpup&#39; to put the # labels at the end, and make sure that if they intersect, # one is bumped up geom_dl(aes(label = name), method = &#39;last.bumpup&#39;) + # Extend the x axis so the labels are visible - # Try the graph a few times until you find a range that works scale_x_date(limits = c(min(df$date), lubridate::ymd(&#39;2019-10-25&#39;))) . This results in: . . Stata . Unfortunately, performing this technique in Stata requires placing each text() label on the graph. However, this can be automated with the use of a for loop to build the code using locals. . * Load in Google Trends Nobel Search Data * Which contains the Google Trends global search popularity index for the four * research-based Nobel prizes over a month. import delimited &quot;https://raw.githubusercontent.com/LOST-STATS/LOST-STATS.github.io/master/Presentation/Data/Line_Graph_with_Labels_at_the_Beginning_or_End_of_Lines/Research_Nobel_Google_Trends.csv&quot;, clear * Convert the date variable to an actual date * (not necessary in all implementations) g ymddate = date(date, &quot;YMD&quot;) * Format the new variable as a date so we see it properly on the x-axis format ymddate %td * Graph log(hits) for visibility g loghits = log(hits) * Get the different prize types to graph levelsof name, l(names) * Figure out the last time period in the data set quietly summarize ymddate local lastday = r(max) * Start constructing a local that contains all the line graphs to graph local lines * Start constructing a local that contains the text labels to add local textlabs * Loop through each one foreach n in `names&#39; { * Add in the line graph code * by building on the local we already have (`lines&#39;) and adding a new twoway segment local lines `lines&#39; (line loghits ymddate if name == &quot;`n&#39;&quot;) * Figure out the value this line hits on the last point on the graph quietly summ loghits if name == &quot;`n&#39;&quot; &amp; ymddate == `lastday&#39; * The text command takes the y-value (from the mean we just took) * the x-value (the last day on the graph), * and the text label (the name we are working with) * Plus place(r) to put it to the RIGHT of that point local textlabs `textlabs&#39; text(`r(mean)&#39; `lastday&#39; &quot;`n&#39;&quot;, place(r)) } * Finally, graph our lines * with the twoway lines we&#39;ve specified, followed by the text labels * We&#39;re sure to remove the legend with legend(off) * and extend the x-axis so we can see the labels with xscale(range()) quietly summarize ymddate local start = r(min) local end = r(max) + 5 twoway `lines&#39;, `textlabs&#39; legend(off) xscale(range(`start&#39; `end&#39;)) xtitle(&quot;Date&quot;) ytitle(&quot;Log of Google Trends Index&quot;) . This results in: . .",
    "url": "/Presentation/line_graph_with_labels_at_the_beginning_or_end.html",
    "relUrl": "/Presentation/line_graph_with_labels_at_the_beginning_or_end.html"
  }
  ,"27": {
    "title": "Linear Mixed-Effects Regression",
    "content": "Linear Mixed-Effects Regression . Mixed-effects regression goes by many names, including hierarchical linear model, random coefficient model, and random parameter models. In a mixed-effects regression, some of the parameters are “random effects” which are allowed to vary over the sample. Others are “fixed effects”, which are not. Note that this use of the term “fixed effects” is not the same as in fixed effects regression. . For example, consider the model . The intercept has a $j$ subscript and is allowed to vary over the sample at the level, where may indicate individual or group, depending on context. The slope on , , is similarly allowed to vary over the sample. These are random effects. is not allowed to vary over the sample and so is fixed. . The random parameters have their own “level-two” equations, which may or may not include level-two covariates. . For more information see Wikipedia. . Keep in Mind . The assumptions necessary to use a mixed-effects model in general are the same as for most linear models. However, in addition, mixed-effects models assume that the error terms at different levels are unrelated. | At the second level, statistical power depends on the number of different values there are. Mixed-effects models may perform poorly if the coefficient is allowed to vary over only a few groups. | There’s no need to stop at two levels - the second-level coefficients can also be allowed to vary at a higher level. | . Also Consider . There are many variations of mixed-effects models for working with non-linear data, see nonlinear mixed-effects models. | If the goal is making predictions within subgroups, you may want to consider multi-level regression with poststratification. | . Implementations . R . One common way to fit mixed-effects models in R is with the lmer function in the lme4 package. To fit fully Bayesian models you may want to consider instead using STAN with the rstan package. See the multi-level regression with poststratification page for more information. . # Install lme4 if necessary # install.packages(&#39;lme4&#39;) # Load up lme4 library(lme4) # Load up university instructor evaluations data from lme4 data(InstEval) # We&#39;ll be treating lecture age as a numeric variable InstEval$lectage &lt;- as.numeric(InstEval$lectage) # Let&#39;s look at the relationship between lecture ratings andhow long ago the lecture took place # with a control for whether the lecture was a service lecture ols &lt;- lm(y ~ lectage + service, data = InstEval) summary(ols) # Now we will use lmer to allow the intercept to vary at the department level me1 &lt;- lmer(y ~ lectage + service + (1 | dept), data = InstEval) summary(me1) # Now we will allow the slope on lectage to vary at the department level me2 &lt;- lmer(y ~ lectage + service + (-1 + lectage | dept), data = InstEval) summary(me2) # Now both the intercept and lectage slope will vary at the department level me3 &lt;- lmer(y ~ lectage + service + (lectage | dept), data = InstEval) summary(me3) . Stata . Stata has a family of functions based around the mixed command that can estimate mixed-effects models. . * Load NLS-W data sysuse nlsw88.dta, clear * We are going to estimate the relationship between hourly wage and job tenure * with a contorl for marital status * Without mixed effects reg wage tenure married * Now we will allow the intercept to vary with occupation mixed wage tenure married || occupation: * Next we will allow the slope on tenure to vary with occupation mixed wage tenure married || occupation: tenure, nocons * Now, both! mixed wage tenure married || occupation: tenure * Finally we will allow the intercept and tenure slope to vary over both occupation * and age mixed wage tenure married || occupation: tenure || age: tenure .",
    "url": "/Model_Estimation/linear_mixed_effects_regression.html",
    "relUrl": "/Model_Estimation/linear_mixed_effects_regression.html"
  }
  ,"28": {
    "title": "Logit Model",
    "content": "Logit Regressions . A logistical regression (Logit) is a statistical method for a best-fit line between a binary [0/1] outcome variable and any number of independent variables. Logit regressions follow a logistical distribution and the predicted probabilities are bounded between 0 and 1. . For more information about Logit, see Wikipedia: Logit. . Keep in Mind . The beta coefficients from a logit model are maximum likelihood estimations. They are not the marginal effect, as you would see in an OLS estimation. So you cannot interpret the beta coefficient as a marginal effect of on . | To obtain the marginal effect, you need to perform a post-estimation command to discover the marginal effect. In general, you can ‘eye-ball’ the marginal effect by dividing the logit beta coefficient by 4. | . Implementations . Gretl . # Load auto data open auto.gdt # Run logit using the auto data, with mpg as the outcome variable # and headroom, trunk, and weight as predictors logit mpg const headroom trunk weight . R . R can run a logit regression using the glm() function. However, to get marginal effects you will need to calculate them by hand or use a package. We will use the mfx package, although the margins package is another good option, which produces tidy model output. . # If necessary, install the mfx package # install.packages(&#39;mfx&#39;) # mfx is only needed for the marginal effect, not the regression itself library(mfx) # Load mtcars data data(mtcars) # Use the glm() function to run logit # Here we are predicting engine type using # miles per gallon and number of cylinders as predictors my_logit &lt;- glm(vs ~ mpg + cyl, data = mtcars, family = binomial(link = &#39;logit&#39;)) # The family argument says we are working with binary data # and using a logit link function (rather than, say, probit) # The results summary(my_logit) # Marginal effects logitmfx(vs ~ mpg + cyl, data = mtcars) . Stata . * Load auto data sysuse auto.dta * Logit Estimation logit foreign mpg weight headroom trunk * Recover the Marginal Effects (Beta Coefficient in OLS) margins, dydx(*) .",
    "url": "/Model_Estimation/logit_model.html",
    "relUrl": "/Model_Estimation/logit_model.html"
  }
  ,"29": {
    "title": "Marginal Effects Plots for Interactions with Continuous Variables",
    "content": "Marginal Effects Plots for Interactions with Continuous Variables . In many contexts, the effect of one variable on another might be allowed to vary. For example, the relationship between income and mortality is nonlinear, so the effect of an additional dollar of income on mortality is different for someone earning $20,000/year than for someone earning $100,000/year. Or maybe the relationship between income and mortality differs depending on how many years of education you have. . A marginal effects plot displays the effect of on for different values of (or ). The plot will often include confidence intervals as well. The same code will often work if there’s not an explicit interaction, but you are, for example, estimating a logit model where the effect of one variable changes with the values of the others. . Keep in Mind . Interactions often have poor statistical power, and you will generally need a lot of observations to tell if the effect of $X$ on is different for two given different values of . | Make sure your graph has clearly labeled axes, so readers can tell whether your y-axis is the predicted value of $Y$ or the marginal effect of on . | . Also Consider . Consider performing a power analysis of interaction terms before running your analysis to see whether you have the statistical power for your interactions | Average marginal effects or marginal effects at the mean can be used to get a single marginal effect averaged over your sample, rather than showing how it varies across the sample. | Marginal effects plots for interactions with categorical variables | . Implementations . R . The interplot package can plot the marginal effect of a variable (y-axis) against different values of some variable. If instead you want the predicted values of on the y-axis, look at the ggeffects package. . # Install relevant packages, if necessary: # install.packages(c(&#39;ggplot2&#39;, &#39;interplot&#39;)) # Load in ggplot2 and interplot library(ggplot2) library(interplot) # Load in the txhousing data data(txhousing) # Estimate a regression with a nonlinear term cubic_model &lt;- lm(sales ~ listings + I(listings^2) + I(listings^3), data = txhousing) # Get the marginal effect of var1 (listings) # at different values of var2 (listings), with confidence ribbon. # This will return a ggplot object, so you can # customize using ggplot elements like labs(). interplot(cubic_model, var1 = &quot;listings&quot;, var2 = &quot;listings&quot;)+ labs(x = &quot;Number of Listings&quot;, y = &quot;Marginal Effect of Listings&quot;) # Try setting adding listings*date to the regression model # and then in interplot set var2 = &quot;date&quot; to get the effect of listings at different values of date . This results in: . . Stata . We will use the marginsplot command, which requires Stata 12 or higher. . * Load in the National Longitudinal Survey of Youth - Women sample sysuse nlsw88.dta * Perform a regression with a nonlinear term regress wage c.tenure##c.tenure * Use margins to calculate the marginal effects * Put the variable we&#39;re interested in getting the effect of in dydx() * And the values we want to evaluate it at in at() margins, dydx(tenure) at(tenure = (0(1)26)) * (If we had interacted with another variable, say age, we would specify similarly, * with at(age = (start(count-by)end))) * Then, marginsplot * The recast() and recastci() options make the effect/CI show up as a line/area * Remove to get points/lines instead. marginsplot, xtitle(&quot;Tenure&quot;) ytitle(&quot;Marginal Effect of Tenure&quot;) recast(line) recastci(rarea) . This results in: . .",
    "url": "/Presentation/marginal_effects_plots_for_interactions_with_continuous_variables.html",
    "relUrl": "/Presentation/marginal_effects_plots_for_interactions_with_continuous_variables.html"
  }
  ,"30": {
    "title": "McFadden's Choice Model (Alternative-Specific Conditional Logit)",
    "content": "McFadden’s Choice Model (Alternative-Specific Conditional Logit) . Discrete choice models are a regression method used to predict a categorical dependent variable with more than two categories. For example, a discrete choice model might be used to predict whether someone is going to take a train, car, or bus to work. . McFadden’s Choice Model is a discrete choice model that uses conditional logit, in which the variables that predict choice can vary either at the individual level (perhaps tall people are more likely to take the bus), or at the alternative level (perhaps the train is cheaper than the bus). . For more information, see Wikipedia: Discrete Choice . Keep in Mind . Just like other regression methods, the McFadden model does not guarantee that the estimates will be causal. Similarly, while the McFadden model is designed so that the results can be interpreted in terms of a “random utility” function, making inferences about utility functions does require additional assumptions. | The standard McFadden model assumes that the choice follows the Independence of Irrelevant Alternatives, which may be a strong assumption. There are variants of the McFadden model that relax this assumption. | If you are working with an estimation command that only allows alternative-specific predictors and not case-specific predictors, you can add them yourself by interacting the case-specific predictors with binary variables for the different alternatives. If is your case-specific variable and your alternatives are “train”, “bus”, and “car”, you’d add , , and to your model. These are your case-specific predictors. | Choice model regressions often have specific demands on how your data is structured. These vary across estimation commands and software packages. However, a common one is this (others will be pointed out in specific Implementations below): The data must contain a variable indicating the choice cases (i.e. you choose a car, that’s one case, then I choose a car, that’s a different case), a variable with the alternatives being chosen between, a binary variable equal to 1 for the alternative actually chosen (this should be 1 or TRUE exactly once within each choice case), and then variables that are case-specific or alternative-specific. | . In the below table, gives the choice case, gives the options, gives the choice, is a variable that varies at the alternative level, and is a variable that varies at the case level. . I Alts Chose X Y . 1 | A | 1 | 10 | 3 | . 1 | B | 0 | 20 | 3 | . 1 | C | 0 | 10.5 | 3 | . 2 | A | 0 | 8 | 5 | . 2 | B | 1 | 9 | 5 | . 3 | C | 0 | 1 | 5 | . This might be referred to as “long” choice data. “Wide” choice data is also common, and looks like: . I Chose Y XA XB XC . 1 | A | 3 | 10 | 20 | 10.5 | . 2 | B | 5 | 8 | 9 | 1 | . Also Consider . In order to relax the independence of irrelevant alternatives assumption and/or more closely model individual preferences, consider the mixed logit, nested logit or hierarchical Bayes conditional logit models. | . Implementations . R . We will implement McFadden’s choice model in R using the mlogit package, which can accept “wide” or “long” data in the mlogit.data function. . # If necessary, install mlogit package # install.packages(&#39;mlogit&#39;) library(mlogit) # Get Car data, in &quot;wide&quot; choice format data(Car) # For this we need to specify the choice variable with choice # whether it&#39;s currently in wide or long format with shape # the column numbers of the alternative-specific variables with varying. # We need alt.levels to tell us what our alternatives are (1-6, as seen in choice). # We also need sep = &quot;&quot; since our wide-format variable names are type1, type2, etc. # If the variable names were type_1, type_2, etc., we&#39;d need sep = &quot;_&quot;. # If this were long data we&#39;d also want: # the case identifier with id.var (for individuals) and/or chid.var # (for multiple choices within individuals) # And a variable indicating the alternatives with alt.var # But could skip the alt.levels and sep arguments mlogit.Car &lt;- mlogit.data(Car, choice = &#39;choice&#39;, shape = &#39;wide&#39;, varying = 5:70, alt.levels = 1:6, sep=&quot;&quot;) # mlogit.Car is now in &quot;long&quot; format # Note that if we did start with &quot;long&quot; format we could probably skip the mlogit.data() step. # Now we can run the regression with mlogit(). # We &quot;regress&quot; the choice on the alternative-specific variables like type, fuel, and price # Then put a pipe separator | # and add our case-specific variables like college model &lt;- mlogit(choice ~ type + fuel + price | college, data = mlogit.Car) # Look at the results summary(model) . Stata . Stata has the McFadden model built in. We will estimate the model using the older asclogit command as well as the cmclogit command that comes with Stata 16. These commands require “long” choice data, as described in the Keep in Mind section. . * Load in car choice data webuse carchoice * To use asclogit, we &quot;regress&quot; our choice variable (purchase) * on any alternative-specific variables (dealers) * then we put our case ID variable consumerid in case() * and our variable specifying alternatives, car, in alternatives() * then finally we put any case-specific variables like gender and income, in casevars() asclogit purchase dealers, case(consumerid) alternatives(car) casevars(gender income) * To use cmclogit, we first declare our data to be choice data with cmset * specifying our case ID variable and then the set of alternatives cmset consumerid car * Now that Stata knows the structure, we can omit those parts from the asclogit * specification, but the rest stays the same! cmclogit purchase dealers, casevars(gender income) . Why bother with the cmclogit version? cmset gives you a lot more information about your data, and makes it easy to transition between different choice model types, including those incorporating panel data (each person makes multiple choices). .",
    "url": "/Model_Estimation/mcfaddens_choice_model.html",
    "relUrl": "/Model_Estimation/mcfaddens_choice_model.html"
  }
  ,"31": {
    "title": "Nonstandard Errors",
    "content": "Nonstandard errors .",
    "url": "/Model_Estimation/Nonstandard_Errors/nonstandard_errors.html",
    "relUrl": "/Model_Estimation/Nonstandard_Errors/nonstandard_errors.html"
  }
  ,"32": {
    "title": "Ordinary Least Squares (Linear Regression)",
    "content": "Ordinary Least Squares (Linear Regression) . Ordinary Least Squares (OLS) is a statistical method that produces a best-fit line between some outcome variable and any number of predictor variables . These predictor variables may also be called independent variables or right-hand-side variables. . For more information about OLS, see Wikipedia: Ordinary Least Squares. . Keep in Mind . OLS assumes that you have specified a true linear relationship. | OLS results are not guaranteed to have a causal interpretation. Just because OLS estimates a positive relationship between and does not necessarily mean that an increase in will cause to increase. | OLS does not require that your variables follow a normal distribution. | . Also Consider . OLS standard errors assume that the model’s error term is IID, which may not be true. Consider whether your analysis should use heteroskedasticity-robust standard errors or cluster-robust standard errors. | If your outcome variable is discrete or bounded, then OLS is by nature incorrectly specified. You may want to use probit or logit instead for a binary outcome variable, or ordered probit or ordered logit for an ordinal outcome variable. | If the goal of your analysis is predicting the outcome variable and you have a very long list of predictor variables, you may want to consider using a method that will select a subset of your predictors. A common way to do this is a penalized regression method like LASSO. | In many contexts, you may want to include interaction terms or polynomials in your regression equation. | . Implementations . Gretl . # Load auto data open https://github.com/LOST-STATS/lost-stats.github.io/blob/master/Data/auto.gdt # Run OLS using the auto data, with mpg as the outcome variable # and headroom, trunk, and weight as predictors ols mpg const headroom trunk weight . Matlab . % Load auto data load(&#39;https://github.com/LOST-STATS/lost-stats.github.io/blob/master/Data/auto.mat&#39;) % Run OLS using the auto data, with mpg as the outcome variable % and headroom, trunk, and weight as predictors intercept = ones(length(headroom),1); X = [intercept headroom trunk weight]; [b,bint,r,rint,stats] = regress(mpg,X); . Python . &#39;&#39;&#39;Load R Datasets&#39;&#39;&#39; mtcars = sm.datasets.get_rdataset(&quot;mtcars&quot;).data # Fit OLS regression model to mtcars ols = smf.ols(formula= &#39;mpg ~ cyl + hp + wt&#39;, data= mtcars).fit() # Look at the OLS results print(ols.summary()) . R . # Load Data # data(mtcars) ## Optional: automatically loaded anyway # Run OLS using the mtcars data, with mpg as the outcome variable # and cyl, hp, and wt as predictors olsmodel &lt;- lm(mpg ~ cyl + hp + wt, data = mtcars) # Look at the results summary(olsmodel) . SAS . /* Load Data */ proc import datafile=&quot;C:mtcars.dbf&quot; out=fromr dbms=dbf; run; /* OLS regression */ proc reg; model mpg = cyl hp wt; run; . Stata . * Load auto data sysuse https://github.com/LOST-STATS/lost-stats.github.io/blob/master/Data/auto.dta * Run OLS using the auto data, with mpg as the outcome variable * and headroom, trunk, and weight as predictors regress mpg headroom trunk weight .",
    "url": "/Model_Estimation/ordinary_least_squares.html",
    "relUrl": "/Model_Estimation/ordinary_least_squares.html"
  }
  ,"33": {
    "title": "Penalized Regression",
    "content": "Penalized Regression . When running a regression, especially one with many predictors, the results have a tendency to overfit the data, reducing out-of-sample predictive properties. . Penalized regression eases this problem by forcing the regression estimator to shrink its coefficients towards 0 in order to avoid the “penalty” term imposed on the coefficients. This process is closely related to the idea of Bayesian shrinkage, and indeed standard penalized regression results are equivalent to regression performed using certain Bayesian priors. . Regular OLS selects coefficients to minimize the sum of squared errors: . Non-OLS regressions similarly select coefficients to minimize a similar objective function. Penalized regression adds a penalty term to that objective function, where is a tuning parameter that determines how harshly to penalize coefficients, and is the -norm of the coefficients, or . . Typically is set to 1 for LASSO regression (least absolute shrinkage and selection operator), which has the effect of tending to set coefficients to 0, i.e. model selection, or to 2 for Ridge Regression. Elastic net regression provides a weighted mix of LASSO and Ridge penalties, commonly referring to the weight as . . Keep in Mind . To avoid being penalized for a constant term, or by differences in scale between variables, it is a very good idea to standardize each variable (subtract the mean and divide by the standard deviation) before running a penalized regression. | Penalized regression can be run for logit and other kinds of regression, not just linear regression. Using penalties with general linear models like logit is common. | Penalized regression coefficients are designed to improve out-of-sample prediction, but they are biased. If the goal is estimation of a parameter, rather than prediction, this should be kept in mind. A common procedure is to use LASSO to select variables, and then run regular regression models with the variables that LASSO has selected. | The parameter is often chosen using cross-validation. Many penalized regression commands include an option to select by cross-validation automatically. | LASSO models commonly include variables along with polynomial transformation of those variables and interactions, allowing LASSO to determine which transformations are worth keeping. | . Also Consider . If it is not important to estimate coefficients but the goal is simply to predict an outcome, then there are many other machine learning methods that do so, and in some cases can handle higher dimensionality or work with smaller samples. | . Implementations . R . We will use the glmnet package. . # Install glmnet and tidyverse if necessary # install.packages(&#39;glmnet&#39;, &#39;tidyverse&#39;) # Load glmnet library(glmnet) # Load iris data data(iris) # Create a matrix with all variables other than our dependent vairable, Sepal.Length # and interactions. # -1 to omit the intercept M &lt;- model.matrix(lm(Sepal.Length ~ (.)^2 - 1, data = iris)) # Add squared terms of numeric variables numeric.var.names &lt;- names(iris)[2:4] M &lt;- cbind(M,as.matrix(iris[,numeric.var.names]^2)) colnames(M)[16:18] &lt;- paste(numeric.var.names,&#39;squared&#39;) # Create a matrix for our dependent variable too Y &lt;- as.matrix(iris$Sepal.Length) # Standardize all variables M &lt;- scale(M) Y &lt;- scale(Y) # Use glmnet to estimate penalized regression # We pick family = &quot;gaussian&quot; for linear regression; # other families work for other kinds of data, like binomial for binary data # In each case, we use cv.glmnet to pick our lambda value using cross-validation # using nfolds folds for cross-validation # Note that alpha = 1 picks LASSO cv.lasso &lt;- cv.glmnet(M, Y, family = &quot;gaussian&quot;, nfolds = 20, alpha = 1) # We might want to see how the choice of lambda relates to out-of-sample error with a plot plot(cv.lasso) # After doing CV, we commonly pick the lambda.min for lambda, # which is the lambda that minimizes out-of-sample error # or lambda.1se, which is one standard error above lambda.min, # which penalizes more harshly. The choice depends on context. lasso.model &lt;- glmnet(M, Y, family = &quot;gaussian&quot;, alpha = 1, lambda = cv.lasso$lambda.min) # coefficients are shown in the beta element. . means LASSO dropped it lasso.model$beta # Running Ridge, or mixing the two with elastic net, simply means picking # alpha = 0 (Ridge), or 0 &lt; alpha &lt; 1 (Elastic Net) cv.ridge &lt;- cv.glmnet(M, Y, family = &quot;gaussian&quot;, nfolds = 20, alpha = 0) ridge.model &lt;- glmnet(M, Y, family = &quot;gaussian&quot;, alpha = 0, lambda = cv.ridge$lambda.min) cv.elasticnet &lt;- cv.glmnet(M, Y, family = &quot;gaussian&quot;, nfolds = 20, alpha = .5) elasticnet.model &lt;- glmnet(M, Y, family = &quot;gaussian&quot;, alpha = .5, lambda = cv.elasticnet$lambda.min) . Stata . Penalized regression is one of the few machine learning algorithms that Stata does natively. This requires Stata 16. If you do not have Stata 16, you can alternately perform some forms of penalized regression by installing the lars package using ssc install lars. . * Use NLSY-W data sysuse nlsw88.dta, clear * Construct all squared and interaction terms by loop so we don&#39;t have to specify them all * by hand in the regression function local numeric_vars = &quot;age grade hours ttl_exp tenure&quot; local factor_vars = &quot;race married never_married collgrad south smsa c_city industry occupation union&quot; * Add all squares foreach x in `numeric_vars&#39; { g sq_`x&#39; = `x&#39;^2 } * Turn all factors into dummies so we can standardize them local faccount = 1 local dummy_vars = &quot;&quot; foreach x in `factor_vars&#39; { xi i.`x&#39;, pre(f`count&#39;_) local count = `count&#39; + 1 } * Add all numeric-numeric interactions; these are easy * factor interactions would need a more thorough loop forvalues i = 1(1)5 { local next_i = `i&#39;+1 forvalues j = `next_i&#39;(1)5 { local namei = word(&quot;`numeric_vars&#39;&quot;,`i&#39;) local namej = word(&quot;`numeric_vars&#39;&quot;,`j&#39;) g interact_`i&#39;_`j&#39; = `namei&#39;*`namej&#39; } } * Standardize everything foreach var of varlist `numeric_vars&#39; f*_* interact_* { qui summ `var&#39; qui replace `var&#39; = (`var&#39; - r(mean))/r(sd) } * Use the lasso command to run LASSO * using sel(cv) to select lambda using cross-validation * we specify a linear model here, but logit/probit/poisson would work lasso linear wage `numeric_vars&#39; f*_* interact_*, sel(cv) * get list of included coefficients lassocoef * We can use elasticnet to run Elastic Net * By default, alpha will be selected by cross-validation as well elasticnet linear wage `numeric_vars&#39; f*_* interact_*, sel(cv) .",
    "url": "/Machine_Learning/penalized_regression.html",
    "relUrl": "/Machine_Learning/penalized_regression.html"
  }
  ,"34": {
    "title": "Probit Model",
    "content": "Probit Regressions . A Probit regression is a statistical method for a best-fit line between a binary [0/1] outcome variable and any number of independent variables. Probit regressions follow a standard normal probability distribution and the predicted values are bounded between 0 and 1. . For more information about Probit, see Wikipedia: Probit. . Keep in Mind . The beta coefficients from a probit model are maximum likelihood estimations. They are not the marginal effect, as you would see in an OLS estimation. So you cannot interpret the beta coefficient as a marginal effect of on . | To obtain the marginal effect, you need to perform a post-estimation command to discover the marginal effect. In general, you can ‘eye-ball’ the marginal effect by dividing the probit beta coefficient by 2.5. | . Implementations . R . R can run a probit regression using the glm() function. However, to get marginal effects you will need to calculate them by hand or use a package. We will use the mfx package, although the margins package is another good option, which produces tidy model output. . # If necessary, install the mfx package # install.packages(&#39;mfx&#39;) # mfx is only needed for the marginal effect, not the regression itself library(mfx) # Load mtcars data data(mtcars) # Use the glm() function to run probit # Here we are predicting engine type using # miles per gallon and number of cylinders as predictors my_probit &lt;- glm(vs ~ mpg + cyl, data = mtcars, family = binomial(link = &#39;probit&#39;)) # The family argument says we are working with binary data # and using a probit link function (rather than, say, logit) # The results summary(my_probit) # Marginal effects probitmfx(vs ~ mpg + cyl, data = mtcars) . Stata . * Load auto data sysuse auto.dta * Probi Estimation probit foreign mpg weight headroom trunk * Recover the Marginal Effects (Beta Coefficient in OLS) margins, dydx(*) . Gretl . # Load auto data open auto.gdt # Run probit using the auto data, with mpg as the outcome variable # and headroom, trunk, and weight as predictors probit mpg const headroom trunk weight .",
    "url": "/Model_Estimation/probit_model.html",
    "relUrl": "/Model_Estimation/probit_model.html"
  }
  ,"35": {
    "title": "Reshaping Data",
    "content": "Reshaping Data .",
    "url": "/Data_Manipulation/Reshaping/reshape.html",
    "relUrl": "/Data_Manipulation/Reshaping/reshape.html"
  }
  ,"36": {
    "title": "Reshape Panel Data from Long to Wide",
    "content": "Reshape Panel Data from Long to Wide . Panel data is data in which individuals are observed at multiple points in time. There are two standard ways of storing this data: . In wide format, there is one row per individual. Then, for each variable in the data set that varies over time, there is one column per time period. For example: . Individual FixedCharacteristic TimeVarying1990 TimeVarying1991 TimeVarying1992 . 1 | C | 16 | 20 | 22 | . 2 | H | 23.4 | 10 | 14 | . This format makes it easy to perform calculations across multiple years. . In long format, there is one row per individual per time period: . Individual FixedCharacteristic Year TimeVarying . 1 | C | 1990 | 16 | . 1 | C | 1991 | 20 | . 1 | C | 1992 | 22 | . 2 | H | 1990 | 23.4 | . 2 | H | 1991 | 10 | . 2 | H | 1992 | 14 | . This format makes it easy to run models like fixed effects. . Reshaping is the method of converting wide-format data to long and vice versa. . Keep in Mind . If your data has multiple observations per individual/time, then standard reshaping techniques generally won’t work. | It’s a good idea to check your data by directly looking at it both before and after a reshape to check that it worked properly. | . Also Consider . To go in the other direction, reshape from wide to long. | Determine the observation level of a data set. | . Implementations . R . There are many ways to reshape in R, including base-R reshape and the deprecated reshape2::melt and cast and tidyr::gather and spread. We will be using the tidyr package function pivot_wider, which requires tidyr version 1.0.0 or later. . # install.packages(&#39;tidyr&#39;) library(tidyr) # Load in population, which has one row per country per year data(&quot;population&quot;) # If we look at the data, we&#39;ll see that we have: # identifying information in &quot;country&quot;, # a time indicator in &quot;year&quot;, # and our values in &quot;population&quot; head(population) . Now we think: . Think about the set of variables that contain the values we’re interested in reshaping. Here’s it’s population. This list of variable names will be our values_from argument. | Think about what we want the new variables to be called. The variable variable says which variable we’re looking at. So that will be our names_from argument. And we want to specify that each variable represents population in a given year (rather than some other variable, so we’ll add “pop_” as our names_prefix. | pop_wide &lt;- pivot_wider(population, names_from = year, values_from = population, names_prefix = &quot;pop_&quot;) . Stata . * Load blood pressure data in long format, which contains * blood pressure both before and after a treatment for some patients sysuse bplong.dta . The next steps involve thinking: . Think about the set of variables that identify individuals. Here it’s patient. This will go in i(), so we have i(patient). | Think about the set of variables that vary across time. Here’s it’s bp. This will be one of our “stub”s. | Think about which variable separates the different time periods within individual. Here we have “when”, and this goes in j(), so we have j(when). | * Syntax is: * reshape wide stub, i(individualvars) j(newtimevar) * So we have reshape wide bp i(patient) j(when) * Note that simply typing reshape * will show the syntax for the function . With especially large datasets, the Gtools package provides a much faster version of reshape known as greshape. The syntax can function exactly the same, though they provide alternative syntax that you may find more intuitive. . * First, we will create a toy dataset that is very large to demonstrate the speed gains * If necessary, first install gtools: * ssc install gtools * Clear memory clear all * Turn on return message to see command run time set rmsg on * Set data size to 15 million observations set obs 15000000 * Create ten observations per person generate person_id = floor((_n-1)/10) * Number time periods from 1 to 10 for each person generate time_id = mod((_n-1), 10) + 1 *Create an income in each period generate income = round(rnormal(100, 20)) * Demonstrate the comparative speed of these two reshape approaches. * preserve and restore aren&#39;t a part of the reshape command; * they just store the current state of the data and then restore it, * so we can try our different reshape commands on the same data. *The traditional reshape command preserve reshape wide income, i(person_id) j(time_id) restore *The Gtools reshape command preserve greshape wide income, i(person_id) j(time_id) restore *The Gtools reshape command, alternative syntax preserve greshape wide income, by(person_id) keys(time_id) restore . Note: there is much more guidance to the usage of greshape on the Gtools reshape page. .",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html",
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_long_to_wide.html"
  }
  ,"37": {
    "title": "Reshape Panel Data from Wide to Long",
    "content": "Reshape Panel Data from Wide to Long . Panel data is data in which individuals are observed at multiple points in time. There are two standard ways of storing this data: . In wide format, there is one row per individual. Then, for each variable in the data set that varies over time, there is one column per time period. For example: . Individual FixedCharacteristic TimeVarying1990 TimeVarying1991 TimeVarying1992 . 1 | C | 16 | 20 | 22 | . 2 | H | 23.4 | 10 | 14 | . This format makes it easy to perform calculations across multiple years. . In long format, there is one row per individual per time period: . Individual FixedCharacteristic Year TimeVarying . 1 | C | 1990 | 16 | . 1 | C | 1991 | 20 | . 1 | C | 1992 | 22 | . 2 | H | 1990 | 23.4 | . 2 | H | 1991 | 10 | . 2 | H | 1992 | 14 | . This format makes it easy to run models like fixed effects. . Reshaping is the method of converting wide-format data to long and vice versa.. . Keep in Mind . If your data has multiple observations per individual/time, then standard reshaping techniques generally won’t work. | It’s a good idea to check your data by directly looking at it both before and after a reshape to check that it worked properly. | . Also Consider . To go in the other direction, reshape from long to wide. | Determine the observation level of a data set. | . Implementations . R . There are many ways to reshape in R, including base-R reshape and the deprecated reshape2::melt and cast and tidyr::gather and spread. We will be using the tidyr package function pivot_longer, which requires tidyr version 1.0.0 or later. . # install.packages(&#39;tidyr&#39;) library(tidyr) # Load in billboard, which has one row per song # and one variable per week, for its chart position each week data(&quot;billboard&quot;) # If we look at the data, we&#39;ll see that we have: # identifying information in &quot;artist&quot; and &quot;track&quot; # A variable consistent within individuals &quot;date.entered&quot; # and a bunch of variables containing position information # all named wk and then a number names(billboard) . Now we think: . Think about the set of variables that contain time-varying information. Here’s it’s wk1-wk76. So we can give a list of all the variables we want to widen using the tidyselect helper function starts_with(): starts_with(&quot;wk&quot;). This list of variable names will be our col argument. | Think about what we want the new variables to be called. I’ll call the week time variable “week” (this will be the names_to argument), and the data values currently stored in wk1-wk76 is the “position” (values_to). | Think about the values you want to be in your new time variable. The column names are wk1-wk76 but we want the variable to have 1-76 instead, so we’ll take out the “wk” with names_prefix = &quot;wk&quot;. | billboard_long &lt;- pivot_longer(billboard, col = starts_with(&quot;wk&quot;), names_to = &quot;week&quot;, names_prefix = &quot;wk&quot;, values_to = &quot;position&quot;, values_drop_na = TRUE) # values_drop_na says to drop any rows containing missing values of position. # If reshaping to create multiple variables, see the names_sep or names_pattern options. . Stata . * Load blood pressure data in wide format, which contains * bp_before and bp_after sysuse bpwide.dta . The next steps involve thinking: . Think about the set of variables that identify individuals. Here it’s patient. This will go in i(), so we have i(patient). | Think about the set of variables that vary across time. Here’s it’s bp_. Note the inclusion of the _, so that “before” and “after” will be our time periods. This will be one of our “stub”s. | Think about what we want the new time variable to be called. I’ll just call it “time”, and this goes in j(), so we have j(time). | * Syntax is: * reshape long stub, i(individualvars) j(newtimevar) * So we have reshape long bp_ i(patient) j(time) s * Where the s indicates that our time variable is a string (&quot;before&quot;, &quot;after&quot;) * Note that simply typing reshape * will show the syntax for the function . With especially large datasets, the Gtools package provides a much faster version of reshape known as greshape. The syntax can function exactly the same, though they provide alternative syntax that you may find more intuitive. . * If necessary, install gtools * ssc install gtools * First, we will create a toy dataset that is very large to demonstrate the speed gains * Clear memory clear all * Turn on return message to see command run time set rmsg on * Set data size to 15 million observations set obs 15000000 * Create an ID variable generate person_id = _n * Create 4 separate fake test scores per student generate test_score1 = round(rnormal(180, 30)) generate test_score2 = round(rnormal(180, 30)) generate test_score3 = round(rnormal(180, 30)) generate test_score4 = round(rnormal(180, 30)) * Demonstrate the comparative speed of these two reshape approaches * preserve and restore aren&#39;t a part of the reshape command; * they just store the current state of the data and then restore it, * so we can try our different reshape commands on the same data. * The traditional reshape command preserve reshape long test_score, i(person_id) j(test_number) restore *The Gtools reshape command preserve greshape long test_score, i(person_id) j(test_number) restore *The Gtools reshape command, alternative syntax preserve greshape long test_score, by(person_id) keys(test_number) restore . Note: there is much more guidance to the usage of greshape on the Gtools reshape page. .",
    "url": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html",
    "relUrl": "/Data_Manipulation/Reshaping/reshape_panel_data_from_wide_to_long.html"
  }
  ,"38": {
    "title": "Scatterplot by Group on Shared Axes",
    "content": "Scatterplot by Group on Shared Axes . Scatterplots are a standard data visualization tool that allows you to look at the relationship between two variables and . If you want to see how the relationship between and might be different for Group A as opposed to Group B, then you might want to plot the scatterplot for both groups on the same set of axes, so you can compare them. . Keep in Mind . Scatterplots may not work well if the data is discrete, or if there are a large number of data points. | . Also Consider . Sometimes, instead of putting both Group A and Group B on the same set of axes, it makes more sense to plot them separately, and put the plots next to each other. See Faceted Graphs. | There are many ways to make the scatterplots of the two groups distinct. See Styling Scatterplots. | . Implementations . R . library(ggplot2) # Load auto data data(mtcars) # Make sure that our grouping variable is a factor # and labeled properly mtcars$Transmission &lt;- factor(mtcars$am, labels = c(&quot;Automatic&quot;, &quot;Manual&quot;)) # Put wt on the x-axis, mpg on the y-axis, ggplot(mtcars, aes(x = wt, y = mpg, # distinguish the Transmission values by color, color = Transmission)) + # make it a scatterplot with geom_point() geom_point()+ # And label properly labs(x = &quot;Car Weight&quot;, y = &quot;MPG&quot;) . This results in: . . Stata . * Load auto data sysuse auto.dta * Start a twoway command * Then, for each group, put its scatter command in () * Using if to plot each group separately * And specifying mcolor or msymbol (etc.) to differentiate them twoway (scatter weight mpg if foreign == 0, mcolor(black)) (scatter weight mpg if foreign == 1, mcolor(blue)) * Add a legend option so you know what the colors mean twoway (scatter weight mpg if foreign == 0, mcolor(black)) (scatter weight mpg if foreign == 1, mcolor(blue)), legend(lab(1 Domestic) lab(2 Foreign)) xtitle(&quot;Weight&quot;) ytitle(&quot;MPG&quot;) . This results in: . .",
    "url": "/Presentation/scatterplot_by_group_on_shared_axes.html",
    "relUrl": "/Presentation/scatterplot_by_group_on_shared_axes.html"
  }
  ,"39": {
    "title": "Home",
    "content": "Home . Welcome to the Library of Statistical Techniques (LOST)! . LOST is a publicly-editable website with the goal of making it easy to execute statistical techniques in statistical software. . Each page of the website contains a statistical technique — which may be an estimation method, a data manipulation or cleaning method, a method for presenting or visualizing results, or any of the other kinds of things that statistical software typically does. . For each of those techniques, the LOST page will contain code for performing that method in a variety of packages and languages. It may also contain information (or links) with thorough descriptions of the method, but the focus here is on implementation. How can you do it in your language of choice? If there are multiple ways, how are those ways different? Is the way you used to do it outdated, or does it do something unexpected? What’s the R equivalent of that command you know about in Stata or SAS, or vice versa? . In short, LOST is a Rosetta Stone for statistical software. . If you are interested in contributing to LOST, please see the Contributing page. . LOST was originated in 2019 by Nick Huntington-Klein and is maintained by volunteer contributors. The project’s GitHub page is here. .",
    "url": "/",
    "relUrl": "/"
  }
  ,"40": {
    "title": "",
    "content": "Folder for adding user contributed data. .",
    "url": "/Data/",
    "relUrl": "/Data/"
  }
  
}